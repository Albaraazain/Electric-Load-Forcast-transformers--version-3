{
    "sourceFile": "data_loading/datasets/transformer_dataset.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1733265086470,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733265115807,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,167 @@\n+import numpy as np\r\n+import pandas as pd\r\n+import torch\r\n+from typing import Tuple, List, Any\r\n+\r\n+from data_loading.base.base_dataset import BaseDataset, DatasetConfig\r\n+from data_loading.features.time_features import CyclicalTimeFeature, WorkdayFeature\r\n+from data_loading.preprocessing.data_scaler import DataScaler\r\n+from data_loading.preprocessing.data_transformer import DataTransformer\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n+\r\n+class TransformerDataset(BaseDataset):\r\n+    \"\"\"Dataset implementation for transformer models with proper sequence handling\"\"\"\r\n+\r\n+    def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n+        super().__init__(df, config)\r\n+        self.rows: torch.Tensor = torch.empty(0)\r\n+        self.logger = Logger.get_logger(__name__)\r\n+        self._prepare_time_series_data()\r\n+        self._debug_counter = 0\r\n+        self._debug_frequency = 10000\r\n+\r\n+    def __len__(self) -> int:\r\n+        \"\"\"\r\n+        Calculate the number of available sequences in the dataset.\r\n+        \r\n+        Returns:\r\n+            Number of sequences that can be extracted from the data\r\n+        \"\"\"\r\n+        # Make sure we have data\r\n+        if self.rows is None or len(self.rows) == 0:\r\n+            return 0\r\n+            \r\n+        # Calculate total sequence length needed for input and forecast\r\n+        total_window = (\r\n+            self.config.window_size +    # Input sequence length\r\n+            self.config.horizon_size     # Forecast sequence length\r\n+        )\r\n+        \r\n+        # Return maximum number of sequences we can create\r\n+        # Subtract total_window - 1 to ensure we have enough points for the last sequence\r\n+        return max(0, len(self.rows) - (total_window - 1))\r\n+\r\n+    def __getitem__(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\r\n+        if self.rows is None:\r\n+            raise ValueError(\"Dataset not properly initialized\")\r\n+\r\n+        input_window = self.config.window_size\r\n+        forecast_window = self.config.horizon_size\r\n+\r\n+        try:\r\n+            # Get sequences with gradient tracking\r\n+            src_seq = self.rows[index:index + input_window].detach().requires_grad_(True)\r\n+            full_target = self.rows[\r\n+                index + input_window:index + input_window + forecast_window\r\n+            ].detach().requires_grad_(True)\r\n+            \r\n+            # Prepare sequences while maintaining gradients\r\n+            tgt_input_seq = full_target[:-1].requires_grad_(True)\r\n+            target_seq = full_target[1:, 0:1].requires_grad_(True)\r\n+\r\n+            # Add batch dimension if needed while preserving gradients\r\n+            if len(src_seq.shape) == 2:\r\n+                src_seq = src_seq.unsqueeze(0)\r\n+                tgt_input_seq = tgt_input_seq.unsqueeze(0)\r\n+                target_seq = target_seq.unsqueeze(0)\r\n+\r\n+            # Debug gradient tracking (helpful for diagnosis)\r\n+            if self._debug_counter % self._debug_frequency == 0:\r\n+                self.logger.debug(\"Dataset sample gradient info\", {\r\n+                    \"sample\": self._debug_counter,\r\n+                    \"src_seq_requires_grad\": src_seq.requires_grad,\r\n+                    \"tgt_input_requires_grad\": tgt_input_seq.requires_grad,\r\n+                    \"target_requires_grad\": target_seq.requires_grad,\r\n+                    \"source_shape\": src_seq.shape,\r\n+                    \"target_input_shape\": tgt_input_seq.shape,\r\n+                    \"target_shape\": target_seq.shape\r\n+                })\r\n+            \r\n+            self._debug_counter += 1\r\n+\r\n+            return (src_seq, tgt_input_seq), target_seq\r\n+                \r\n+        except Exception as e:\r\n+            self.logger.error(\"Error in __getitem__\", {\r\n+                \"index\": index,\r\n+                \"error\": str(e),\r\n+                \"dataset_size\": len(self.rows),\r\n+                \"input_window\": input_window,\r\n+                \"forecast_window\": forecast_window\r\n+            })\r\n+            raise\r\n+\r\n+    def _prepare_time_series_data(self) -> None:\r\n+        \"\"\"Prepare time series data for transformer model input\"\"\"\r\n+        self.logger.info(\"Preparing transformer dataset\", {\r\n+            \"dataframe_shape\": self._df.shape\r\n+        })\r\n+        \r\n+        if len(self._df) == 0:\r\n+            raise ValueError(\"Empty dataframe provided\")\r\n+\r\n+        try:\r\n+            # Initialize processors\r\n+            scaler = DataScaler(self.config.time_series_scaler)\r\n+            transformer = DataTransformer()\r\n+\r\n+            # Process load data\r\n+            load_data = np.array(self._df[self.config.target_variable])\r\n+            time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n+\r\n+            # Scale and normalize data\r\n+            if self.config.time_series_scaler:\r\n+                scaled_data = (\r\n+                    scaler.fit_transform(load_data)\r\n+                    if self.config.is_training_set\r\n+                    else scaler.transform(load_data)\r\n+                )\r\n+                scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n+            else:\r\n+                scaled_data = load_data\r\n+\r\n+            # Generate features for each timestamp\r\n+            sequence_rows = []\r\n+            for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n+                features = [load_value]  # Main value first\r\n+                \r\n+                if self.config.include_time_information:\r\n+                    timestamp_pd = pd.Timestamp(time_stamp)\r\n+                    # Add cyclical time encodings\r\n+                    features.extend([\r\n+                        np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n+                        np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n+                        np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                        np.cos(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                        np.sin(2 * np.pi * timestamp_pd.month / 12),\r\n+                        np.cos(2 * np.pi * timestamp_pd.month / 12)\r\n+                    ])\r\n+\r\n+                sequence_rows.append(features)\r\n+\r\n+            # Convert to tensor and store\r\n+            self.rows = torch.tensor(\r\n+                np.array(sequence_rows, dtype=np.float32), \r\n+                requires_grad=True,  # Enable gradient tracking\r\n+                dtype=torch.float32\r\n+            )\r\n+\r\n+            \r\n+            # Print debug information\r\n+            self.logger.debug(\"Feature generation complete\", {\r\n+                \"total_sequences\": len(self.rows),\r\n+                \"features_per_sequence\": self.rows.shape[-1],\r\n+                \"value_range\": f\"[{self.rows.min():.2f}, {self.rows.max():.2f}]\"\r\n+            })\r\n+\r\n+            # Validate tensor\r\n+            if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n+                raise ValueError(\"Invalid values detected in final tensor\")\r\n+\r\n+        except Exception as e:\r\n+            self.logger.error(\"Error preparing time series data\", {\r\n+                \"error\": str(e),\r\n+                \"dataframe_info\": str(self._df.info())\r\n+            })\r\n+            raise\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733265134820,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,192 @@\n+import numpy as np\r\n+import pandas as pd\r\n+import torch\r\n+from typing import Tuple, List, Any\r\n+\r\n+from data_loading.base.base_dataset import BaseDataset, DatasetConfig\r\n+from data_loading.features.time_features import CyclicalTimeFeature, WorkdayFeature\r\n+from data_loading.preprocessing.data_scaler import DataScaler\r\n+from data_loading.preprocessing.data_transformer import DataTransformer\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n+\r\n+class TransformerDataset(BaseDataset):\r\n+    \"\"\"Dataset implementation for transformer models with proper sequence handling\"\"\"\r\n+\r\n+    def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n+        super().__init__(df, config)\r\n+        self.rows: torch.Tensor = torch.empty(0)\r\n+        self.logger = Logger.get_logger(__name__)\r\n+        self._prepare_time_series_data()\r\n+        self._debug_counter = 0\r\n+        self._debug_frequency = 10000\r\n+\r\n+    def __len__(self) -> int:\r\n+        \"\"\"\r\n+        Calculate the number of available sequences in the dataset.\r\n+        \r\n+        Returns:\r\n+            Number of sequences that can be extracted from the data\r\n+        \"\"\"\r\n+        # Make sure we have data\r\n+        if self.rows is None or len(self.rows) == 0:\r\n+            return 0\r\n+            \r\n+        # Calculate total sequence length needed for input and forecast\r\n+        total_window = (\r\n+            self.config.window_size +    # Input sequence length\r\n+            self.config.horizon_size     # Forecast sequence length\r\n+        )\r\n+        \r\n+        # Return maximum number of sequences we can create\r\n+        # Subtract total_window - 1 to ensure we have enough points for the last sequence\r\n+        return max(0, len(self.rows) - (total_window - 1))\r\n+    \r\n+    def _validate_gradients(self, tensors: Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]) -> None:\r\n+        \"\"\"Validate gradient settings of output tensors.\"\"\"\r\n+        (src_seq, tgt_input_seq), target_seq = tensors\r\n+        \r\n+        # Check gradient tracking\r\n+        if not src_seq.requires_grad:\r\n+            self.logger.warning(\"Source sequence missing gradient tracking\")\r\n+            \r\n+        if not tgt_input_seq.requires_grad:\r\n+            self.logger.warning(\"Target input sequence missing gradient tracking\")\r\n+            \r\n+        # We typically don't need gradients for the target\r\n+        # as it's used for loss computation\r\n+        \r\n+        # Validate tensor properties\r\n+        for name, tensor in [\r\n+            (\"src_seq\", src_seq), \r\n+            (\"tgt_input_seq\", tgt_input_seq), \r\n+            (\"target_seq\", target_seq)\r\n+        ]:\r\n+            if torch.isnan(tensor).any():\r\n+                raise ValueError(f\"NaN values detected in {name}\")\r\n+            if torch.isinf(tensor).any():\r\n+                raise ValueError(f\"Inf values detected in {name}\")\r\n+\r\n+    def __getitem__(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\r\n+        if self.rows is None:\r\n+            raise ValueError(\"Dataset not properly initialized\")\r\n+\r\n+        input_window = self.config.window_size\r\n+        forecast_window = self.config.horizon_size\r\n+\r\n+        try:\r\n+            # Get sequences with gradient tracking\r\n+            src_seq = self.rows[index:index + input_window].detach().requires_grad_(True)\r\n+            full_target = self.rows[\r\n+                index + input_window:index + input_window + forecast_window\r\n+            ].detach().requires_grad_(True)\r\n+            \r\n+            # Prepare sequences while maintaining gradients\r\n+            tgt_input_seq = full_target[:-1].requires_grad_(True)\r\n+            target_seq = full_target[1:, 0:1].requires_grad_(True)\r\n+\r\n+            # Add batch dimension if needed while preserving gradients\r\n+            if len(src_seq.shape) == 2:\r\n+                src_seq = src_seq.unsqueeze(0)\r\n+                tgt_input_seq = tgt_input_seq.unsqueeze(0)\r\n+                target_seq = target_seq.unsqueeze(0)\r\n+\r\n+            # Debug gradient tracking (helpful for diagnosis)\r\n+            if self._debug_counter % self._debug_frequency == 0:\r\n+                self.logger.debug(\"Dataset sample gradient info\", {\r\n+                    \"sample\": self._debug_counter,\r\n+                    \"src_seq_requires_grad\": src_seq.requires_grad,\r\n+                    \"tgt_input_requires_grad\": tgt_input_seq.requires_grad,\r\n+                    \"target_requires_grad\": target_seq.requires_grad,\r\n+                    \"source_shape\": src_seq.shape,\r\n+                    \"target_input_shape\": tgt_input_seq.shape,\r\n+                    \"target_shape\": target_seq.shape\r\n+                })\r\n+            \r\n+            self._debug_counter += 1\r\n+\r\n+            return (src_seq, tgt_input_seq), target_seq\r\n+                \r\n+        except Exception as e:\r\n+            self.logger.error(\"Error in __getitem__\", {\r\n+                \"index\": index,\r\n+                \"error\": str(e),\r\n+                \"dataset_size\": len(self.rows),\r\n+                \"input_window\": input_window,\r\n+                \"forecast_window\": forecast_window\r\n+            })\r\n+            raise\r\n+\r\n+    def _prepare_time_series_data(self) -> None:\r\n+        \"\"\"Prepare time series data for transformer model input\"\"\"\r\n+        self.logger.info(\"Preparing transformer dataset\", {\r\n+            \"dataframe_shape\": self._df.shape\r\n+        })\r\n+        \r\n+        if len(self._df) == 0:\r\n+            raise ValueError(\"Empty dataframe provided\")\r\n+\r\n+        try:\r\n+            # Initialize processors\r\n+            scaler = DataScaler(self.config.time_series_scaler)\r\n+            transformer = DataTransformer()\r\n+\r\n+            # Process load data\r\n+            load_data = np.array(self._df[self.config.target_variable])\r\n+            time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n+\r\n+            # Scale and normalize data\r\n+            if self.config.time_series_scaler:\r\n+                scaled_data = (\r\n+                    scaler.fit_transform(load_data)\r\n+                    if self.config.is_training_set\r\n+                    else scaler.transform(load_data)\r\n+                )\r\n+                scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n+            else:\r\n+                scaled_data = load_data\r\n+\r\n+            # Generate features for each timestamp\r\n+            sequence_rows = []\r\n+            for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n+                features = [load_value]  # Main value first\r\n+                \r\n+                if self.config.include_time_information:\r\n+                    timestamp_pd = pd.Timestamp(time_stamp)\r\n+                    # Add cyclical time encodings\r\n+                    features.extend([\r\n+                        np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n+                        np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n+                        np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                        np.cos(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                        np.sin(2 * np.pi * timestamp_pd.month / 12),\r\n+                        np.cos(2 * np.pi * timestamp_pd.month / 12)\r\n+                    ])\r\n+\r\n+                sequence_rows.append(features)\r\n+\r\n+            # Convert to tensor and store\r\n+            self.rows = torch.tensor(\r\n+                np.array(sequence_rows, dtype=np.float32), \r\n+                requires_grad=True,  # Enable gradient tracking\r\n+                dtype=torch.float32\r\n+            )\r\n+\r\n+            \r\n+            # Print debug information\r\n+            self.logger.debug(\"Feature generation complete\", {\r\n+                \"total_sequences\": len(self.rows),\r\n+                \"features_per_sequence\": self.rows.shape[-1],\r\n+                \"value_range\": f\"[{self.rows.min():.2f}, {self.rows.max():.2f}]\"\r\n+            })\r\n+\r\n+            # Validate tensor\r\n+            if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n+                raise ValueError(\"Invalid values detected in final tensor\")\r\n+\r\n+        except Exception as e:\r\n+            self.logger.error(\"Error preparing time series data\", {\r\n+                \"error\": str(e),\r\n+                \"dataframe_info\": str(self._df.info())\r\n+            })\r\n+            raise\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733265261166,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -116,77 +116,77 @@\n                 \"forecast_window\": forecast_window\r\n             })\r\n             raise\r\n \r\n-    def _prepare_time_series_data(self) -> None:\r\n-        \"\"\"Prepare time series data for transformer model input\"\"\"\r\n-        self.logger.info(\"Preparing transformer dataset\", {\r\n-            \"dataframe_shape\": self._df.shape\r\n-        })\r\n-        \r\n-        if len(self._df) == 0:\r\n-            raise ValueError(\"Empty dataframe provided\")\r\n+def _prepare_time_series_data(self) -> None:\r\n+    \"\"\"Prepare time series data with proper gradient tracking.\"\"\"\r\n+    self.logger.info(\"Preparing transformer dataset\", {\r\n+        \"dataframe_shape\": self._df.shape\r\n+    })\r\n+    \r\n+    if len(self._df) == 0:\r\n+        raise ValueError(\"Empty dataframe provided\")\r\n \r\n-        try:\r\n-            # Initialize processors\r\n-            scaler = DataScaler(self.config.time_series_scaler)\r\n-            transformer = DataTransformer()\r\n+    try:\r\n+        # Initialize processors\r\n+        scaler = DataScaler(self.config.time_series_scaler)\r\n+        transformer = DataTransformer()\r\n \r\n-            # Process load data\r\n-            load_data = np.array(self._df[self.config.target_variable])\r\n-            time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n+        # Process load data\r\n+        load_data = np.array(self._df[self.config.target_variable])\r\n+        time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n \r\n-            # Scale and normalize data\r\n-            if self.config.time_series_scaler:\r\n-                scaled_data = (\r\n-                    scaler.fit_transform(load_data)\r\n-                    if self.config.is_training_set\r\n-                    else scaler.transform(load_data)\r\n-                )\r\n-                scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n-            else:\r\n-                scaled_data = load_data\r\n+        # Scale and normalize data\r\n+        if self.config.time_series_scaler:\r\n+            scaled_data = (\r\n+                scaler.fit_transform(load_data)\r\n+                if self.config.is_training_set\r\n+                else scaler.transform(load_data)\r\n+            )\r\n+            scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n+        else:\r\n+            scaled_data = load_data\r\n \r\n-            # Generate features for each timestamp\r\n-            sequence_rows = []\r\n-            for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n-                features = [load_value]  # Main value first\r\n-                \r\n-                if self.config.include_time_information:\r\n-                    timestamp_pd = pd.Timestamp(time_stamp)\r\n-                    # Add cyclical time encodings\r\n-                    features.extend([\r\n-                        np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n-                        np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n-                        np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n-                        np.cos(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n-                        np.sin(2 * np.pi * timestamp_pd.month / 12),\r\n-                        np.cos(2 * np.pi * timestamp_pd.month / 12)\r\n-                    ])\r\n+        # Generate features for each timestamp\r\n+        sequence_rows = []\r\n+        for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n\\ No newline at end of file\n+            features = [load_value]  # Main value first\r\n+            \r\n+            if self.config.include_time_information:\r\n+                timestamp_pd = pd.Timestamp(time_stamp)\r\n+                # Add cyclical time encodings\r\n+                features.extend([\r\n+                    np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n+                    np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n+                    np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                    np.cos(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                    np.sin(2 * np.pi * timestamp_pd.month / 12),\r\n+                    np.cos(2 * np.pi * timestamp_pd.month / 12)\r\n+                ])\r\n \r\n-                sequence_rows.append(features)\r\n+            sequence_rows.append(features)\r\n \r\n-            # Convert to tensor and store\r\n-            self.rows = torch.tensor(\r\n-                np.array(sequence_rows, dtype=np.float32), \r\n-                requires_grad=True,  # Enable gradient tracking\r\n-                dtype=torch.float32\r\n-            )\r\n+        # Convert to tensor with gradient tracking enabled\r\n+        self.rows = torch.tensor(\r\n+            np.array(sequence_rows, dtype=np.float32),\r\n+            requires_grad=True,\r\n+            dtype=torch.float32\r\n+        )\r\n \r\n-            \r\n-            # Print debug information\r\n-            self.logger.debug(\"Feature generation complete\", {\r\n-                \"total_sequences\": len(self.rows),\r\n-                \"features_per_sequence\": self.rows.shape[-1],\r\n-                \"value_range\": f\"[{self.rows.min():.2f}, {self.rows.max():.2f}]\"\r\n-            })\r\n+        # Print debug information\r\n+        self.logger.debug(\"Feature generation complete\", {\r\n+            \"total_sequences\": len(self.rows),\r\n+            \"features_per_sequence\": self.rows.shape[-1],\r\n+            \"value_range\": f\"[{self.rows.min():.2f}, {self.rows.max():.2f}]\",\r\n+            \"requires_grad\": self.rows.requires_grad\r\n+        })\r\n \r\n-            # Validate tensor\r\n-            if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n-                raise ValueError(\"Invalid values detected in final tensor\")\r\n+        # Validate tensor\r\n+        if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n+            raise ValueError(\"Invalid values detected in final tensor\")\r\n \r\n-        except Exception as e:\r\n-            self.logger.error(\"Error preparing time series data\", {\r\n-                \"error\": str(e),\r\n-                \"dataframe_info\": str(self._df.info())\r\n-            })\r\n-            raise\n+    except Exception as e:\r\n+        self.logger.error(\"Error preparing time series data\", {\r\n+            \"error\": str(e),\r\n+            \"dataframe_info\": str(self._df.info())\r\n+        })\r\n+        raise\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733265281738,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,47 +67,53 @@\n             if torch.isinf(tensor).any():\r\n                 raise ValueError(f\"Inf values detected in {name}\")\r\n \r\n     def __getitem__(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\r\n+        \"\"\"Get a data item with proper gradient tracking.\"\"\"\r\n         if self.rows is None:\r\n             raise ValueError(\"Dataset not properly initialized\")\r\n \r\n         input_window = self.config.window_size\r\n         forecast_window = self.config.horizon_size\r\n \r\n         try:\r\n-            # Get sequences with gradient tracking\r\n-            src_seq = self.rows[index:index + input_window].detach().requires_grad_(True)\r\n-            full_target = self.rows[\r\n-                index + input_window:index + input_window + forecast_window\r\n-            ].detach().requires_grad_(True)\r\n+            # Get sequences with gradient tracking maintained\r\n+            src_seq = self.rows[index:index + input_window]\r\n+            full_target = self.rows[index + input_window:index + input_window + forecast_window]\r\n             \r\n-            # Prepare sequences while maintaining gradients\r\n-            tgt_input_seq = full_target[:-1].requires_grad_(True)\r\n-            target_seq = full_target[1:, 0:1].requires_grad_(True)\r\n+            # Prepare sequences for teacher forcing while maintaining gradients\r\n+            tgt_input_seq = full_target[:-1]\r\n+            target_seq = full_target[1:, 0:1]  # Only keep energy consumption value\r\n \r\n-            # Add batch dimension if needed while preserving gradients\r\n-            if len(src_seq.shape) == 2:\r\n-                src_seq = src_seq.unsqueeze(0)\r\n-                tgt_input_seq = tgt_input_seq.unsqueeze(0)\r\n-                target_seq = target_seq.unsqueeze(0)\r\n-\r\n-            # Debug gradient tracking (helpful for diagnosis)\r\n+            # Debug information with gradient tracking status\r\n             if self._debug_counter % self._debug_frequency == 0:\r\n-                self.logger.debug(\"Dataset sample gradient info\", {\r\n+                self.logger.debug(\"Dataset sample info\", {\r\n                     \"sample\": self._debug_counter,\r\n-                    \"src_seq_requires_grad\": src_seq.requires_grad,\r\n-                    \"tgt_input_requires_grad\": tgt_input_seq.requires_grad,\r\n-                    \"target_requires_grad\": target_seq.requires_grad,\r\n                     \"source_shape\": src_seq.shape,\r\n                     \"target_input_shape\": tgt_input_seq.shape,\r\n-                    \"target_shape\": target_seq.shape\r\n+                    \"target_shape\": target_seq.shape,\r\n+                    \"source_range\": f\"[{src_seq.min():.2f}, {src_seq.max():.2f}]\",\r\n+                    \"target_range\": f\"[{target_seq.min():.2f}, {target_seq.max():.2f}]\",\r\n+                    \"source_requires_grad\": src_seq.requires_grad,\r\n+                    \"target_input_requires_grad\": tgt_input_seq.requires_grad,\r\n+                    \"target_requires_grad\": target_seq.requires_grad\r\n                 })\r\n             \r\n             self._debug_counter += 1\r\n \r\n+            # Add batch dimension while preserving gradients\r\n+            if len(src_seq.shape) == 2:\r\n+                src_seq = src_seq.unsqueeze(0)\r\n+                tgt_input_seq = tgt_input_seq.unsqueeze(0)\r\n+                target_seq = target_seq.unsqueeze(0)\r\n+\r\n+            # Ensure tensors have proper gradient tracking\r\n+            src_seq.requires_grad_(True)\r\n+            tgt_input_seq.requires_grad_(True)\r\n+            # Note: target_seq typically doesn't need gradients as it's the ground truth\r\n+\r\n             return (src_seq, tgt_input_seq), target_seq\r\n-                \r\n+            \r\n         except Exception as e:\r\n             self.logger.error(\"Error in __getitem__\", {\r\n                 \"index\": index,\r\n                 \"error\": str(e),\r\n@@ -116,77 +122,78 @@\n                 \"forecast_window\": forecast_window\r\n             })\r\n             raise\r\n \r\n-def _prepare_time_series_data(self) -> None:\r\n-    \"\"\"Prepare time series data with proper gradient tracking.\"\"\"\r\n-    self.logger.info(\"Preparing transformer dataset\", {\r\n-        \"dataframe_shape\": self._df.shape\r\n-    })\r\n-    \r\n-    if len(self._df) == 0:\r\n-        raise ValueError(\"Empty dataframe provided\")\r\n \r\n-    try:\r\n-        # Initialize processors\r\n-        scaler = DataScaler(self.config.time_series_scaler)\r\n-        transformer = DataTransformer()\r\n+    def _prepare_time_series_data(self) -> None:\r\n+        \"\"\"Prepare time series data with proper gradient tracking.\"\"\"\r\n+        self.logger.info(\"Preparing transformer dataset\", {\r\n+            \"dataframe_shape\": self._df.shape\r\n+        })\r\n+        \r\n+        if len(self._df) == 0:\r\n+            raise ValueError(\"Empty dataframe provided\")\r\n \r\n-        # Process load data\r\n-        load_data = np.array(self._df[self.config.target_variable])\r\n-        time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n+        try:\r\n+            # Initialize processors\r\n+            scaler = DataScaler(self.config.time_series_scaler)\r\n+            transformer = DataTransformer()\r\n \r\n-        # Scale and normalize data\r\n-        if self.config.time_series_scaler:\r\n-            scaled_data = (\r\n-                scaler.fit_transform(load_data)\r\n-                if self.config.is_training_set\r\n-                else scaler.transform(load_data)\r\n-            )\r\n-            scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n-        else:\r\n-            scaled_data = load_data\r\n+            # Process load data\r\n+            load_data = np.array(self._df[self.config.target_variable])\r\n+            time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n \r\n-        # Generate features for each timestamp\r\n-        sequence_rows = []\r\n-        for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n-            features = [load_value]  # Main value first\r\n-            \r\n-            if self.config.include_time_information:\r\n-                timestamp_pd = pd.Timestamp(time_stamp)\r\n-                # Add cyclical time encodings\r\n-                features.extend([\r\n-                    np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n-                    np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n-                    np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n-                    np.cos(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n-                    np.sin(2 * np.pi * timestamp_pd.month / 12),\r\n-                    np.cos(2 * np.pi * timestamp_pd.month / 12)\r\n-                ])\r\n+            # Scale and normalize data\r\n+            if self.config.time_series_scaler:\r\n+                scaled_data = (\r\n+                    scaler.fit_transform(load_data)\r\n+                    if self.config.is_training_set\r\n+                    else scaler.transform(load_data)\r\n+                )\r\n+                scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n+            else:\r\n+                scaled_data = load_data\r\n \r\n-            sequence_rows.append(features)\r\n+            # Generate features for each timestamp\r\n\\ No newline at end of file\n+            sequence_rows = []\r\n+            for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n+                features = [load_value]  # Main value first\r\n+                \r\n+                if self.config.include_time_information:\r\n+                    timestamp_pd = pd.Timestamp(time_stamp)\r\n+                    # Add cyclical time encodings\r\n+                    features.extend([\r\n+                        np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n+                        np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n+                        np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                        np.cos(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                        np.sin(2 * np.pi * timestamp_pd.month / 12),\r\n+                        np.cos(2 * np.pi * timestamp_pd.month / 12)\r\n+                    ])\r\n \r\n-        # Convert to tensor with gradient tracking enabled\r\n-        self.rows = torch.tensor(\r\n-            np.array(sequence_rows, dtype=np.float32),\r\n-            requires_grad=True,\r\n-            dtype=torch.float32\r\n-        )\r\n+                sequence_rows.append(features)\r\n \r\n-        # Print debug information\r\n-        self.logger.debug(\"Feature generation complete\", {\r\n-            \"total_sequences\": len(self.rows),\r\n-            \"features_per_sequence\": self.rows.shape[-1],\r\n-            \"value_range\": f\"[{self.rows.min():.2f}, {self.rows.max():.2f}]\",\r\n-            \"requires_grad\": self.rows.requires_grad\r\n-        })\r\n+            # Convert to tensor with gradient tracking enabled\r\n+            self.rows = torch.tensor(\r\n+                np.array(sequence_rows, dtype=np.float32),\r\n+                requires_grad=True,\r\n+                dtype=torch.float32\r\n+            )\r\n \r\n-        # Validate tensor\r\n-        if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n-            raise ValueError(\"Invalid values detected in final tensor\")\r\n+            # Print debug information\r\n+            self.logger.debug(\"Feature generation complete\", {\r\n+                \"total_sequences\": len(self.rows),\r\n+                \"features_per_sequence\": self.rows.shape[-1],\r\n+                \"value_range\": f\"[{self.rows.min():.2f}, {self.rows.max():.2f}]\",\r\n+                \"requires_grad\": self.rows.requires_grad\r\n+            })\r\n \r\n-    except Exception as e:\r\n-        self.logger.error(\"Error preparing time series data\", {\r\n-            \"error\": str(e),\r\n-            \"dataframe_info\": str(self._df.info())\r\n-        })\r\n-        raise\n+            # Validate tensor\r\n+            if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n+                raise ValueError(\"Invalid values detected in final tensor\")\r\n+\r\n+        except Exception as e:\r\n+            self.logger.error(\"Error preparing time series data\", {\r\n+                \"error\": str(e),\r\n+                \"dataframe_info\": str(self._df.info())\r\n+            })\r\n+            raise\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733265086470,
            "name": "Commit-0",
            "content": "import numpy as np\r\nimport pandas as pd\r\nimport torch\r\nfrom typing import Tuple, List, Any\r\n\r\nfrom data_loading.base.base_dataset import BaseDataset, DatasetConfig\r\nfrom data_loading.features.time_features import CyclicalTimeFeature, WorkdayFeature\r\nfrom data_loading.preprocessing.data_scaler import DataScaler\r\nfrom data_loading.preprocessing.data_transformer import DataTransformer\r\nfrom utils.logging.logger import Logger\r\nfrom utils.logging.config import LoggerConfig, LogLevel\r\n\r\nclass TransformerDataset(BaseDataset):\r\n    \"\"\"Dataset implementation for transformer models with proper sequence handling\"\"\"\r\n\r\n    def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n        super().__init__(df, config)\r\n        self.rows: torch.Tensor = torch.empty(0)\r\n        self.logger = Logger.get_logger(__name__)\r\n        self._prepare_time_series_data()\r\n        self._debug_counter = 0\r\n        self._debug_frequency = 10000\r\n\r\n    def __len__(self) -> int:\r\n        \"\"\"\r\n        Calculate the number of available sequences in the dataset.\r\n        \r\n        Returns:\r\n            Number of sequences that can be extracted from the data\r\n        \"\"\"\r\n        # Make sure we have data\r\n        if self.rows is None or len(self.rows) == 0:\r\n            return 0\r\n            \r\n        # Calculate total sequence length needed for input and forecast\r\n        total_window = (\r\n            self.config.window_size +    # Input sequence length\r\n            self.config.horizon_size     # Forecast sequence length\r\n        )\r\n        \r\n        # Return maximum number of sequences we can create\r\n        # Subtract total_window - 1 to ensure we have enough points for the last sequence\r\n        return max(0, len(self.rows) - (total_window - 1))\r\n\r\n    def __getitem__(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\r\n        \"\"\"\r\n        Get a data item formatted for transformer training.\r\n        \r\n        Args:\r\n            index: Position in dataset to retrieve\r\n            \r\n        Returns:\r\n            ((src_seq, tgt_input_seq), target_seq):\r\n            - src_seq: Source sequence for encoder [seq_len, features]\r\n            - tgt_input_seq: Target input sequence for decoder [seq_len-1, features]\r\n            - target_seq: Target values to predict [seq_len-1, 1]\r\n        \"\"\"\r\n        if self.rows is None:\r\n            raise ValueError(\"Dataset not properly initialized\")\r\n\r\n        # Calculate window sizes\r\n        input_window = self.config.window_size\r\n        forecast_window = self.config.horizon_size\r\n\r\n        try:\r\n            # Get source sequence for encoder\r\n            src_seq = self.rows[index:index + input_window]\r\n            \r\n            # Get target sequence\r\n            full_target = self.rows[index + input_window:index + input_window + forecast_window]\r\n            \r\n            # Prepare sequences for teacher forcing:\r\n            # - Remove last timestep from target input (shifted right)\r\n            # - Remove first timestep from target output (shifted left)\r\n            tgt_input_seq = full_target[:-1]\r\n            target_seq = full_target[1:, 0:1]  # Only keep energy consumption value\r\n\r\n            # Debug information\r\n            if self._debug_counter % self._debug_frequency == 0:\r\n                self.logger.debug(\"Dataset sample info\", {\r\n                    \"sample\": self._debug_counter,\r\n                    \"source_shape\": src_seq.shape,\r\n                    \"target_input_shape\": tgt_input_seq.shape,\r\n                    \"target_shape\": target_seq.shape,\r\n                    \"source_range\": f\"[{src_seq.min():.2f}, {src_seq.max():.2f}]\",\r\n                    \"target_range\": f\"[{target_seq.min():.2f}, {target_seq.max():.2f}]\"\r\n                })\r\n            \r\n            self._debug_counter += 1\r\n\r\n            # Add batch dimension if needed\r\n            if len(src_seq.shape) == 2:\r\n                src_seq = src_seq.unsqueeze(0)\r\n                tgt_input_seq = tgt_input_seq.unsqueeze(0)\r\n                target_seq = target_seq.unsqueeze(0)\r\n\r\n            return (src_seq, tgt_input_seq), target_seq\r\n            \r\n        except Exception as e:\r\n            self.logger.error(\"Error in __getitem__\", {\r\n                \"index\": index,\r\n                \"error\": str(e),\r\n                \"dataset_size\": len(self.rows),\r\n                \"input_window\": input_window,\r\n                \"forecast_window\": forecast_window\r\n            })\r\n            raise\r\n\r\n    def _prepare_time_series_data(self) -> None:\r\n        \"\"\"Prepare time series data for transformer model input\"\"\"\r\n        self.logger.info(\"Preparing transformer dataset\", {\r\n            \"dataframe_shape\": self._df.shape\r\n        })\r\n        \r\n        if len(self._df) == 0:\r\n            raise ValueError(\"Empty dataframe provided\")\r\n\r\n        try:\r\n            # Initialize processors\r\n            scaler = DataScaler(self.config.time_series_scaler)\r\n            transformer = DataTransformer()\r\n\r\n            # Process load data\r\n            load_data = np.array(self._df[self.config.target_variable])\r\n            time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n\r\n            # Scale and normalize data\r\n            if self.config.time_series_scaler:\r\n                scaled_data = (\r\n                    scaler.fit_transform(load_data)\r\n                    if self.config.is_training_set\r\n                    else scaler.transform(load_data)\r\n                )\r\n                scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n            else:\r\n                scaled_data = load_data\r\n\r\n            # Generate features for each timestamp\r\n            sequence_rows = []\r\n            for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n                features = [load_value]  # Main value first\r\n                \r\n                if self.config.include_time_information:\r\n                    timestamp_pd = pd.Timestamp(time_stamp)\r\n                    # Add cyclical time encodings\r\n                    features.extend([\r\n                        np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n                        np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n                        np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n                        np.cos(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n                        np.sin(2 * np.pi * timestamp_pd.month / 12),\r\n                        np.cos(2 * np.pi * timestamp_pd.month / 12)\r\n                    ])\r\n\r\n                sequence_rows.append(features)\r\n\r\n            # Convert to tensor and store\r\n            self.rows = torch.tensor(\r\n                np.array(sequence_rows, dtype=np.float32), \r\n                requires_grad=True,  # Enable gradient tracking\r\n                dtype=torch.float32\r\n            )\r\n\r\n            \r\n            # Print debug information\r\n            self.logger.debug(\"Feature generation complete\", {\r\n                \"total_sequences\": len(self.rows),\r\n                \"features_per_sequence\": self.rows.shape[-1],\r\n                \"value_range\": f\"[{self.rows.min():.2f}, {self.rows.max():.2f}]\"\r\n            })\r\n\r\n            # Validate tensor\r\n            if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n                raise ValueError(\"Invalid values detected in final tensor\")\r\n\r\n        except Exception as e:\r\n            self.logger.error(\"Error preparing time series data\", {\r\n                \"error\": str(e),\r\n                \"dataframe_info\": str(self._df.info())\r\n            })\r\n            raise"
        }
    ]
}