{
    "sourceFile": "data_loading/datasets/transformer_dataset.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 13,
            "patches": [
                {
                    "date": 1733005186538,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733005260749,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,14 +35,20 @@\n \r\n         input_seq = self.rows[\r\n                     index:index + self.config.time_series_window_in_hours\r\n                     ]\r\n-\r\n+        \r\n         target_seq = self.rows[\r\n                     index + self.config.time_series_window_in_hours:\r\n                     index + self.config.time_series_window_in_hours + self.config.forecasting_horizon_in_hours\r\n                     ]\r\n \r\n+        print(f\"\\nDebug - Dataset __getitem__:\")\r\n+        print(f\"Input sequence shape: {input_seq.shape}\")\r\n+        print(f\"Target sequence shape: {target_seq.shape}\")\r\n+        print(f\"Input sequence value range: [{input_seq.min():.2f}, {input_seq.max():.2f}]\")\r\n+        print(f\"Target sequence value range: [{target_seq.min():.2f}, {target_seq.max():.2f}]\")\r\n+\r\n         return input_seq, target_seq\r\n \r\n     def _prepare_time_series_data(self) -> None:\r\n         \"\"\"Prepare time series data for model input\"\"\"\r\n"
                },
                {
                    "date": 1733005889313,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -86,12 +86,18 @@\n                     scaler.fit_transform(load_data)\r\n                     if self.config.is_training_set\r\n                     else scaler.transform(load_data)\r\n                 )\r\n-                print(f\"Debug - After scaling shape: {scaled_data.shape}\")\r\n+                # Add normalization\r\n+                scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n+                print(f\"Debug - After normalization range: [{scaled_data.min():.2f}, {scaled_data.max():.2f}]\")\r\n             else:\r\n                 scaled_data = load_data\r\n \r\n+            # Add validation check\r\n+            if np.isnan(scaled_data).any() or np.isinf(scaled_data).any():\r\n+                raise ValueError(\"Invalid values detected after scaling\")\r\n+\r\n             # Extract timestamps and prepare features\r\n             time_stamps = transformer.extract_timestamps(\r\n                 self._df,\r\n                 self.config.time_variable\r\n@@ -120,8 +126,14 @@\n \r\n             self.rows = torch.tensor(np.array(sequence_rows, dtype=np.float32))\r\n             print(f\"Debug - Final rows tensor shape: {self.rows.shape}\")\r\n \r\n+            # Add final validation\r\n+            if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n+                raise ValueError(\"Invalid values detected in final tensor\")\r\n+            \r\n+            print(f\"Debug - Final value range: [{self.rows.min():.2f}, {self.rows.max():.2f}]\")\r\n+\r\n             # Set input and target tensors\r\n             self.prepared_time_series_input = self.rows[\r\n                 :len(self.rows) - self.config.forecasting_horizon_in_hours\r\n             ]\r\n"
                },
                {
                    "date": 1733007333571,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,157 @@\n+# data_loading/datasets/transformer_dataset.py\r\n+import numpy as np\r\n+import pandas as pd\r\n+import torch\r\n+from typing import Tuple, List, Any\r\n+\r\n+from data_loading.base.base_dataset import BaseDataset, DatasetConfig\r\n+from data_loading.features.time_features import CyclicalTimeFeature, WorkdayFeature\r\n+from data_loading.preprocessing.data_scaler import DataScaler\r\n+from data_loading.preprocessing.data_transformer import DataTransformer\r\n+\r\n+class TransformerDataset(BaseDataset):\r\n+    \"\"\"Dataset implementation for transformer models\"\"\"\r\n+\r\n+    def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n+        super().__init__(df, config)\r\n+        self.rows: torch.Tensor = torch.empty(0)\r\n+        self._prepare_time_series_data()\r\n+        self._debug_counter = 0  # Add debug counter\r\n+        self._debug_frequency = 1000  # Print debug info every 1000 samples\r\n+\r\n+    def __len__(self) -> int:\r\n+        if self.rows is None:\r\n+            return 0\r\n+        return max(0, len(self.rows) - self.config.time_series_window_in_hours - self.config.forecasting_horizon_in_hours)\r\n+\r\n+    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\r\n+        \"\"\"Get a data item by index\r\n+\r\n+        Returns:\r\n+            Tuple containing:\r\n+            - Input sequence [time_series_window_in_hours, features]\r\n+            - Target sequence [forecasting_horizon_in_hours, features]\r\n+        \"\"\"\r\n+        if self.rows is None:\r\n+            raise ValueError(\"Dataset not properly initialized\")\r\n+\r\n+        input_seq = self.rows[\r\n+                    index:index + self.config.time_series_window_in_hours\r\n+                    ]\r\n+        \r\n+        target_seq = self.rows[\r\n+                    index + self.config.time_series_window_in_hours:\r\n+                    index + self.config.time_series_window_in_hours + self.config.forecasting_horizon_in_hours\r\n+                    ]\r\n+\r\n+        # Print debug info periodically\r\n+        if self._debug_counter % self._debug_frequency == 0:\r\n+            print(f\"\\nDebug - Dataset __getitem__ (sample {self._debug_counter}):\")\r\n+            print(f\"Input sequence shape: {input_seq.shape}\")\r\n+            print(f\"Target sequence shape: {target_seq.shape}\")\r\n+            print(f\"Input sequence value range: [{input_seq.min():.2f}, {input_seq.max():.2f}]\")\r\n+            print(f\"Target sequence value range: [{target_seq.min():.2f}, {target_seq.max():.2f}]\")\r\n+        \r\n+        self._debug_counter += 1\r\n+        return input_seq, target_seq\r\n+\r\n+    def _prepare_time_series_data(self) -> None:\r\n+        \"\"\"Prepare time series data for model input\"\"\"\r\n+        print(f\"\\nDebug - TransformerDataset preparation:\")\r\n+        print(f\"Initial DataFrame shape: {self._df.shape}\")\r\n+        \r\n+        if len(self._df) == 0:\r\n+            raise ValueError(\"Empty dataframe provided\")\r\n+\r\n+        # Initialize processors\r\n+        scaler = DataScaler(self.config.time_series_scaler)\r\n+        transformer = DataTransformer()\r\n+\r\n+        # Make sure we have enough data points\r\n+        min_required = (self.config.time_series_window_in_hours + \r\n+                    self.config.forecasting_horizon_in_hours) * 4  # 4 points per hour\r\n+        print(f\"Debug - Required data points: {min_required}\")\r\n+        print(f\"Debug - Available data points: {len(self._df)}\")\r\n+        \r\n+        if len(self._df) < min_required:\r\n+            raise ValueError(f\"Not enough data points. Need at least {min_required}, got {len(self._df)}\")\r\n+\r\n+        try:\r\n+            # Extract raw data\r\n+            raw_data = np.array(self._df[self.config.target_variable])\r\n+            print(f\"Debug - Raw data shape: {raw_data.shape}\")\r\n+            \r\n+            # Average data\r\n+            load_data = transformer.average_by_window(raw_data, window_size=4)\r\n+            print(f\"Debug - After averaging shape: {load_data.shape}\")\r\n+            \r\n+            # Scale data\r\n+            if self.config.time_series_scaler:\r\n+                scaled_data = (\r\n+                    scaler.fit_transform(load_data)\r\n+                    if self.config.is_training_set\r\n+                    else scaler.transform(load_data)\r\n+                )\r\n+                # Add normalization\r\n+                scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n+                print(f\"Debug - After normalization range: [{scaled_data.min():.2f}, {scaled_data.max():.2f}]\")\r\n+            else:\r\n+                scaled_data = load_data\r\n+\r\n+            # Add validation check\r\n+            if np.isnan(scaled_data).any() or np.isinf(scaled_data).any():\r\n+                raise ValueError(\"Invalid values detected after scaling\")\r\n+\r\n+            # Extract timestamps and prepare features\r\n+            time_stamps = transformer.extract_timestamps(\r\n+                self._df,\r\n+                self.config.time_variable\r\n+            )\r\n+            print(f\"Debug - Time stamps shape: {time_stamps.shape}\")\r\n+\r\n+            # Initialize feature generators\r\n+            hour_feature = CyclicalTimeFeature(24)\r\n+            week_feature = CyclicalTimeFeature(53)\r\n+            workday_feature = WorkdayFeature()\r\n+\r\n+            sequence_rows = []\r\n+\r\n+            # Generate features for each timestamp\r\n+            print(f\"Debug - Generating features for {len(scaled_data)} time points\")\r\n+            for idx, (load_value, time_stamp) in enumerate(zip(scaled_data, time_stamps)):\r\n+                features = [load_value]\r\n+\r\n+                if self.config.include_time_information:\r\n+                    timestamp_pd = pd.Timestamp(time_stamp)\r\n+                    features.extend(hour_feature.generate(pd.Series([timestamp_pd.hour])))\r\n+                    features.extend(week_feature.generate(pd.Series([timestamp_pd.isocalendar()[1]])))\r\n+                    features.extend(workday_feature.generate(timestamp_pd))\r\n+\r\n+                sequence_rows.append(features)\r\n+\r\n+            self.rows = torch.tensor(np.array(sequence_rows, dtype=np.float32))\r\n+            print(f\"Debug - Final rows tensor shape: {self.rows.shape}\")\r\n+\r\n+            # Add final validation\r\n+            if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n+                raise ValueError(\"Invalid values detected in final tensor\")\r\n+            \r\n+            print(f\"Debug - Final value range: [{self.rows.min():.2f}, {self.rows.max():.2f}]\")\r\n+\r\n+            # Set input and target tensors\r\n+            self.prepared_time_series_input = self.rows[\r\n+                :len(self.rows) - self.config.forecasting_horizon_in_hours\r\n+            ]\r\n+            self.prepared_time_series_target = self.rows[\r\n+                self.config.time_series_window_in_hours:\r\n+            ]\r\n+\r\n+            print(f\"Debug - Final shapes:\")\r\n+            print(f\"  Input tensor: {self.prepared_time_series_input.shape if self.prepared_time_series_input is not None else None}\")\r\n+            print(f\"  Target tensor: {self.prepared_time_series_target.shape if self.prepared_time_series_target is not None else None}\")\r\n+\r\n+        except Exception as e:\r\n+            print(f\"\\nError preparing time series data: {str(e)}\")\r\n+            print(f\"DataFrame info:\")\r\n+            print(self._df.info())\r\n+            raise\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733007398310,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,9 +16,9 @@\n         super().__init__(df, config)\r\n         self.rows: torch.Tensor = torch.empty(0)\r\n         self._prepare_time_series_data()\r\n         self._debug_counter = 0  # Add debug counter\r\n-        self._debug_frequency = 1000  # Print debug info every 1000 samples\r\n+        self._debug_frequency = 10000  # Print debug info every 1000 samples\r\n \r\n     def __len__(self) -> int:\r\n         if self.rows is None:\r\n             return 0\r\n@@ -153,157 +153,5 @@\n         except Exception as e:\r\n             print(f\"\\nError preparing time series data: {str(e)}\")\r\n             print(f\"DataFrame info:\")\r\n             print(self._df.info())\r\n-            raise\n-# data_loading/datasets/transformer_dataset.py\r\n-import numpy as np\r\n-import pandas as pd\r\n-import torch\r\n-from typing import Tuple, List, Any\r\n-\r\n-from data_loading.base.base_dataset import BaseDataset, DatasetConfig\r\n-from data_loading.features.time_features import CyclicalTimeFeature, WorkdayFeature\r\n-from data_loading.preprocessing.data_scaler import DataScaler\r\n-from data_loading.preprocessing.data_transformer import DataTransformer\r\n-\r\n-class TransformerDataset(BaseDataset):\r\n-    \"\"\"Dataset implementation for transformer models\"\"\"\r\n-\r\n-    def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n-        super().__init__(df, config)\r\n-        self.rows: torch.Tensor = torch.empty(0)\r\n-        self._prepare_time_series_data()\r\n-\r\n-    def __len__(self) -> int:\r\n-        if self.rows is None:\r\n-            return 0\r\n-        return max(0, len(self.rows) - self.config.time_series_window_in_hours - self.config.forecasting_horizon_in_hours)\r\n-\r\n-    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\r\n-        \"\"\"Get a data item by index\r\n-\r\n-        Returns:\r\n-            Tuple containing:\r\n-            - Input sequence [time_series_window_in_hours, features]\r\n-            - Target sequence [forecasting_horizon_in_hours, features]\r\n-        \"\"\"\r\n-        if self.rows is None:\r\n-            raise ValueError(\"Dataset not properly initialized\")\r\n-\r\n-        input_seq = self.rows[\r\n-                    index:index + self.config.time_series_window_in_hours\r\n-                    ]\r\n-        \r\n-        target_seq = self.rows[\r\n-                    index + self.config.time_series_window_in_hours:\r\n-                    index + self.config.time_series_window_in_hours + self.config.forecasting_horizon_in_hours\r\n-                    ]\r\n-\r\n-        print(f\"\\nDebug - Dataset __getitem__:\")\r\n-        print(f\"Input sequence shape: {input_seq.shape}\")\r\n-        print(f\"Target sequence shape: {target_seq.shape}\")\r\n-        print(f\"Input sequence value range: [{input_seq.min():.2f}, {input_seq.max():.2f}]\")\r\n-        print(f\"Target sequence value range: [{target_seq.min():.2f}, {target_seq.max():.2f}]\")\r\n-\r\n-        return input_seq, target_seq\r\n-\r\n-    def _prepare_time_series_data(self) -> None:\r\n-        \"\"\"Prepare time series data for model input\"\"\"\r\n-        print(f\"\\nDebug - TransformerDataset preparation:\")\r\n-        print(f\"Initial DataFrame shape: {self._df.shape}\")\r\n-        \r\n-        if len(self._df) == 0:\r\n-            raise ValueError(\"Empty dataframe provided\")\r\n-\r\n-        # Initialize processors\r\n-        scaler = DataScaler(self.config.time_series_scaler)\r\n-        transformer = DataTransformer()\r\n-\r\n-        # Make sure we have enough data points\r\n-        min_required = (self.config.time_series_window_in_hours + \r\n-                    self.config.forecasting_horizon_in_hours) * 4  # 4 points per hour\r\n-        print(f\"Debug - Required data points: {min_required}\")\r\n-        print(f\"Debug - Available data points: {len(self._df)}\")\r\n-        \r\n-        if len(self._df) < min_required:\r\n-            raise ValueError(f\"Not enough data points. Need at least {min_required}, got {len(self._df)}\")\r\n-\r\n-        try:\r\n-            # Extract raw data\r\n-            raw_data = np.array(self._df[self.config.target_variable])\r\n-            print(f\"Debug - Raw data shape: {raw_data.shape}\")\r\n-            \r\n-            # Average data\r\n-            load_data = transformer.average_by_window(raw_data, window_size=4)\r\n-            print(f\"Debug - After averaging shape: {load_data.shape}\")\r\n-            \r\n-            # Scale data\r\n-            if self.config.time_series_scaler:\r\n-                scaled_data = (\r\n-                    scaler.fit_transform(load_data)\r\n-                    if self.config.is_training_set\r\n-                    else scaler.transform(load_data)\r\n-                )\r\n-                # Add normalization\r\n-                scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n-                print(f\"Debug - After normalization range: [{scaled_data.min():.2f}, {scaled_data.max():.2f}]\")\r\n-            else:\r\n-                scaled_data = load_data\r\n-\r\n-            # Add validation check\r\n-            if np.isnan(scaled_data).any() or np.isinf(scaled_data).any():\r\n-                raise ValueError(\"Invalid values detected after scaling\")\r\n-\r\n-            # Extract timestamps and prepare features\r\n-            time_stamps = transformer.extract_timestamps(\r\n-                self._df,\r\n-                self.config.time_variable\r\n-            )\r\n-            print(f\"Debug - Time stamps shape: {time_stamps.shape}\")\r\n-\r\n-            # Initialize feature generators\r\n-            hour_feature = CyclicalTimeFeature(24)\r\n-            week_feature = CyclicalTimeFeature(53)\r\n-            workday_feature = WorkdayFeature()\r\n-\r\n-            sequence_rows = []\r\n-\r\n-            # Generate features for each timestamp\r\n-            print(f\"Debug - Generating features for {len(scaled_data)} time points\")\r\n-            for idx, (load_value, time_stamp) in enumerate(zip(scaled_data, time_stamps)):\r\n-                features = [load_value]\r\n-\r\n-                if self.config.include_time_information:\r\n-                    timestamp_pd = pd.Timestamp(time_stamp)\r\n-                    features.extend(hour_feature.generate(pd.Series([timestamp_pd.hour])))\r\n-                    features.extend(week_feature.generate(pd.Series([timestamp_pd.isocalendar()[1]])))\r\n-                    features.extend(workday_feature.generate(timestamp_pd))\r\n-\r\n-                sequence_rows.append(features)\r\n-\r\n-            self.rows = torch.tensor(np.array(sequence_rows, dtype=np.float32))\r\n-            print(f\"Debug - Final rows tensor shape: {self.rows.shape}\")\r\n-\r\n-            # Add final validation\r\n-            if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n-                raise ValueError(\"Invalid values detected in final tensor\")\r\n-            \r\n-            print(f\"Debug - Final value range: [{self.rows.min():.2f}, {self.rows.max():.2f}]\")\r\n-\r\n-            # Set input and target tensors\r\n-            self.prepared_time_series_input = self.rows[\r\n-                :len(self.rows) - self.config.forecasting_horizon_in_hours\r\n-            ]\r\n-            self.prepared_time_series_target = self.rows[\r\n-                self.config.time_series_window_in_hours:\r\n-            ]\r\n-\r\n-            print(f\"Debug - Final shapes:\")\r\n-            print(f\"  Input tensor: {self.prepared_time_series_input.shape if self.prepared_time_series_input is not None else None}\")\r\n-            print(f\"  Target tensor: {self.prepared_time_series_target.shape if self.prepared_time_series_target is not None else None}\")\r\n-\r\n-        except Exception as e:\r\n-            print(f\"\\nError preparing time series data: {str(e)}\")\r\n-            print(f\"DataFrame info:\")\r\n-            print(self._df.info())\r\n             raise\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733061971740,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,8 @@\n             raise ValueError(\"Empty dataframe provided\")\r\n \r\n         # Initialize processors\r\n         scaler = DataScaler(self.config.time_series_scaler)\r\n-        transformer = DataTransformer()\r\n \r\n         # Make sure we have enough data points\r\n         min_required = (self.config.time_series_window_in_hours + \r\n                     self.config.forecasting_horizon_in_hours) * 4  # 4 points per hour\r\n@@ -76,16 +75,12 @@\n         if len(self._df) < min_required:\r\n             raise ValueError(f\"Not enough data points. Need at least {min_required}, got {len(self._df)}\")\r\n \r\n         try:\r\n-            # Extract raw data\r\n-            raw_data = np.array(self._df[self.config.target_variable])\r\n-            print(f\"Debug - Raw data shape: {raw_data.shape}\")\r\n+            # Extract raw data (already in 15-min intervals)\r\n+            load_data = np.array(self._df[self.config.target_variable])\r\n+            print(f\"Debug - Load data shape: {load_data.shape}\")\r\n             \r\n-            # Average data\r\n-            load_data = transformer.average_by_window(raw_data, window_size=4)\r\n-            print(f\"Debug - After averaging shape: {load_data.shape}\")\r\n-            \r\n             # Scale data\r\n             if self.config.time_series_scaler:\r\n                 scaled_data = (\r\n                     scaler.fit_transform(load_data)\r\n"
                },
                {
                    "date": 1733062042304,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,8 +64,9 @@\n             raise ValueError(\"Empty dataframe provided\")\r\n \r\n         # Initialize processors\r\n         scaler = DataScaler(self.config.time_series_scaler)\r\n+        transformer = DataTransformer()  # Add this line to instantiate DataTransformer\r\n \r\n         # Make sure we have enough data points\r\n         min_required = (self.config.time_series_window_in_hours + \r\n                     self.config.forecasting_horizon_in_hours) * 4  # 4 points per hour\r\n@@ -96,9 +97,9 @@\n             # Add validation check\r\n             if np.isnan(scaled_data).any() or np.isinf(scaled_data).any():\r\n                 raise ValueError(\"Invalid values detected after scaling\")\r\n \r\n-            # Extract timestamps and prepare features\r\n+            # Extract timestamps and prepare features using the instantiated transformer\r\n             time_stamps = transformer.extract_timestamps(\r\n                 self._df,\r\n                 self.config.time_variable\r\n             )\r\n"
                },
                {
                    "date": 1733062194863,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,31 +21,31 @@\n \r\n     def __len__(self) -> int:\r\n         if self.rows is None:\r\n             return 0\r\n-        return max(0, len(self.rows) - self.config.time_series_window_in_hours - self.config.forecasting_horizon_in_hours)\r\n+        total_window = (self.config.time_series_window_in_hours + \r\n+                       self.config.forecasting_horizon_in_hours) * self.config.points_per_hour\r\n+        return max(0, len(self.rows) - total_window)\r\n \r\n     def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\r\n-        \"\"\"Get a data item by index\r\n-\r\n-        Returns:\r\n-            Tuple containing:\r\n-            - Input sequence [time_series_window_in_hours, features]\r\n-            - Target sequence [forecasting_horizon_in_hours, features]\r\n-        \"\"\"\r\n+        \"\"\"Get a data item by index\"\"\"\r\n         if self.rows is None:\r\n             raise ValueError(\"Dataset not properly initialized\")\r\n \r\n+        # Adjust window sizes to account for 15-minute intervals\r\n+        input_window = self.config.time_series_window_in_hours * self.config.points_per_hour\r\n+        forecast_window = self.config.forecasting_horizon_in_hours * self.config.points_per_hour\r\n+\r\n         input_seq = self.rows[\r\n-                    index:index + self.config.time_series_window_in_hours\r\n+                    index:index + input_window\r\n                     ]\r\n         \r\n         target_seq = self.rows[\r\n-                    index + self.config.time_series_window_in_hours:\r\n-                    index + self.config.time_series_window_in_hours + self.config.forecasting_horizon_in_hours\r\n+                    index + input_window:\r\n+                    index + input_window + forecast_window\r\n                     ]\r\n \r\n-        # Print debug info periodically\r\n+        # Debug info\r\n         if self._debug_counter % self._debug_frequency == 0:\r\n             print(f\"\\nDebug - Dataset __getitem__ (sample {self._debug_counter}):\")\r\n             print(f\"Input sequence shape: {input_seq.shape}\")\r\n             print(f\"Target sequence shape: {target_seq.shape}\")\r\n"
                },
                {
                    "date": 1733101635650,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,27 +34,25 @@\n         # Adjust window sizes to account for 15-minute intervals\r\n         input_window = self.config.time_series_window_in_hours * self.config.points_per_hour\r\n         forecast_window = self.config.forecasting_horizon_in_hours * self.config.points_per_hour\r\n \r\n-        input_seq = self.rows[\r\n-                    index:index + input_window\r\n-                    ]\r\n-        \r\n-        target_seq = self.rows[\r\n-                    index + input_window:\r\n-                    index + input_window + forecast_window\r\n-                    ]\r\n+        # Get full feature sequences\r\n+        input_seq = self.rows[index:index + input_window]\r\n+        target_seq = self.rows[index + input_window:index + input_window + forecast_window]\r\n \r\n+        # Extract energy consumption value (first feature) for target\r\n+        target_values = target_seq[:, 0:1]  # Shape: [seq_len, 1]\r\n+\r\n         # Debug info\r\n         if self._debug_counter % self._debug_frequency == 0:\r\n             print(f\"\\nDebug - Dataset __getitem__ (sample {self._debug_counter}):\")\r\n-            print(f\"Input sequence shape: {input_seq.shape}\")\r\n-            print(f\"Target sequence shape: {target_seq.shape}\")\r\n+            print(f\"Input sequence shape: {input_seq.shape}\")  # Should be [96, 10]\r\n+            print(f\"Target sequence shape: {target_values.shape}\")  # Should be [48, 1]\r\n             print(f\"Input sequence value range: [{input_seq.min():.2f}, {input_seq.max():.2f}]\")\r\n-            print(f\"Target sequence value range: [{target_seq.min():.2f}, {target_seq.max():.2f}]\")\r\n+            print(f\"Target sequence value range: [{target_values.min():.2f}, {target_values.max():.2f}]\")\r\n         \r\n         self._debug_counter += 1\r\n-        return input_seq, target_seq\r\n+        return input_seq, target_values  # Return all features for input, but only energy consumption for target\r\n \r\n     def _prepare_time_series_data(self) -> None:\r\n         \"\"\"Prepare time series data for model input\"\"\"\r\n         print(f\"\\nDebug - TransformerDataset preparation:\")\r\n"
                },
                {
                    "date": 1733180683887,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,11 +30,11 @@\n         \"\"\"Get a data item by index\"\"\"\r\n         if self.rows is None:\r\n             raise ValueError(\"Dataset not properly initialized\")\r\n \r\n-        # Adjust window sizes to account for 15-minute intervals\r\n-        input_window = self.config.time_series_window_in_hours * self.config.points_per_hour\r\n-        forecast_window = self.config.forecasting_horizon_in_hours * self.config.points_per_hour\r\n+        # Calculate window sizes in data points\r\n+        input_window = self.config.window_size  # Use window_size property\r\n+        forecast_window = self.config.horizon_size  # Use horizon_size property\r\n \r\n         # Get full feature sequences\r\n         input_seq = self.rows[index:index + input_window]\r\n         target_seq = self.rows[index + input_window:index + input_window + forecast_window]\r\n@@ -64,11 +64,10 @@\n         # Initialize processors\r\n         scaler = DataScaler(self.config.time_series_scaler)\r\n         transformer = DataTransformer()  # Add this line to instantiate DataTransformer\r\n \r\n-        # Make sure we have enough data points\r\n-        min_required = (self.config.time_series_window_in_hours + \r\n-                    self.config.forecasting_horizon_in_hours) * 4  # 4 points per hour\r\n+        # Calculate required points based on window sizes\r\n+        min_required = self.config.window_size + self.config.horizon_size\r\n         print(f\"Debug - Required data points: {min_required}\")\r\n         print(f\"Debug - Available data points: {len(self._df)}\")\r\n         \r\n         if len(self._df) < min_required:\r\n"
                },
                {
                    "date": 1733182619024,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,4 @@\n-# data_loading/datasets/transformer_dataset.py\r\n import numpy as np\r\n import pandas as pd\r\n import torch\r\n from typing import Tuple, List, Any\r\n@@ -9,94 +8,112 @@\n from data_loading.preprocessing.data_scaler import DataScaler\r\n from data_loading.preprocessing.data_transformer import DataTransformer\r\n \r\n class TransformerDataset(BaseDataset):\r\n-    \"\"\"Dataset implementation for transformer models\"\"\"\r\n+    \"\"\"Dataset implementation for transformer models with proper sequence handling\"\"\"\r\n \r\n     def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n         super().__init__(df, config)\r\n         self.rows: torch.Tensor = torch.empty(0)\r\n         self._prepare_time_series_data()\r\n-        self._debug_counter = 0  # Add debug counter\r\n-        self._debug_frequency = 10000  # Print debug info every 1000 samples\r\n+        self._debug_counter = 0\r\n+        self._debug_frequency = 10000\r\n \r\n     def __len__(self) -> int:\r\n         if self.rows is None:\r\n             return 0\r\n+        # Calculate total window considering both input and forecast windows\r\n         total_window = (self.config.time_series_window_in_hours + \r\n                        self.config.forecasting_horizon_in_hours) * self.config.points_per_hour\r\n         return max(0, len(self.rows) - total_window)\r\n \r\n-    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\r\n-        \"\"\"Get a data item by index\"\"\"\r\n+    def __getitem__(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\r\n+        \"\"\"\r\n+        Get a data item formatted for transformer training.\r\n+        \r\n+        Returns:\r\n+            ((src_seq, tgt_input_seq), target_seq) where:\r\n+            - src_seq: Input sequence for encoder\r\n+            - tgt_input_seq: Target sequence for decoder (shifted right for teacher forcing)\r\n+            - target_seq: Actual target values to predict\r\n+        \"\"\"\r\n         if self.rows is None:\r\n             raise ValueError(\"Dataset not properly initialized\")\r\n \r\n-        # Calculate window sizes in data points\r\n-        input_window = self.config.window_size  # Use window_size property\r\n-        forecast_window = self.config.horizon_size  # Use horizon_size property\r\n+        # Calculate window sizes\r\n+        input_window = self.config.window_size\r\n+        forecast_window = self.config.horizon_size\r\n \r\n-        # Get full feature sequences\r\n-        input_seq = self.rows[index:index + input_window]\r\n-        target_seq = self.rows[index + input_window:index + input_window + forecast_window]\r\n+        # Get source sequence for encoder\r\n+        src_seq = self.rows[index:index + input_window]\r\n+        \r\n+        # Get target sequence\r\n+        full_target = self.rows[index + input_window:index + input_window + forecast_window]\r\n+        \r\n+        # Create sequences for teacher forcing:\r\n+        # - tgt_input is all but the last timestep (input to decoder)\r\n+        # - target is all but the first timestep (what we want to predict)\r\n+        tgt_input_seq = full_target[:-1]\r\n+        target_seq = full_target[1:, 0:1]  # Only take energy consumption value for target\r\n \r\n-        # Extract energy consumption value (first feature) for target\r\n-        target_values = target_seq[:, 0:1]  # Shape: [seq_len, 1]\r\n-\r\n-        # Debug info\r\n+        # Debug information\r\n         if self._debug_counter % self._debug_frequency == 0:\r\n             print(f\"\\nDebug - Dataset __getitem__ (sample {self._debug_counter}):\")\r\n-            print(f\"Input sequence shape: {input_seq.shape}\")  # Should be [96, 10]\r\n-            print(f\"Target sequence shape: {target_values.shape}\")  # Should be [48, 1]\r\n-            print(f\"Input sequence value range: [{input_seq.min():.2f}, {input_seq.max():.2f}]\")\r\n-            print(f\"Target sequence value range: [{target_values.min():.2f}, {target_values.max():.2f}]\")\r\n+            print(f\"Source sequence shape: {src_seq.shape}\")\r\n+            print(f\"Target input sequence shape: {tgt_input_seq.shape}\")\r\n+            print(f\"Target sequence shape: {target_seq.shape}\")\r\n+            print(f\"Source sequence range: [{src_seq.min():.2f}, {src_seq.max():.2f}]\")\r\n+            print(f\"Target sequence range: [{target_seq.min():.2f}, {target_seq.max():.2f}]\")\r\n         \r\n         self._debug_counter += 1\r\n-        return input_seq, target_values  # Return all features for input, but only energy consumption for target\r\n+        \r\n+        # Return format that transformer models expect:\r\n+        # ((encoder_input, decoder_input), target)\r\n+        return (src_seq, tgt_input_seq), target_seq\r\n \r\n     def _prepare_time_series_data(self) -> None:\r\n-        \"\"\"Prepare time series data for model input\"\"\"\r\n+        \"\"\"Prepare time series data with enhanced feature generation\"\"\"\r\n         print(f\"\\nDebug - TransformerDataset preparation:\")\r\n         print(f\"Initial DataFrame shape: {self._df.shape}\")\r\n         \r\n         if len(self._df) == 0:\r\n             raise ValueError(\"Empty dataframe provided\")\r\n \r\n-        # Initialize processors\r\n-        scaler = DataScaler(self.config.time_series_scaler)\r\n-        transformer = DataTransformer()  # Add this line to instantiate DataTransformer\r\n+        try:\r\n+            # Initialize processors\r\n+            scaler = DataScaler(self.config.time_series_scaler)\r\n+            transformer = DataTransformer()\r\n \r\n-        # Calculate required points based on window sizes\r\n-        min_required = self.config.window_size + self.config.horizon_size\r\n-        print(f\"Debug - Required data points: {min_required}\")\r\n-        print(f\"Debug - Available data points: {len(self._df)}\")\r\n-        \r\n-        if len(self._df) < min_required:\r\n-            raise ValueError(f\"Not enough data points. Need at least {min_required}, got {len(self._df)}\")\r\n+            # Calculate required points and validate data size\r\n+            min_required = self.config.window_size + self.config.horizon_size\r\n+            print(f\"Debug - Required data points: {min_required}\")\r\n+            print(f\"Debug - Available data points: {len(self._df)}\")\r\n+            \r\n+            if len(self._df) < min_required:\r\n+                raise ValueError(f\"Not enough data points. Need at least {min_required}, got {len(self._df)}\")\r\n \r\n-        try:\r\n-            # Extract raw data (already in 15-min intervals)\r\n+            # Process load data\r\n             load_data = np.array(self._df[self.config.target_variable])\r\n             print(f\"Debug - Load data shape: {load_data.shape}\")\r\n             \r\n-            # Scale data\r\n+            # Scale and normalize data\r\n             if self.config.time_series_scaler:\r\n                 scaled_data = (\r\n                     scaler.fit_transform(load_data)\r\n                     if self.config.is_training_set\r\n                     else scaler.transform(load_data)\r\n                 )\r\n-                # Add normalization\r\n+                # Add normalization for better training stability\r\n                 scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n                 print(f\"Debug - After normalization range: [{scaled_data.min():.2f}, {scaled_data.max():.2f}]\")\r\n             else:\r\n                 scaled_data = load_data\r\n \r\n-            # Add validation check\r\n+            # Validate scaled data\r\n             if np.isnan(scaled_data).any() or np.isinf(scaled_data).any():\r\n                 raise ValueError(\"Invalid values detected after scaling\")\r\n \r\n-            # Extract timestamps and prepare features using the instantiated transformer\r\n+            # Generate time-based features\r\n             time_stamps = transformer.extract_timestamps(\r\n                 self._df,\r\n                 self.config.time_variable\r\n             )\r\n@@ -107,13 +124,13 @@\n             week_feature = CyclicalTimeFeature(53)\r\n             workday_feature = WorkdayFeature()\r\n \r\n             sequence_rows = []\r\n-\r\n+            print(f\"Debug - Generating features for {len(scaled_data)} time points\")\r\n+            \r\n             # Generate features for each timestamp\r\n-            print(f\"Debug - Generating features for {len(scaled_data)} time points\")\r\n-            for idx, (load_value, time_stamp) in enumerate(zip(scaled_data, time_stamps)):\r\n-                features = [load_value]\r\n+            for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n+                features = [load_value]  # Start with the main value\r\n \r\n                 if self.config.include_time_information:\r\n                     timestamp_pd = pd.Timestamp(time_stamp)\r\n                     features.extend(hour_feature.generate(pd.Series([timestamp_pd.hour])))\r\n@@ -121,23 +138,24 @@\n                     features.extend(workday_feature.generate(timestamp_pd))\r\n \r\n                 sequence_rows.append(features)\r\n \r\n+            # Convert to tensor\r\n             self.rows = torch.tensor(np.array(sequence_rows, dtype=np.float32))\r\n             print(f\"Debug - Final rows tensor shape: {self.rows.shape}\")\r\n \r\n-            # Add final validation\r\n+            # Validate final tensor\r\n             if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n                 raise ValueError(\"Invalid values detected in final tensor\")\r\n             \r\n             print(f\"Debug - Final value range: [{self.rows.min():.2f}, {self.rows.max():.2f}]\")\r\n \r\n-            # Set input and target tensors\r\n+            # Set input and target tensors for reference\r\n             self.prepared_time_series_input = self.rows[\r\n-                :len(self.rows) - self.config.forecasting_horizon_in_hours\r\n+                :len(self.rows) - self.config.horizon_size\r\n             ]\r\n             self.prepared_time_series_target = self.rows[\r\n-                self.config.time_series_window_in_hours:\r\n+                self.config.window_size:\r\n             ]\r\n \r\n             print(f\"Debug - Final shapes:\")\r\n             print(f\"  Input tensor: {self.prepared_time_series_input.shape if self.prepared_time_series_input is not None else None}\")\r\n"
                },
                {
                    "date": 1733182767532,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,34 +8,26 @@\n from data_loading.preprocessing.data_scaler import DataScaler\r\n from data_loading.preprocessing.data_transformer import DataTransformer\r\n \r\n class TransformerDataset(BaseDataset):\r\n-    \"\"\"Dataset implementation for transformer models with proper sequence handling\"\"\"\r\n+    \"\"\"Dataset implementation for transformer models with proper dimension handling\"\"\"\r\n \r\n     def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n         super().__init__(df, config)\r\n         self.rows: torch.Tensor = torch.empty(0)\r\n         self._prepare_time_series_data()\r\n         self._debug_counter = 0\r\n         self._debug_frequency = 10000\r\n \r\n-    def __len__(self) -> int:\r\n-        if self.rows is None:\r\n-            return 0\r\n-        # Calculate total window considering both input and forecast windows\r\n-        total_window = (self.config.time_series_window_in_hours + \r\n-                       self.config.forecasting_horizon_in_hours) * self.config.points_per_hour\r\n-        return max(0, len(self.rows) - total_window)\r\n-\r\n     def __getitem__(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\r\n         \"\"\"\r\n-        Get a data item formatted for transformer training.\r\n+        Get a data item formatted for transformer training with proper dimensions.\r\n         \r\n         Returns:\r\n-            ((src_seq, tgt_input_seq), target_seq) where:\r\n-            - src_seq: Input sequence for encoder\r\n-            - tgt_input_seq: Target sequence for decoder (shifted right for teacher forcing)\r\n-            - target_seq: Actual target values to predict\r\n+            ((src_seq, tgt_input_seq), target_seq):\r\n+            - src_seq: Input sequence [seq_len, features]\r\n+            - tgt_input_seq: Target input [seq_len-1, features]\r\n+            - target_seq: Target values [seq_len-1, 1]\r\n         \"\"\"\r\n         if self.rows is None:\r\n             raise ValueError(\"Dataset not properly initialized\")\r\n \r\n@@ -48,15 +40,12 @@\n         \r\n         # Get target sequence\r\n         full_target = self.rows[index + input_window:index + input_window + forecast_window]\r\n         \r\n-        # Create sequences for teacher forcing:\r\n-        # - tgt_input is all but the last timestep (input to decoder)\r\n-        # - target is all but the first timestep (what we want to predict)\r\n-        tgt_input_seq = full_target[:-1]\r\n-        target_seq = full_target[1:, 0:1]  # Only take energy consumption value for target\r\n-\r\n-        # Debug information\r\n+        # Prepare sequences for teacher forcing\r\n+        tgt_input_seq = full_target[:-1]  # Remove last timestep for teacher forcing input\r\n+        target_seq = full_target[1:, 0:1]  # Only energy consumption values, shifted by 1\r\n+        \r\n         if self._debug_counter % self._debug_frequency == 0:\r\n             print(f\"\\nDebug - Dataset __getitem__ (sample {self._debug_counter}):\")\r\n             print(f\"Source sequence shape: {src_seq.shape}\")\r\n             print(f\"Target input sequence shape: {tgt_input_seq.shape}\")\r\n@@ -64,15 +53,19 @@\n             print(f\"Source sequence range: [{src_seq.min():.2f}, {src_seq.max():.2f}]\")\r\n             print(f\"Target sequence range: [{target_seq.min():.2f}, {target_seq.max():.2f}]\")\r\n         \r\n         self._debug_counter += 1\r\n+\r\n+        # Add batch dimension if needed\r\n+        if len(src_seq.shape) == 2:\r\n+            src_seq = src_seq.unsqueeze(0)\r\n+            tgt_input_seq = tgt_input_seq.unsqueeze(0)\r\n+            target_seq = target_seq.unsqueeze(0)\r\n         \r\n-        # Return format that transformer models expect:\r\n-        # ((encoder_input, decoder_input), target)\r\n         return (src_seq, tgt_input_seq), target_seq\r\n \r\n     def _prepare_time_series_data(self) -> None:\r\n-        \"\"\"Prepare time series data with enhanced feature generation\"\"\"\r\n+        \"\"\"Prepare time series data with consistent feature dimensions\"\"\"\r\n         print(f\"\\nDebug - TransformerDataset preparation:\")\r\n         print(f\"Initial DataFrame shape: {self._df.shape}\")\r\n         \r\n         if len(self._df) == 0:\r\n@@ -82,86 +75,61 @@\n             # Initialize processors\r\n             scaler = DataScaler(self.config.time_series_scaler)\r\n             transformer = DataTransformer()\r\n \r\n-            # Calculate required points and validate data size\r\n+            # Validate data size\r\n             min_required = self.config.window_size + self.config.horizon_size\r\n-            print(f\"Debug - Required data points: {min_required}\")\r\n-            print(f\"Debug - Available data points: {len(self._df)}\")\r\n+            print(f\"Debug - Required points: {min_required}\")\r\n+            print(f\"Debug - Available points: {len(self._df)}\")\r\n             \r\n             if len(self._df) < min_required:\r\n                 raise ValueError(f\"Not enough data points. Need at least {min_required}, got {len(self._df)}\")\r\n \r\n             # Process load data\r\n             load_data = np.array(self._df[self.config.target_variable])\r\n-            print(f\"Debug - Load data shape: {load_data.shape}\")\r\n-            \r\n-            # Scale and normalize data\r\n+            time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n+\r\n+            # Scale and normalize\r\n             if self.config.time_series_scaler:\r\n                 scaled_data = (\r\n                     scaler.fit_transform(load_data)\r\n                     if self.config.is_training_set\r\n                     else scaler.transform(load_data)\r\n                 )\r\n-                # Add normalization for better training stability\r\n                 scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n-                print(f\"Debug - After normalization range: [{scaled_data.min():.2f}, {scaled_data.max():.2f}]\")\r\n             else:\r\n                 scaled_data = load_data\r\n \r\n-            # Validate scaled data\r\n-            if np.isnan(scaled_data).any() or np.isinf(scaled_data).any():\r\n-                raise ValueError(\"Invalid values detected after scaling\")\r\n-\r\n-            # Generate time-based features\r\n-            time_stamps = transformer.extract_timestamps(\r\n-                self._df,\r\n-                self.config.time_variable\r\n-            )\r\n-            print(f\"Debug - Time stamps shape: {time_stamps.shape}\")\r\n-\r\n-            # Initialize feature generators\r\n-            hour_feature = CyclicalTimeFeature(24)\r\n-            week_feature = CyclicalTimeFeature(53)\r\n-            workday_feature = WorkdayFeature()\r\n-\r\n+            # Generate features\r\n             sequence_rows = []\r\n-            print(f\"Debug - Generating features for {len(scaled_data)} time points\")\r\n-            \r\n-            # Generate features for each timestamp\r\n             for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n-                features = [load_value]  # Start with the main value\r\n+                features = [load_value]  # Main value first\r\n \r\n                 if self.config.include_time_information:\r\n                     timestamp_pd = pd.Timestamp(time_stamp)\r\n-                    features.extend(hour_feature.generate(pd.Series([timestamp_pd.hour])))\r\n-                    features.extend(week_feature.generate(pd.Series([timestamp_pd.isocalendar()[1]])))\r\n-                    features.extend(workday_feature.generate(timestamp_pd))\r\n+                    # Add time features\r\n+                    features.extend([\r\n+                        np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n+                        np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n+                        np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                        np.cos(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n+                        np.sin(2 * np.pi * timestamp_pd.month / 12),\r\n+                        np.cos(2 * np.pi * timestamp_pd.month / 12)\r\n+                    ])\r\n \r\n                 sequence_rows.append(features)\r\n \r\n-            # Convert to tensor\r\n+            # Convert to tensor with proper shape\r\n             self.rows = torch.tensor(np.array(sequence_rows, dtype=np.float32))\r\n-            print(f\"Debug - Final rows tensor shape: {self.rows.shape}\")\r\n+            \r\n+            print(f\"Debug - Final shapes:\")\r\n+            print(f\"  Rows tensor: {self.rows.shape}\")\r\n+            print(f\"  Feature dimension: {self.rows.shape[-1]}\")\r\n \r\n-            # Validate final tensor\r\n+            # Validate tensor\r\n             if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n                 raise ValueError(\"Invalid values detected in final tensor\")\r\n-            \r\n-            print(f\"Debug - Final value range: [{self.rows.min():.2f}, {self.rows.max():.2f}]\")\r\n \r\n-            # Set input and target tensors for reference\r\n-            self.prepared_time_series_input = self.rows[\r\n-                :len(self.rows) - self.config.horizon_size\r\n-            ]\r\n-            self.prepared_time_series_target = self.rows[\r\n-                self.config.window_size:\r\n-            ]\r\n-\r\n-            print(f\"Debug - Final shapes:\")\r\n-            print(f\"  Input tensor: {self.prepared_time_series_input.shape if self.prepared_time_series_input is not None else None}\")\r\n-            print(f\"  Target tensor: {self.prepared_time_series_target.shape if self.prepared_time_series_target is not None else None}\")\r\n-\r\n         except Exception as e:\r\n             print(f\"\\nError preparing time series data: {str(e)}\")\r\n             print(f\"DataFrame info:\")\r\n             print(self._df.info())\r\n"
                },
                {
                    "date": 1733182953380,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,71 +1,95 @@\n-import numpy as np\r\n-import pandas as pd\r\n-import torch\r\n-from typing import Tuple, List, Any\r\n-\r\n-from data_loading.base.base_dataset import BaseDataset, DatasetConfig\r\n-from data_loading.features.time_features import CyclicalTimeFeature, WorkdayFeature\r\n-from data_loading.preprocessing.data_scaler import DataScaler\r\n-from data_loading.preprocessing.data_transformer import DataTransformer\r\n-\r\n class TransformerDataset(BaseDataset):\r\n-    \"\"\"Dataset implementation for transformer models with proper dimension handling\"\"\"\r\n+    \"\"\"Dataset implementation for transformer models with proper sequence handling\"\"\"\r\n \r\n     def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n         super().__init__(df, config)\r\n         self.rows: torch.Tensor = torch.empty(0)\r\n         self._prepare_time_series_data()\r\n         self._debug_counter = 0\r\n         self._debug_frequency = 10000\r\n \r\n+    def __len__(self) -> int:\r\n+        \"\"\"\r\n+        Calculate the number of available sequences in the dataset.\r\n+        \r\n+        Returns:\r\n+            Number of sequences that can be extracted from the data\r\n+        \"\"\"\r\n+        # Make sure we have data\r\n+        if self.rows is None or len(self.rows) == 0:\r\n+            return 0\r\n+            \r\n+        # Calculate total sequence length needed for input and forecast\r\n+        total_window = (\r\n+            self.config.window_size +    # Input sequence length\r\n+            self.config.horizon_size     # Forecast sequence length\r\n+        )\r\n+        \r\n+        # Return maximum number of sequences we can create\r\n+        # Subtract total_window - 1 to ensure we have enough points for the last sequence\r\n+        return max(0, len(self.rows) - (total_window - 1))\r\n+\r\n     def __getitem__(self, index: int) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor]:\r\n         \"\"\"\r\n-        Get a data item formatted for transformer training with proper dimensions.\r\n+        Get a data item formatted for transformer training.\r\n         \r\n+        Args:\r\n+            index: Position in dataset to retrieve\r\n+            \r\n         Returns:\r\n             ((src_seq, tgt_input_seq), target_seq):\r\n-            - src_seq: Input sequence [seq_len, features]\r\n-            - tgt_input_seq: Target input [seq_len-1, features]\r\n-            - target_seq: Target values [seq_len-1, 1]\r\n+            - src_seq: Source sequence for encoder [seq_len, features]\r\n+            - tgt_input_seq: Target input sequence for decoder [seq_len-1, features]\r\n+            - target_seq: Target values to predict [seq_len-1, 1]\r\n         \"\"\"\r\n         if self.rows is None:\r\n             raise ValueError(\"Dataset not properly initialized\")\r\n \r\n         # Calculate window sizes\r\n         input_window = self.config.window_size\r\n         forecast_window = self.config.horizon_size\r\n \r\n-        # Get source sequence for encoder\r\n-        src_seq = self.rows[index:index + input_window]\r\n-        \r\n-        # Get target sequence\r\n-        full_target = self.rows[index + input_window:index + input_window + forecast_window]\r\n-        \r\n-        # Prepare sequences for teacher forcing\r\n-        tgt_input_seq = full_target[:-1]  # Remove last timestep for teacher forcing input\r\n-        target_seq = full_target[1:, 0:1]  # Only energy consumption values, shifted by 1\r\n-        \r\n-        if self._debug_counter % self._debug_frequency == 0:\r\n-            print(f\"\\nDebug - Dataset __getitem__ (sample {self._debug_counter}):\")\r\n-            print(f\"Source sequence shape: {src_seq.shape}\")\r\n-            print(f\"Target input sequence shape: {tgt_input_seq.shape}\")\r\n-            print(f\"Target sequence shape: {target_seq.shape}\")\r\n-            print(f\"Source sequence range: [{src_seq.min():.2f}, {src_seq.max():.2f}]\")\r\n-            print(f\"Target sequence range: [{target_seq.min():.2f}, {target_seq.max():.2f}]\")\r\n-        \r\n-        self._debug_counter += 1\r\n+        try:\r\n+            # Get source sequence for encoder\r\n+            src_seq = self.rows[index:index + input_window]\r\n+            \r\n+            # Get target sequence\r\n+            full_target = self.rows[index + input_window:index + input_window + forecast_window]\r\n+            \r\n+            # Prepare sequences for teacher forcing:\r\n+            # - Remove last timestep from target input (shifted right)\r\n+            # - Remove first timestep from target output (shifted left)\r\n+            tgt_input_seq = full_target[:-1]\r\n+            target_seq = full_target[1:, 0:1]  # Only keep energy consumption value\r\n \r\n-        # Add batch dimension if needed\r\n-        if len(src_seq.shape) == 2:\r\n-            src_seq = src_seq.unsqueeze(0)\r\n-            tgt_input_seq = tgt_input_seq.unsqueeze(0)\r\n-            target_seq = target_seq.unsqueeze(0)\r\n-        \r\n-        return (src_seq, tgt_input_seq), target_seq\r\n+            # Debug information\r\n+            if self._debug_counter % self._debug_frequency == 0:\r\n+                print(f\"\\nDebug - Dataset __getitem__ (sample {self._debug_counter}):\")\r\n+                print(f\"Source sequence shape: {src_seq.shape}\")\r\n+                print(f\"Target input sequence shape: {tgt_input_seq.shape}\")\r\n+                print(f\"Target sequence shape: {target_seq.shape}\")\r\n+                print(f\"Source sequence range: [{src_seq.min():.2f}, {src_seq.max():.2f}]\")\r\n+                print(f\"Target sequence range: [{target_seq.min():.2f}, {target_seq.max():.2f}]\")\r\n+            \r\n+            self._debug_counter += 1\r\n \r\n+            # Add batch dimension if needed\r\n+            if len(src_seq.shape) == 2:\r\n+                src_seq = src_seq.unsqueeze(0)\r\n+                tgt_input_seq = tgt_input_seq.unsqueeze(0)\r\n+                target_seq = target_seq.unsqueeze(0)\r\n+\r\n+            return (src_seq, tgt_input_seq), target_seq\r\n+            \r\n+        except Exception as e:\r\n+            print(f\"Error in __getitem__ at index {index}: {str(e)}\")\r\n+            print(f\"Dataset size: {len(self.rows)}\")\r\n+            print(f\"Requested windows: input={input_window}, forecast={forecast_window}\")\r\n+            raise\r\n+\r\n     def _prepare_time_series_data(self) -> None:\r\n-        \"\"\"Prepare time series data with consistent feature dimensions\"\"\"\r\n+        \"\"\"Prepare time series data for transformer model input\"\"\"\r\n         print(f\"\\nDebug - TransformerDataset preparation:\")\r\n         print(f\"Initial DataFrame shape: {self._df.shape}\")\r\n         \r\n         if len(self._df) == 0:\r\n@@ -75,21 +99,13 @@\n             # Initialize processors\r\n             scaler = DataScaler(self.config.time_series_scaler)\r\n             transformer = DataTransformer()\r\n \r\n-            # Validate data size\r\n-            min_required = self.config.window_size + self.config.horizon_size\r\n-            print(f\"Debug - Required points: {min_required}\")\r\n-            print(f\"Debug - Available points: {len(self._df)}\")\r\n-            \r\n-            if len(self._df) < min_required:\r\n-                raise ValueError(f\"Not enough data points. Need at least {min_required}, got {len(self._df)}\")\r\n-\r\n             # Process load data\r\n             load_data = np.array(self._df[self.config.target_variable])\r\n             time_stamps = transformer.extract_timestamps(self._df, self.config.time_variable)\r\n \r\n-            # Scale and normalize\r\n+            # Scale and normalize data\r\n             if self.config.time_series_scaler:\r\n                 scaled_data = (\r\n                     scaler.fit_transform(load_data)\r\n                     if self.config.is_training_set\r\n@@ -98,16 +114,16 @@\n                 scaled_data = (scaled_data - np.mean(scaled_data)) / np.std(scaled_data)\r\n             else:\r\n                 scaled_data = load_data\r\n \r\n-            # Generate features\r\n+            # Generate features for each timestamp\r\n             sequence_rows = []\r\n             for load_value, time_stamp in zip(scaled_data, time_stamps):\r\n                 features = [load_value]  # Main value first\r\n-\r\n+                \r\n                 if self.config.include_time_information:\r\n                     timestamp_pd = pd.Timestamp(time_stamp)\r\n-                    # Add time features\r\n+                    # Add cyclical time encodings\r\n                     features.extend([\r\n                         np.sin(2 * np.pi * timestamp_pd.hour / 24),\r\n                         np.cos(2 * np.pi * timestamp_pd.hour / 24),\r\n                         np.sin(2 * np.pi * timestamp_pd.dayofweek / 7),\r\n@@ -117,14 +133,16 @@\n                     ])\r\n \r\n                 sequence_rows.append(features)\r\n \r\n-            # Convert to tensor with proper shape\r\n+            # Convert to tensor and store\r\n             self.rows = torch.tensor(np.array(sequence_rows, dtype=np.float32))\r\n             \r\n-            print(f\"Debug - Final shapes:\")\r\n-            print(f\"  Rows tensor: {self.rows.shape}\")\r\n-            print(f\"  Feature dimension: {self.rows.shape[-1]}\")\r\n+            # Print debug information\r\n+            print(f\"Debug - Feature generation:\")\r\n+            print(f\"  Total sequences: {len(self.rows)}\")\r\n+            print(f\"  Features per sequence: {self.rows.shape[-1]}\")\r\n+            print(f\"  Value range: [{self.rows.min():.2f}, {self.rows.max():.2f}]\")\r\n \r\n             # Validate tensor\r\n             if torch.isnan(self.rows).any() or torch.isinf(self.rows).any():\r\n                 raise ValueError(\"Invalid values detected in final tensor\")\r\n"
                },
                {
                    "date": 1733182970722,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,14 @@\n+import numpy as np\r\n+import pandas as pd\r\n+import torch\r\n+from typing import Tuple, List, Any\r\n+\r\n+from data_loading.base.base_dataset import BaseDataset, DatasetConfig\r\n+from data_loading.features.time_features import CyclicalTimeFeature, WorkdayFeature\r\n+from data_loading.preprocessing.data_scaler import DataScaler\r\n+from data_loading.preprocessing.data_transformer import DataTransformer\r\n+\r\n class TransformerDataset(BaseDataset):\r\n     \"\"\"Dataset implementation for transformer models with proper sequence handling\"\"\"\r\n \r\n     def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n"
                }
            ],
            "date": 1733005186538,
            "name": "Commit-0",
            "content": "# data_loading/datasets/transformer_dataset.py\r\nimport numpy as np\r\nimport pandas as pd\r\nimport torch\r\nfrom typing import Tuple, List, Any\r\n\r\nfrom data_loading.base.base_dataset import BaseDataset, DatasetConfig\r\nfrom data_loading.features.time_features import CyclicalTimeFeature, WorkdayFeature\r\nfrom data_loading.preprocessing.data_scaler import DataScaler\r\nfrom data_loading.preprocessing.data_transformer import DataTransformer\r\n\r\nclass TransformerDataset(BaseDataset):\r\n    \"\"\"Dataset implementation for transformer models\"\"\"\r\n\r\n    def __init__(self, df: pd.DataFrame, config: DatasetConfig):\r\n        super().__init__(df, config)\r\n        self.rows: torch.Tensor = torch.empty(0)\r\n        self._prepare_time_series_data()\r\n\r\n    def __len__(self) -> int:\r\n        if self.rows is None:\r\n            return 0\r\n        return max(0, len(self.rows) - self.config.time_series_window_in_hours - self.config.forecasting_horizon_in_hours)\r\n\r\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"Get a data item by index\r\n\r\n        Returns:\r\n            Tuple containing:\r\n            - Input sequence [time_series_window_in_hours, features]\r\n            - Target sequence [forecasting_horizon_in_hours, features]\r\n        \"\"\"\r\n        if self.rows is None:\r\n            raise ValueError(\"Dataset not properly initialized\")\r\n\r\n        input_seq = self.rows[\r\n                    index:index + self.config.time_series_window_in_hours\r\n                    ]\r\n\r\n        target_seq = self.rows[\r\n                    index + self.config.time_series_window_in_hours:\r\n                    index + self.config.time_series_window_in_hours + self.config.forecasting_horizon_in_hours\r\n                    ]\r\n\r\n        return input_seq, target_seq\r\n\r\n    def _prepare_time_series_data(self) -> None:\r\n        \"\"\"Prepare time series data for model input\"\"\"\r\n        print(f\"\\nDebug - TransformerDataset preparation:\")\r\n        print(f\"Initial DataFrame shape: {self._df.shape}\")\r\n        \r\n        if len(self._df) == 0:\r\n            raise ValueError(\"Empty dataframe provided\")\r\n\r\n        # Initialize processors\r\n        scaler = DataScaler(self.config.time_series_scaler)\r\n        transformer = DataTransformer()\r\n\r\n        # Make sure we have enough data points\r\n        min_required = (self.config.time_series_window_in_hours + \r\n                    self.config.forecasting_horizon_in_hours) * 4  # 4 points per hour\r\n        print(f\"Debug - Required data points: {min_required}\")\r\n        print(f\"Debug - Available data points: {len(self._df)}\")\r\n        \r\n        if len(self._df) < min_required:\r\n            raise ValueError(f\"Not enough data points. Need at least {min_required}, got {len(self._df)}\")\r\n\r\n        try:\r\n            # Extract raw data\r\n            raw_data = np.array(self._df[self.config.target_variable])\r\n            print(f\"Debug - Raw data shape: {raw_data.shape}\")\r\n            \r\n            # Average data\r\n            load_data = transformer.average_by_window(raw_data, window_size=4)\r\n            print(f\"Debug - After averaging shape: {load_data.shape}\")\r\n            \r\n            # Scale data\r\n            if self.config.time_series_scaler:\r\n                scaled_data = (\r\n                    scaler.fit_transform(load_data)\r\n                    if self.config.is_training_set\r\n                    else scaler.transform(load_data)\r\n                )\r\n                print(f\"Debug - After scaling shape: {scaled_data.shape}\")\r\n            else:\r\n                scaled_data = load_data\r\n\r\n            # Extract timestamps and prepare features\r\n            time_stamps = transformer.extract_timestamps(\r\n                self._df,\r\n                self.config.time_variable\r\n            )\r\n            print(f\"Debug - Time stamps shape: {time_stamps.shape}\")\r\n\r\n            # Initialize feature generators\r\n            hour_feature = CyclicalTimeFeature(24)\r\n            week_feature = CyclicalTimeFeature(53)\r\n            workday_feature = WorkdayFeature()\r\n\r\n            sequence_rows = []\r\n\r\n            # Generate features for each timestamp\r\n            print(f\"Debug - Generating features for {len(scaled_data)} time points\")\r\n            for idx, (load_value, time_stamp) in enumerate(zip(scaled_data, time_stamps)):\r\n                features = [load_value]\r\n\r\n                if self.config.include_time_information:\r\n                    timestamp_pd = pd.Timestamp(time_stamp)\r\n                    features.extend(hour_feature.generate(pd.Series([timestamp_pd.hour])))\r\n                    features.extend(week_feature.generate(pd.Series([timestamp_pd.isocalendar()[1]])))\r\n                    features.extend(workday_feature.generate(timestamp_pd))\r\n\r\n                sequence_rows.append(features)\r\n\r\n            self.rows = torch.tensor(np.array(sequence_rows, dtype=np.float32))\r\n            print(f\"Debug - Final rows tensor shape: {self.rows.shape}\")\r\n\r\n            # Set input and target tensors\r\n            self.prepared_time_series_input = self.rows[\r\n                :len(self.rows) - self.config.forecasting_horizon_in_hours\r\n            ]\r\n            self.prepared_time_series_target = self.rows[\r\n                self.config.time_series_window_in_hours:\r\n            ]\r\n\r\n            print(f\"Debug - Final shapes:\")\r\n            print(f\"  Input tensor: {self.prepared_time_series_input.shape if self.prepared_time_series_input is not None else None}\")\r\n            print(f\"  Target tensor: {self.prepared_time_series_target.shape if self.prepared_time_series_target is not None else None}\")\r\n\r\n        except Exception as e:\r\n            print(f\"\\nError preparing time series data: {str(e)}\")\r\n            print(f\"DataFrame info:\")\r\n            print(self._df.info())\r\n            raise"
        }
    ]
}