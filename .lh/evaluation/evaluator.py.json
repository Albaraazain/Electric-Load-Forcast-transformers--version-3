{
    "sourceFile": "evaluation/evaluator.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 5,
            "patches": [
                {
                    "date": 1733003602145,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733177657476,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,12 +14,14 @@\n \r\n     def __init__(\r\n             self,\r\n             scaler: Optional[StandardScaler] = None,\r\n-            metric_config: Optional[MetricConfig] = None\r\n+            metric_config: Optional[MetricConfig] = None,\r\n+            resolution_minutes: Optional[int] = None\r\n     ):\r\n         self.scaler = scaler\r\n         self.metric_config = metric_config or MetricConfig()\r\n+        self.resolution_minutes = resolution_minutes\r\n \r\n     def evaluate(\r\n             self,\r\n             predictions: Tensor,\r\n"
                },
                {
                    "date": 1733246836298,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,15 +1,7 @@\n # evaluation/evaluator.py\r\n-import contextlib\r\n-from typing import Optional, Tuple, List\r\n-import torch\r\n-from sklearn.preprocessing import StandardScaler\r\n-from torch import Tensor\r\n-import numpy as np\r\n+from utils.logging import Logger, LoggerConfig, LogLevel\r\n \r\n-from .metrics import Metrics, MetricConfig\r\n-from .results import EvaluationResult, PredictionComparison\r\n-\r\n class Evaluator:\r\n     \"\"\"Evaluates model predictions against actual values.\"\"\"\r\n \r\n     def __init__(\r\n@@ -17,8 +9,27 @@\n             scaler: Optional[StandardScaler] = None,\r\n             metric_config: Optional[MetricConfig] = None,\r\n             resolution_minutes: Optional[int] = None\r\n     ):\r\n+        self.logger = Logger.get_logger(\r\n+            __name__,\r\n+            LoggerConfig(\r\n+                level=LogLevel.INFO,\r\n+                component_name=\"evaluator\",\r\n+                file_path=\"logs/evaluation.log\",\r\n+                json_output=True\r\n+            )\r\n+        )\r\n+        \r\n+        self.logger.info(\r\n+            \"Initializing evaluator\",\r\n+            extra={\r\n+                \"resolution_minutes\": resolution_minutes,\r\n+                \"has_scaler\": scaler is not None,\r\n+                \"metric_config\": metric_config.__dict__ if metric_config else None\r\n+            }\r\n+        )\r\n+        \r\n         self.scaler = scaler\r\n         self.metric_config = metric_config or MetricConfig()\r\n         self.resolution_minutes = resolution_minutes\r\n \r\n@@ -28,68 +39,97 @@\n             targets: Tensor,\r\n             timestamps: np.ndarray,\r\n             num_variables: int = 1\r\n     ) -> EvaluationResult:\r\n-        \"\"\"\r\n-        Evaluate model predictions.\r\n+        \"\"\"Evaluate model predictions.\"\"\"\r\n+        self.logger.info(\r\n+            \"Starting evaluation\",\r\n+            extra={\r\n+                \"predictions_shape\": predictions.shape,\r\n+                \"targets_shape\": targets.shape,\r\n+                \"num_variables\": num_variables\r\n+            }\r\n+        )\r\n \r\n-        Args:\r\n-            predictions: Model predictions\r\n-            targets: Actual target values\r\n-            timestamps: Timestamps for each prediction\r\n-            num_variables: Number of target variables\r\n+        try:\r\n+            # Unscale predictions if scaler is provided\r\n+            pred_np, target_np = self._prepare_data(predictions, targets)\r\n+            self.logger.debug(\r\n+                \"Data prepared for evaluation\",\r\n+                extra={\r\n+                    \"pred_range\": [float(pred_np.min()), float(pred_np.max())],\r\n+                    \"target_range\": [float(target_np.min()), float(target_np.max())]\r\n+                }\r\n+            )\r\n \r\n-        Returns:\r\n-            Evaluation results including metrics and comparisons\r\n-        \"\"\"\r\n-        # Unscale predictions if scaler is provided\r\n-        pred_np, target_np = self._prepare_data(predictions, targets)\r\n+            # Calculate overall metrics\r\n+            total_metrics = Metrics.calculate_all_metrics(\r\n+                torch.tensor(pred_np),\r\n+                torch.tensor(target_np),\r\n+                self.metric_config\r\n+            )\r\n+            self.logger.info(\"Overall metrics calculated\", extra={\"metrics\": total_metrics})\r\n \r\n-        # Calculate overall metrics\r\n-        total_metrics = Metrics.calculate_all_metrics(\r\n-            torch.tensor(pred_np),\r\n-            torch.tensor(target_np),\r\n-            self.metric_config\r\n-        )\r\n-\r\n-        # Calculate per-variable metrics if multiple variables\r\n-        variable_metrics = {}\r\n-        if num_variables > 1:\r\n-            for i in range(num_variables):\r\n-                variable_metrics[f'var_{i}'] = Metrics.calculate_all_metrics(\r\n-                    torch.tensor(pred_np[:, i]),\r\n-                    torch.tensor(target_np[:, i]),\r\n-                    self.metric_config\r\n+            # Calculate per-variable metrics if multiple variables\r\n+            variable_metrics = {}\r\n+            if num_variables > 1:\r\n+                for i in range(num_variables):\r\n+                    variable_metrics[f'var_{i}'] = Metrics.calculate_all_metrics(\r\n+                        torch.tensor(pred_np[:, i]),\r\n+                        torch.tensor(target_np[:, i]),\r\n+                        self.metric_config\r\n+                    )\r\n+                self.logger.info(\r\n+                    \"Variable-specific metrics calculated\", \r\n+                    extra={\"variable_metrics\": variable_metrics}\r\n                 )\r\n \r\n-        # Create detailed comparisons\r\n-        comparisons = self._create_comparisons(pred_np, target_np, timestamps)\r\n+            # Create detailed comparisons\r\n+            comparisons = self._create_comparisons(pred_np, target_np, timestamps)\r\n+            self.logger.debug(f\"Created {len(comparisons)} detailed comparisons\")\r\n \r\n-        return EvaluationResult(\r\n-            total_metrics=total_metrics,\r\n-            variable_metrics=variable_metrics,\r\n-            comparisons=comparisons\r\n-        )\r\n+            result = EvaluationResult(\r\n+                total_metrics=total_metrics,\r\n+                variable_metrics=variable_metrics,\r\n+                comparisons=comparisons\r\n+            )\r\n+            \r\n+            self.logger.info(\"Evaluation completed successfully\")\r\n+            return result\r\n \r\n+        except Exception as e:\r\n+            self.logger.error(\r\n+                \"Evaluation failed\",\r\n+                extra={\r\n+                    \"error\": str(e),\r\n+                    \"error_type\": type(e).__name__\r\n+                }\r\n+            )\r\n+            raise\r\n+\r\n     def _prepare_data(\r\n             self,\r\n             predictions: Tensor,\r\n             targets: Tensor\r\n     ) -> Tuple[np.ndarray, np.ndarray]:\r\n-        \"\"\"Prepare data for evaluation by unscaling if necessary.\"\"\"\r\n+        \"\"\"Prepare data for evaluation.\"\"\"\r\n+        self.logger.debug(\"Preparing data for evaluation\")\r\n+        \r\n         # Convert to numpy and reshape if needed\r\n         pred_np = predictions.detach().cpu().numpy()\r\n         target_np = targets.detach().cpu().numpy()\r\n \r\n-        # Reshape if more than 2 dimensions\r\n-        if len(pred_np.shape) > 2:\r\n-            pred_np = pred_np.reshape(-1, pred_np.shape[-1])\r\n-            target_np = target_np.reshape(-1, target_np.shape[-1])\r\n-\r\n         if self.scaler is not None:\r\n-            with contextlib.suppress(Exception):\r\n+            try:\r\n                 pred_np = self.scaler.inverse_transform(pred_np)\r\n                 target_np = self.scaler.inverse_transform(target_np)\r\n+                self.logger.debug(\"Data unscaled successfully\")\r\n+            except Exception as e:\r\n+                self.logger.warning(\r\n+                    \"Failed to unscale data\",\r\n+                    extra={\"error\": str(e)}\r\n+                )\r\n+\r\n         return pred_np, target_np\r\n \r\n     def _create_comparisons(\r\n             self,\r\n@@ -97,12 +137,32 @@\n             targets: np.ndarray,\r\n             timestamps: np.ndarray\r\n     ) -> List[PredictionComparison]:\r\n         \"\"\"Create detailed comparisons for each timestamp.\"\"\"\r\n-        return [\r\n-            PredictionComparison(\r\n-                timestamp=timestamp,\r\n-                predicted=pred,\r\n-                actual=target\r\n+        self.logger.debug(\"Creating detailed comparisons\")\r\n+        \r\n+        try:\r\n\\ No newline at end of file\n+            comparisons = [\r\n+                PredictionComparison(\r\n+                    timestamp=timestamp,\r\n+                    predicted=pred,\r\n+                    actual=target\r\n+                )\r\n+                for pred, target, timestamp in zip(predictions, targets, timestamps)\r\n+            ]\r\n+            \r\n+            self.logger.debug(\r\n+                f\"Created {len(comparisons)} comparison records\",\r\n+                extra={\r\n+                    \"first_timestamp\": str(comparisons[0].timestamp),\r\n+                    \"last_timestamp\": str(comparisons[-1].timestamp)\r\n+                }\r\n             )\r\n-            for pred, target, timestamp in zip(predictions, targets, timestamps)\r\n-        ]\n+            \r\n+            return comparisons\r\n+            \r\n+        except Exception as e:\r\n+            self.logger.error(\r\n+                \"Failed to create comparisons\",\r\n+                extra={\"error\": str(e)}\r\n+            )\r\n+            raise\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733247006211,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,12 @@\n-# evaluation/evaluator.py\r\n+from typing import Optional, Tuple, List\r\n+import numpy as np\r\n+import torch\r\n+from torch import Tensor\r\n+from sklearn.preprocessing import StandardScaler\r\n from utils.logging import Logger, LoggerConfig, LogLevel\r\n+from utils.metrics import Metrics, MetricConfig\r\n+from utils.models import EvaluationResult, PredictionComparison\r\n \r\n class Evaluator:\r\n     \"\"\"Evaluates model predictions against actual values.\"\"\"\r\n \r\n"
                },
                {
                    "date": 1733247206835,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,12 +1,13 @@\n from typing import Optional, Tuple, List\r\n import numpy as np\r\n import torch\r\n from torch import Tensor\r\n+from pathlib import Path\r\n from sklearn.preprocessing import StandardScaler\r\n+from evaluation.results import EvaluationResult, PredictionComparison\r\n from utils.logging import Logger, LoggerConfig, LogLevel\r\n-from utils.metrics import Metrics, MetricConfig\r\n-from utils.models import EvaluationResult, PredictionComparison\r\n+from evaluation.metrics import Metrics, MetricConfig  # Changed from utils.metrics\r\n \r\n class Evaluator:\r\n     \"\"\"Evaluates model predictions against actual values.\"\"\"\r\n \r\n@@ -20,9 +21,9 @@\n             __name__,\r\n             LoggerConfig(\r\n                 level=LogLevel.INFO,\r\n                 component_name=\"evaluator\",\r\n-                file_path=\"logs/evaluation.log\",\r\n+                file_path=Path(\"logs/evaluation.log\"),\r\n                 json_output=True\r\n             )\r\n         )\r\n         \r\n"
                },
                {
                    "date": 1733247508925,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,12 +3,15 @@\n import torch\r\n from torch import Tensor\r\n from pathlib import Path\r\n from sklearn.preprocessing import StandardScaler\r\n+from evaluation.metrics import Metrics, MetricConfig\r\n from evaluation.results import EvaluationResult, PredictionComparison\r\n-from utils.logging import Logger, LoggerConfig, LogLevel\r\n-from evaluation.metrics import Metrics, MetricConfig  # Changed from utils.metrics\r\n+from utils.logging.config import LogLevel, LoggerConfig\r\n+from utils.logging.logger import Logger  # Assuming metrics are in evaluation package\r\n \r\n+# evaluation/evaluator.py\r\n+\r\n class Evaluator:\r\n     \"\"\"Evaluates model predictions against actual values.\"\"\"\r\n \r\n     def __init__(\r\n"
                }
            ],
            "date": 1733003602145,
            "name": "Commit-0",
            "content": "# evaluation/evaluator.py\r\nimport contextlib\r\nfrom typing import Optional, Tuple, List\r\nimport torch\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom torch import Tensor\r\nimport numpy as np\r\n\r\nfrom .metrics import Metrics, MetricConfig\r\nfrom .results import EvaluationResult, PredictionComparison\r\n\r\nclass Evaluator:\r\n    \"\"\"Evaluates model predictions against actual values.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            scaler: Optional[StandardScaler] = None,\r\n            metric_config: Optional[MetricConfig] = None\r\n    ):\r\n        self.scaler = scaler\r\n        self.metric_config = metric_config or MetricConfig()\r\n\r\n    def evaluate(\r\n            self,\r\n            predictions: Tensor,\r\n            targets: Tensor,\r\n            timestamps: np.ndarray,\r\n            num_variables: int = 1\r\n    ) -> EvaluationResult:\r\n        \"\"\"\r\n        Evaluate model predictions.\r\n\r\n        Args:\r\n            predictions: Model predictions\r\n            targets: Actual target values\r\n            timestamps: Timestamps for each prediction\r\n            num_variables: Number of target variables\r\n\r\n        Returns:\r\n            Evaluation results including metrics and comparisons\r\n        \"\"\"\r\n        # Unscale predictions if scaler is provided\r\n        pred_np, target_np = self._prepare_data(predictions, targets)\r\n\r\n        # Calculate overall metrics\r\n        total_metrics = Metrics.calculate_all_metrics(\r\n            torch.tensor(pred_np),\r\n            torch.tensor(target_np),\r\n            self.metric_config\r\n        )\r\n\r\n        # Calculate per-variable metrics if multiple variables\r\n        variable_metrics = {}\r\n        if num_variables > 1:\r\n            for i in range(num_variables):\r\n                variable_metrics[f'var_{i}'] = Metrics.calculate_all_metrics(\r\n                    torch.tensor(pred_np[:, i]),\r\n                    torch.tensor(target_np[:, i]),\r\n                    self.metric_config\r\n                )\r\n\r\n        # Create detailed comparisons\r\n        comparisons = self._create_comparisons(pred_np, target_np, timestamps)\r\n\r\n        return EvaluationResult(\r\n            total_metrics=total_metrics,\r\n            variable_metrics=variable_metrics,\r\n            comparisons=comparisons\r\n        )\r\n\r\n    def _prepare_data(\r\n            self,\r\n            predictions: Tensor,\r\n            targets: Tensor\r\n    ) -> Tuple[np.ndarray, np.ndarray]:\r\n        \"\"\"Prepare data for evaluation by unscaling if necessary.\"\"\"\r\n        # Convert to numpy and reshape if needed\r\n        pred_np = predictions.detach().cpu().numpy()\r\n        target_np = targets.detach().cpu().numpy()\r\n\r\n        # Reshape if more than 2 dimensions\r\n        if len(pred_np.shape) > 2:\r\n            pred_np = pred_np.reshape(-1, pred_np.shape[-1])\r\n            target_np = target_np.reshape(-1, target_np.shape[-1])\r\n\r\n        if self.scaler is not None:\r\n            with contextlib.suppress(Exception):\r\n                pred_np = self.scaler.inverse_transform(pred_np)\r\n                target_np = self.scaler.inverse_transform(target_np)\r\n        return pred_np, target_np\r\n\r\n    def _create_comparisons(\r\n            self,\r\n            predictions: np.ndarray,\r\n            targets: np.ndarray,\r\n            timestamps: np.ndarray\r\n    ) -> List[PredictionComparison]:\r\n        \"\"\"Create detailed comparisons for each timestamp.\"\"\"\r\n        return [\r\n            PredictionComparison(\r\n                timestamp=timestamp,\r\n                predicted=pred,\r\n                actual=target\r\n            )\r\n            for pred, target, timestamp in zip(predictions, targets, timestamps)\r\n        ]"
        }
    ]
}