{
    "sourceFile": "models/architectures/transformers/base_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 13,
            "patches": [
                {
                    "date": 1733089622876,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733237263457,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,26 +11,40 @@\n class BaseTransformer(BaseModel, ABC):\r\n     def __init__(self, config: Dict[str, Any]):\r\n         super().__init__(config)\r\n \r\n+        # Validate configuration\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        missing_keys = [key for key in required_keys if key not in config]\r\n+        if missing_keys:\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n+        # Initialize with validated dimensions\r\n         self.d_model = config['d_model']\r\n+        self.input_features = config['input_features']\r\n+        \r\n+        # Validate d_model is appropriate for number of heads\r\n+        if self.d_model % config['n_heads'] != 0:\r\n+            raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({config['n_heads']})\")\r\n+            \r\n+        # Rest of initialization\r\n         self.n_heads = config['n_heads']\r\n         self.n_encoder_layers = config['n_encoder_layers']\r\n         self.n_decoder_layers = config['n_decoder_layers']\r\n         self.d_ff = config['d_ff']\r\n         self.dropout = config['dropout']\r\n-        self.input_features = config['input_features']\r\n         self.batch_first = config.get('batch_first', True)  # Default to batch_first=True\r\n \r\n-        # Common components with proper initialization\r\n+        # Initialize embeddings with validated dimensions\r\n         self.encoder_embedding = CombinedEmbedding(\r\n-            self.d_model,\r\n-            self.input_features,\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n             dropout=self.dropout\r\n         )\r\n+        \r\n         self.decoder_embedding = CombinedEmbedding(\r\n-            self.d_model,\r\n-            self.input_features,\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n             dropout=self.dropout\r\n         )\r\n \r\n         # Initialize encoder and decoder layers\r\n"
                },
                {
                    "date": 1733237521935,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,173 @@\n+from abc import ABC, abstractmethod\r\n+from typing import Optional, Dict, Any, Tuple\r\n+import torch\r\n+from torch import nn, Tensor\r\n+\r\n+from models.base.base_model import BaseModel\r\n+from models.components.embeddings import CombinedEmbedding\r\n+from models.components.attention import MultiHeadAttention\r\n+\r\n+\r\n+class BaseTransformer(BaseModel, ABC):\r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        \"\"\"\r\n+        Initialize transformer with enhanced validation and setup.\r\n+        \r\n+        Args:\r\n+            config: Configuration dictionary containing model parameters\r\n+        \"\"\"\r\n+        super().__init__(config)\r\n+        \r\n+        # Validate core configuration\r\n+        self._validate_config(config)\r\n+        \r\n+        # Model dimensions\r\n+        self.d_model = config['d_model']\r\n+        self.n_heads = config['n_heads']\r\n+        self.input_features = config['input_features']\r\n+        self.n_encoder_layers = config['n_encoder_layers']\r\n+        self.n_decoder_layers = config['n_decoder_layers']\r\n+        self.d_ff = config['d_ff']\r\n+        self.dropout = config['dropout']\r\n+        self.batch_first = config.get('batch_first', True)\r\n+        \r\n+        # Initialize embeddings\r\n+        self.encoder_embedding = self._create_embedding()\r\n+        self.decoder_embedding = self._create_embedding()\r\n+        \r\n+        # Create model components\r\n+        self.encoder_layers = self._create_encoder_layers()\r\n+        self.decoder_layers = self._create_decoder_layers()\r\n+        \r\n+        # Output projection with proper scaling\r\n+        self.output_projection = nn.Sequential(\r\n+            nn.Linear(self.d_model, self.d_model // 2),\r\n+            nn.ReLU(),\r\n+            nn.Linear(self.d_model // 2, 1)\r\n+        )\r\n+        self._initialize_output_projection()\r\n+\r\n+    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n+        \"\"\"Validate configuration parameters.\"\"\"\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        missing_keys = [key for key in required_keys if key not in config]\r\n+        if missing_keys:\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+            \r\n+        if config['d_model'] % config['n_heads'] != 0:\r\n+            raise ValueError(\r\n+                f\"d_model ({config['d_model']}) must be divisible by \"\r\n+                f\"n_heads ({config['n_heads']})\"\r\n+            )\r\n+\r\n+    def _create_embedding(self) -> CombinedEmbedding:\r\n+        \"\"\"Create embedding layer with proper initialization.\"\"\"\r\n+        return CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=self.dropout\r\n+        )\r\n+\r\n+    def _initialize_output_projection(self) -> None:\r\n+        \"\"\"Initialize output projection layers.\"\"\"\r\n+        for layer in self.output_projection:\r\n+            if isinstance(layer, nn.Linear):\r\n+                nn.init.xavier_uniform_(layer.weight)\r\n+                if layer.bias is not None:\r\n+                    nn.init.zeros_(layer.bias)\r\n+\r\n+    @abstractmethod\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    def encode(\r\n+        self,\r\n+        src: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Encode input sequence.\"\"\"\r\n+        # Apply embedding\r\n+        src = self.encoder_embedding(src)\r\n+        \r\n+        # Pass through encoder layers\r\n+        for layer in self.encoder_layers:\r\n+            src = layer(\r\n+                src,\r\n+                src_mask=src_mask, \r\n+                src_key_padding_mask=src_key_padding_mask\r\n+            )\r\n+        return src\r\n+\r\n+    def decode(\r\n+        self,\r\n+        tgt: Tensor,\r\n+        memory: Tensor,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        memory_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+        memory_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Decode target sequence.\"\"\"\r\n+        # Apply embedding\r\n+        tgt = self.decoder_embedding(tgt)\r\n+        \r\n+        # Pass through decoder layers\r\n+        for layer in self.decoder_layers:\r\n+            tgt = layer(\r\n+                tgt,\r\n+                memory,\r\n+                tgt_mask=tgt_mask,\r\n+                memory_mask=memory_mask,\r\n+                tgt_key_padding_mask=tgt_key_padding_mask,\r\n+                memory_key_padding_mask=memory_key_padding_mask\r\n+            )\r\n+        return tgt\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        tgt: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Forward pass that returns only the final output.\"\"\"\r\n+        # Encode source sequence\r\n+        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n+        \r\n+        # Decode target sequence\r\n+        output = self.decode(\r\n+            tgt,\r\n+            memory,\r\n+            tgt_mask=tgt_mask,\r\n+            memory_mask=src_mask,\r\n+            tgt_key_padding_mask=tgt_key_padding_mask,\r\n+            memory_key_padding_mask=src_key_padding_mask\r\n+        )\r\n+        \r\n+        # Project to output dimension\r\n+        output = self.output_projection(output)\r\n+        \r\n+        return output\r\n+\r\n+    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n+        \"\"\"Generate a square mask for the sequence. \r\n+        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\r\n+        \"\"\"\r\n+        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n+        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n+        return mask\r\n+\r\n+    def get_input_dims(self) -> int:\r\n+        return self.input_features\r\n+\r\n+    def get_output_dims(self) -> int:\r\n+        return 1\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733237647597,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,32 +9,33 @@\n \r\n \r\n class BaseTransformer(BaseModel, ABC):\r\n     def __init__(self, config: Dict[str, Any]):\r\n-        \"\"\"\r\n-        Initialize transformer with enhanced validation and setup.\r\n-        \r\n-        Args:\r\n-            config: Configuration dictionary containing model parameters\r\n-        \"\"\"\r\n         super().__init__(config)\r\n-        \r\n+\r\n         # Validate core configuration\r\n         self._validate_config(config)\r\n         \r\n-        # Model dimensions\r\n+        # Initialize dimensions\r\n         self.d_model = config['d_model']\r\n+        self.input_features = config['input_features']  # Total features\r\n         self.n_heads = config['n_heads']\r\n-        self.input_features = config['input_features']\r\n         self.n_encoder_layers = config['n_encoder_layers']\r\n         self.n_decoder_layers = config['n_decoder_layers']\r\n         self.d_ff = config['d_ff']\r\n         self.dropout = config['dropout']\r\n-        self.batch_first = config.get('batch_first', True)\r\n         \r\n-        # Initialize embeddings\r\n-        self.encoder_embedding = self._create_embedding()\r\n-        self.decoder_embedding = self._create_embedding()\r\n+        # Initialize embeddings with total feature count\r\n+        self.encoder_embedding = CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,  # Pass total features\r\n+            dropout=self.dropout\r\n+        )\r\n+        self.decoder_embedding = CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,  # Pass total features\r\n+            dropout=self.dropout\r\n+        )\r\n         \r\n         # Create model components\r\n         self.encoder_layers = self._create_encoder_layers()\r\n         self.decoder_layers = self._create_decoder_layers()\r\n@@ -169,158 +170,5 @@\n     def get_input_dims(self) -> int:\r\n         return self.input_features\r\n \r\n     def get_output_dims(self) -> int:\r\n-        return 1\n-from abc import ABC, abstractmethod\r\n-from typing import Optional, Dict, Any, Tuple\r\n-import torch\r\n-from torch import nn, Tensor\r\n-\r\n-from models.base.base_model import BaseModel\r\n-from models.components.embeddings import CombinedEmbedding\r\n-from models.components.attention import MultiHeadAttention\r\n-\r\n-\r\n-class BaseTransformer(BaseModel, ABC):\r\n-    def __init__(self, config: Dict[str, Any]):\r\n-        super().__init__(config)\r\n-\r\n-        # Validate configuration\r\n-        required_keys = ['d_model', 'n_heads', 'input_features']\r\n-        missing_keys = [key for key in required_keys if key not in config]\r\n-        if missing_keys:\r\n-            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n-\r\n-        # Initialize with validated dimensions\r\n-        self.d_model = config['d_model']\r\n-        self.input_features = config['input_features']\r\n-        \r\n-        # Validate d_model is appropriate for number of heads\r\n-        if self.d_model % config['n_heads'] != 0:\r\n-            raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({config['n_heads']})\")\r\n-            \r\n-        # Rest of initialization\r\n-        self.n_heads = config['n_heads']\r\n-        self.n_encoder_layers = config['n_encoder_layers']\r\n-        self.n_decoder_layers = config['n_decoder_layers']\r\n-        self.d_ff = config['d_ff']\r\n-        self.dropout = config['dropout']\r\n-        self.batch_first = config.get('batch_first', True)  # Default to batch_first=True\r\n-\r\n-        # Initialize embeddings with validated dimensions\r\n-        self.encoder_embedding = CombinedEmbedding(\r\n-            d_model=self.d_model,\r\n-            input_features=self.input_features,\r\n-            dropout=self.dropout\r\n-        )\r\n-        \r\n-        self.decoder_embedding = CombinedEmbedding(\r\n-            d_model=self.d_model,\r\n-            input_features=self.input_features,\r\n-            dropout=self.dropout\r\n-        )\r\n-\r\n-        # Initialize encoder and decoder layers\r\n-        self.encoder_layers = self._create_encoder_layers()\r\n-        self.decoder_layers = self._create_decoder_layers()\r\n-\r\n-        # Output projection with proper initialization\r\n-        self.output_projection = nn.Linear(self.d_model, 1)\r\n-        nn.init.xavier_uniform_(self.output_projection.weight)\r\n-        nn.init.zeros_(self.output_projection.bias)\r\n-\r\n-    @abstractmethod\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n-        pass\r\n-\r\n-    @abstractmethod\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n-        pass\r\n-\r\n-    def encode(\r\n-        self,\r\n-        src: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Encode input sequence.\"\"\"\r\n-        # Apply embedding\r\n-        src = self.encoder_embedding(src)\r\n-        \r\n-        # Pass through encoder layers\r\n-        for layer in self.encoder_layers:\r\n-            src = layer(\r\n-                src,\r\n-                src_mask=src_mask, \r\n-                src_key_padding_mask=src_key_padding_mask\r\n-            )\r\n-        return src\r\n-\r\n-    def decode(\r\n-        self,\r\n-        tgt: Tensor,\r\n-        memory: Tensor,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        memory_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-        memory_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Decode target sequence.\"\"\"\r\n-        # Apply embedding\r\n-        tgt = self.decoder_embedding(tgt)\r\n-        \r\n-        # Pass through decoder layers\r\n-        for layer in self.decoder_layers:\r\n-            tgt = layer(\r\n-                tgt,\r\n-                memory,\r\n-                tgt_mask=tgt_mask,\r\n-                memory_mask=memory_mask,\r\n-                tgt_key_padding_mask=tgt_key_padding_mask,\r\n-                memory_key_padding_mask=memory_key_padding_mask\r\n-            )\r\n-        return tgt\r\n-\r\n-    def forward(\r\n-        self,\r\n-        src: Tensor,\r\n-        tgt: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Forward pass that returns only the final output.\"\"\"\r\n-        # Encode source sequence\r\n-        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n-        \r\n-        # Decode target sequence\r\n-        output = self.decode(\r\n-            tgt,\r\n-            memory,\r\n-            tgt_mask=tgt_mask,\r\n-            memory_mask=src_mask,\r\n-            tgt_key_padding_mask=tgt_key_padding_mask,\r\n-            memory_key_padding_mask=src_key_padding_mask\r\n-        )\r\n-        \r\n-        # Project to output dimension\r\n-        output = self.output_projection(output)\r\n-        \r\n-        return output\r\n-\r\n-    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n-        \"\"\"Generate a square mask for the sequence. \r\n-        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\r\n-        \"\"\"\r\n-        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n-        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n-        return mask\r\n-\r\n-    def get_input_dims(self) -> int:\r\n-        return self.input_features\r\n-\r\n-    def get_output_dims(self) -> int:\r\n         return 1\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733237653249,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,12 +50,11 @@\n \r\n     def _validate_config(self, config: Dict[str, Any]) -> None:\r\n         \"\"\"Validate configuration parameters.\"\"\"\r\n         required_keys = ['d_model', 'n_heads', 'input_features']\r\n-        missing_keys = [key for key in required_keys if key not in config]\r\n-        if missing_keys:\r\n+        if missing_keys := [key for key in required_keys if key not in config]:\r\n             raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n-            \r\n+\r\n         if config['d_model'] % config['n_heads'] != 0:\r\n             raise ValueError(\r\n                 f\"d_model ({config['d_model']}) must be divisible by \"\r\n                 f\"n_heads ({config['n_heads']})\"\r\n"
                },
                {
                    "date": 1733237773303,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,44 +10,36 @@\n \r\n class BaseTransformer(BaseModel, ABC):\r\n     def __init__(self, config: Dict[str, Any]):\r\n         super().__init__(config)\r\n+        print(\"\\nDEBUG: BaseTransformer initialization\")\r\n+        print(f\"DEBUG: Configuration received: {config}\")\r\n+        \r\n+        # Validate configuration\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        missing_keys = [key for key in required_keys if key not in config]\r\n+        if missing_keys:\r\n+            print(f\"DEBUG: Missing required keys: {missing_keys}\")\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n \r\n-        # Validate core configuration\r\n-        self._validate_config(config)\r\n-        \r\n-        # Initialize dimensions\r\n         self.d_model = config['d_model']\r\n-        self.input_features = config['input_features']  # Total features\r\n-        self.n_heads = config['n_heads']\r\n-        self.n_encoder_layers = config['n_encoder_layers']\r\n-        self.n_decoder_layers = config['n_decoder_layers']\r\n-        self.d_ff = config['d_ff']\r\n-        self.dropout = config['dropout']\r\n+        self.input_features = config['input_features']\r\n+        print(f\"DEBUG: Model dimensions - d_model: {self.d_model}, input_features: {self.input_features}\")\r\n         \r\n-        # Initialize embeddings with total feature count\r\n+        # Initialize embeddings\r\n+        print(\"DEBUG: Creating encoder embedding\")\r\n         self.encoder_embedding = CombinedEmbedding(\r\n             d_model=self.d_model,\r\n-            input_features=self.input_features,  # Pass total features\r\n-            dropout=self.dropout\r\n+            input_features=self.input_features,\r\n+            dropout=config['dropout']\r\n         )\r\n+        \r\n+        print(\"DEBUG: Creating decoder embedding\")\r\n         self.decoder_embedding = CombinedEmbedding(\r\n             d_model=self.d_model,\r\n-            input_features=self.input_features,  # Pass total features\r\n-            dropout=self.dropout\r\n+            input_features=self.input_features,\r\n+            dropout=config['dropout']\r\n         )\r\n-        \r\n-        # Create model components\r\n-        self.encoder_layers = self._create_encoder_layers()\r\n-        self.decoder_layers = self._create_decoder_layers()\r\n-        \r\n-        # Output projection with proper scaling\r\n-        self.output_projection = nn.Sequential(\r\n-            nn.Linear(self.d_model, self.d_model // 2),\r\n-            nn.ReLU(),\r\n-            nn.Linear(self.d_model // 2, 1)\r\n-        )\r\n-        self._initialize_output_projection()\r\n \r\n     def _validate_config(self, config: Dict[str, Any]) -> None:\r\n         \"\"\"Validate configuration parameters.\"\"\"\r\n         required_keys = ['d_model', 'n_heads', 'input_features']\r\n"
                },
                {
                    "date": 1733239064190,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -77,15 +77,23 @@\n     def _create_decoder_layers(self) -> nn.ModuleList:\r\n         \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n         pass\r\n \r\n-    def encode(\r\n-        self,\r\n-        src: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n+    def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n+              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n         \"\"\"Encode input sequence.\"\"\"\r\n+        print(f\"\\nDEBUG - Encoder devices:\")\r\n+        print(f\"Source device: {src.device}\")\r\n+        print(f\"Source mask device: {src_mask.device if src_mask is not None else 'None'}\")\r\n+        print(f\"Model device: {next(self.parameters()).device}\")\r\n+        \r\n+                # Move masks to correct device\r\n+        if src_mask is not None:\r\n+            src_mask = src_mask.to(src.device)\r\n+        if src_key_padding_mask is not None:\r\n+            src_key_padding_mask = src_key_padding_mask.to(src.device)\r\n+        \r\n+\r\n         # Apply embedding\r\n         src = self.encoder_embedding(src)\r\n         \r\n         # Pass through encoder layers\r\n"
                },
                {
                    "date": 1733239077193,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -96,14 +96,11 @@\n         # Apply embedding\r\n         src = self.encoder_embedding(src)\r\n         \r\n         # Pass through encoder layers\r\n-        for layer in self.encoder_layers:\r\n-            src = layer(\r\n-                src,\r\n-                src_mask=src_mask, \r\n-                src_key_padding_mask=src_key_padding_mask\r\n-            )\r\n+        for i, layer in enumerate(self.encoder_layers):\r\n+            print(f\"Layer {i} input device: {src.device}\")\r\n+            src = layer(src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\r\n         return src\r\n \r\n     def decode(\r\n         self,\r\n"
                },
                {
                    "date": 1733239094654,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,15 +155,14 @@\n         \r\n         return output\r\n \r\n     def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n-        \"\"\"Generate a square mask for the sequence. \r\n-        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\r\n-        \"\"\"\r\n+        \"\"\"Generate a square mask for the sequence.\"\"\"\r\n         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n-        return mask\r\n-\r\n+        # Move mask to same device as model\r\n+        return mask.to(next(self.parameters()).device)\r\n+    \r\n     def get_input_dims(self) -> int:\r\n         return self.input_features\r\n \r\n     def get_output_dims(self) -> int:\r\n"
                },
                {
                    "date": 1733239583115,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,28 +12,27 @@\n     def __init__(self, config: Dict[str, Any]):\r\n         super().__init__(config)\r\n         print(\"\\nDEBUG: BaseTransformer initialization\")\r\n         print(f\"DEBUG: Configuration received: {config}\")\r\n-        \r\n+\r\n         # Validate configuration\r\n         required_keys = ['d_model', 'n_heads', 'input_features']\r\n-        missing_keys = [key for key in required_keys if key not in config]\r\n-        if missing_keys:\r\n+        if missing_keys := [key for key in required_keys if key not in config]:\r\n             print(f\"DEBUG: Missing required keys: {missing_keys}\")\r\n             raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n \r\n         self.d_model = config['d_model']\r\n         self.input_features = config['input_features']\r\n         print(f\"DEBUG: Model dimensions - d_model: {self.d_model}, input_features: {self.input_features}\")\r\n-        \r\n+\r\n         # Initialize embeddings\r\n         print(\"DEBUG: Creating encoder embedding\")\r\n         self.encoder_embedding = CombinedEmbedding(\r\n             d_model=self.d_model,\r\n             input_features=self.input_features,\r\n             dropout=config['dropout']\r\n         )\r\n-        \r\n+\r\n         print(\"DEBUG: Creating decoder embedding\")\r\n         self.decoder_embedding = CombinedEmbedding(\r\n             d_model=self.d_model,\r\n             input_features=self.input_features,\r\n@@ -114,8 +113,13 @@\n         \"\"\"Decode target sequence.\"\"\"\r\n         # Apply embedding\r\n         tgt = self.decoder_embedding(tgt)\r\n         \r\n+        # Create or adjust masks to match sequence dimensions\r\n+        if memory_mask is not None:\r\n+            # Adjust memory mask shape to match [tgt_len, src_len]\r\n+            memory_mask = self._adjust_memory_mask(memory_mask, tgt.size(1), memory.size(1))\r\n+\r\n         # Pass through decoder layers\r\n         for layer in self.decoder_layers:\r\n             tgt = layer(\r\n                 tgt,\r\n@@ -126,8 +130,21 @@\n                 memory_key_padding_mask=memory_key_padding_mask\r\n             )\r\n         return tgt\r\n \r\n+    def _adjust_memory_mask(self, mask: Tensor, tgt_len: int, src_len: int) -> Tensor:\r\n+        \"\"\"Adjust memory mask to match required dimensions.\"\"\"\r\n+        if mask.size(0) != tgt_len or mask.size(1) != src_len:\r\n+            print(f\"DEBUG: Adjusting memory mask from {mask.shape} to ({tgt_len}, {src_len})\")\r\n+            # Create new mask with correct dimensions\r\n+            new_mask = torch.zeros((tgt_len, src_len), device=mask.device)\r\n+            # Copy values where possible\r\n+            min_rows = min(mask.size(0), tgt_len)\r\n+            min_cols = min(mask.size(1), src_len)\r\n+            new_mask[:min_rows, :min_cols] = mask[:min_rows, :min_cols]\r\n+            return new_mask\r\n+        return mask\r\n+\r\n     def forward(\r\n         self,\r\n         src: Tensor,\r\n         tgt: Tensor,\r\n@@ -135,18 +152,24 @@\n         tgt_mask: Optional[Tensor] = None,\r\n         src_key_padding_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n-        \"\"\"Forward pass that returns only the final output.\"\"\"\r\n+        \"\"\"Forward pass with correct mask handling.\"\"\"\r\n+        print(f\"\\nDEBUG - Forward shapes:\")\r\n+        print(f\"Source shape: {src.shape}\")\r\n+        print(f\"Target shape: {tgt.shape}\")\r\n+        print(f\"Source mask shape: {src_mask.shape if src_mask is not None else None}\")\r\n+        print(f\"Target mask shape: {tgt_mask.shape if tgt_mask is not None else None}\")\r\n+        \r\n         # Encode source sequence\r\n         memory = self.encode(src, src_mask, src_key_padding_mask)\r\n         \r\n-        # Decode target sequence\r\n+        # Decode target sequence with adjusted masks\r\n         output = self.decode(\r\n             tgt,\r\n             memory,\r\n             tgt_mask=tgt_mask,\r\n-            memory_mask=src_mask,\r\n+            memory_mask=src_mask,  # This will be adjusted in decode()\r\n             tgt_key_padding_mask=tgt_key_padding_mask,\r\n             memory_key_padding_mask=src_key_padding_mask\r\n         )\r\n         \r\n"
                },
                {
                    "date": 1733239713782,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,9 +21,10 @@\n             raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n \r\n         self.d_model = config['d_model']\r\n         self.input_features = config['input_features']\r\n-        print(f\"DEBUG: Model dimensions - d_model: {self.d_model}, input_features: {self.input_features}\")\r\n+        self.output_features = config.get('output_features', 1)  # Get output size\r\n+        print(f\"DEBUG: Model dimensions - d_model: {self.d_model}, input_features: {self.input_features}, output_features: {self.output_features}\")\r\n \r\n         # Initialize embeddings\r\n         print(\"DEBUG: Creating encoder embedding\")\r\n         self.encoder_embedding = CombinedEmbedding(\r\n@@ -38,8 +39,15 @@\n             input_features=self.input_features,\r\n             dropout=config['dropout']\r\n         )\r\n \r\n+        # Initialize output projection\r\n+        self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n+        print(\"DEBUG: Initialized output projection layer\")\r\n+        \r\n+        # Initialize output projection weights\r\n+        self._initialize_output_projection()\r\n+\r\n     def _validate_config(self, config: Dict[str, Any]) -> None:\r\n         \"\"\"Validate configuration parameters.\"\"\"\r\n         required_keys = ['d_model', 'n_heads', 'input_features']\r\n         if missing_keys := [key for key in required_keys if key not in config]:\r\n"
                },
                {
                    "date": 1733239755789,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,14 +67,13 @@\n             dropout=self.dropout\r\n         )\r\n \r\n     def _initialize_output_projection(self) -> None:\r\n-        \"\"\"Initialize output projection layers.\"\"\"\r\n-        for layer in self.output_projection:\r\n-            if isinstance(layer, nn.Linear):\r\n-                nn.init.xavier_uniform_(layer.weight)\r\n-                if layer.bias is not None:\r\n-                    nn.init.zeros_(layer.bias)\r\n+        \"\"\"Initialize output projection layer weights.\"\"\"\r\n+        # Initialize weights with Xavier/Glorot initialization\r\n+        nn.init.xavier_uniform_(self.output_projection.weight)\r\n+        if self.output_projection.bias is not None:\r\n+            nn.init.zeros_(self.output_projection.bias)\r\n \r\n     @abstractmethod\r\n     def _create_encoder_layers(self) -> nn.ModuleList:\r\n         \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n"
                },
                {
                    "date": 1733239917919,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,8 +46,11 @@\n         \r\n         # Initialize output projection weights\r\n         self._initialize_output_projection()\r\n \r\n+        self.debug_counter = 0\r\n+        self.max_debug_prints = 2  # Only print first 2 times\r\n+\r\n     def _validate_config(self, config: Dict[str, Any]) -> None:\r\n         \"\"\"Validate configuration parameters.\"\"\"\r\n         required_keys = ['d_model', 'n_heads', 'input_features']\r\n         if missing_keys := [key for key in required_keys if key not in config]:\r\n@@ -86,12 +89,14 @@\n \r\n     def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n               src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n         \"\"\"Encode input sequence.\"\"\"\r\n-        print(f\"\\nDEBUG - Encoder devices:\")\r\n-        print(f\"Source device: {src.device}\")\r\n-        print(f\"Source mask device: {src_mask.device if src_mask is not None else 'None'}\")\r\n-        print(f\"Model device: {next(self.parameters()).device}\")\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            print(f\"\\nDEBUG - Encoder devices (call {self.debug_counter + 1}/{self.max_debug_prints}):\")\r\n+            print(f\"Source device: {src.device}\")\r\n+            print(f\"Source mask device: {src_mask.device if src_mask is not None else 'None'}\")\r\n+            print(f\"Model device: {next(self.parameters()).device}\")\r\n+            self.debug_counter += 1\r\n         \r\n                 # Move masks to correct device\r\n         if src_mask is not None:\r\n             src_mask = src_mask.to(src.device)\r\n@@ -160,13 +165,14 @@\n         src_key_padding_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n         \"\"\"Forward pass with correct mask handling.\"\"\"\r\n-        print(f\"\\nDEBUG - Forward shapes:\")\r\n-        print(f\"Source shape: {src.shape}\")\r\n-        print(f\"Target shape: {tgt.shape}\")\r\n-        print(f\"Source mask shape: {src_mask.shape if src_mask is not None else None}\")\r\n-        print(f\"Target mask shape: {tgt_mask.shape if tgt_mask is not None else None}\")\r\n+        if self.debug_counter < self.max_debug_prints:  # Reuse same counter\r\n+            print(f\"\\nDEBUG - Forward shapes (call {self.debug_counter + 1}/{self.max_debug_prints}):\")\r\n+            print(f\"Source shape: {src.shape}\")\r\n+            print(f\"Target shape: {tgt.shape}\")\r\n+            print(f\"Source mask shape: {src_mask.shape if src_mask is not None else None}\")\r\n+            print(f\"Target mask shape: {tgt_mask.shape if tgt_mask is not None else None}\")\r\n         \r\n         # Encode source sequence\r\n         memory = self.encode(src, src_mask, src_key_padding_mask)\r\n         \r\n"
                },
                {
                    "date": 1733250927382,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,51 +5,64 @@\n \r\n from models.base.base_model import BaseModel\r\n from models.components.embeddings import CombinedEmbedding\r\n from models.components.attention import MultiHeadAttention\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n \r\n-\r\n class BaseTransformer(BaseModel, ABC):\r\n     def __init__(self, config: Dict[str, Any]):\r\n         super().__init__(config)\r\n-        print(\"\\nDEBUG: BaseTransformer initialization\")\r\n-        print(f\"DEBUG: Configuration received: {config}\")\r\n+        \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"BaseTransformer\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n \r\n         # Validate configuration\r\n         required_keys = ['d_model', 'n_heads', 'input_features']\r\n         if missing_keys := [key for key in required_keys if key not in config]:\r\n-            print(f\"DEBUG: Missing required keys: {missing_keys}\")\r\n+            self.logger.error(\"Missing required configuration keys\", {\"missing_keys\": missing_keys})\r\n             raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n \r\n         self.d_model = config['d_model']\r\n         self.input_features = config['input_features']\r\n-        self.output_features = config.get('output_features', 1)  # Get output size\r\n-        print(f\"DEBUG: Model dimensions - d_model: {self.d_model}, input_features: {self.input_features}, output_features: {self.output_features}\")\r\n+        self.output_features = config.get('output_features', 1)\r\n+        \r\n+        self.logger.debug(\"Model dimensions initialized\", {\r\n+            \"d_model\": self.d_model,\r\n+            \"input_features\": self.input_features,\r\n+            \"output_features\": self.output_features\r\n+        })\r\n \r\n         # Initialize embeddings\r\n-        print(\"DEBUG: Creating encoder embedding\")\r\n+        self.logger.debug(\"Creating encoder embedding\")\r\n         self.encoder_embedding = CombinedEmbedding(\r\n             d_model=self.d_model,\r\n             input_features=self.input_features,\r\n             dropout=config['dropout']\r\n         )\r\n \r\n-        print(\"DEBUG: Creating decoder embedding\")\r\n+        self.logger.debug(\"Creating decoder embedding\")\r\n         self.decoder_embedding = CombinedEmbedding(\r\n             d_model=self.d_model,\r\n             input_features=self.input_features,\r\n             dropout=config['dropout']\r\n         )\r\n \r\n         # Initialize output projection\r\n         self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n-        print(\"DEBUG: Initialized output projection layer\")\r\n+        self.logger.debug(\"Initialized output projection layer\")\r\n         \r\n         # Initialize output projection weights\r\n         self._initialize_output_projection()\r\n \r\n         self.debug_counter = 0\r\n-        self.max_debug_prints = 2  # Only print first 2 times\r\n+        self.max_debug_prints = 2\r\n \r\n     def _validate_config(self, config: Dict[str, Any]) -> None:\r\n         \"\"\"Validate configuration parameters.\"\"\"\r\n         required_keys = ['d_model', 'n_heads', 'input_features']\r\n@@ -90,27 +103,30 @@\n     def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n               src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n         \"\"\"Encode input sequence.\"\"\"\r\n         if self.debug_counter < self.max_debug_prints:\r\n-            print(f\"\\nDEBUG - Encoder devices (call {self.debug_counter + 1}/{self.max_debug_prints}):\")\r\n-            print(f\"Source device: {src.device}\")\r\n-            print(f\"Source mask device: {src_mask.device if src_mask is not None else 'None'}\")\r\n-            print(f\"Model device: {next(self.parameters()).device}\")\r\n+            self.logger.debug(f\"Encoder device status (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"source_device\": str(src.device),\r\n+                \"source_mask_device\": str(src_mask.device) if src_mask is not None else \"None\",\r\n+                \"model_device\": str(next(self.parameters()).device)\r\n+            })\r\n             self.debug_counter += 1\r\n         \r\n-                # Move masks to correct device\r\n+        # Move masks to correct device\r\n         if src_mask is not None:\r\n             src_mask = src_mask.to(src.device)\r\n         if src_key_padding_mask is not None:\r\n             src_key_padding_mask = src_key_padding_mask.to(src.device)\r\n-        \r\n \r\n         # Apply embedding\r\n         src = self.encoder_embedding(src)\r\n         \r\n         # Pass through encoder layers\r\n         for i, layer in enumerate(self.encoder_layers):\r\n-            print(f\"Layer {i} input device: {src.device}\")\r\n+            self.logger.debug(\"Encoder layer processing\", {\r\n+                \"layer\": i,\r\n+                \"input_device\": str(src.device)\r\n+            })\r\n             src = layer(src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\r\n         return src\r\n \r\n     def decode(\r\n@@ -145,9 +161,12 @@\n \r\n     def _adjust_memory_mask(self, mask: Tensor, tgt_len: int, src_len: int) -> Tensor:\r\n         \"\"\"Adjust memory mask to match required dimensions.\"\"\"\r\n         if mask.size(0) != tgt_len or mask.size(1) != src_len:\r\n-            print(f\"DEBUG: Adjusting memory mask from {mask.shape} to ({tgt_len}, {src_len})\")\r\n+            self.logger.debug(\"Adjusting memory mask dimensions\", {\r\n+                \"original_shape\": tuple(mask.shape),\r\n+                \"new_shape\": (tgt_len, src_len)\r\n+            })\r\n             # Create new mask with correct dimensions\r\n             new_mask = torch.zeros((tgt_len, src_len), device=mask.device)\r\n             # Copy values where possible\r\n             min_rows = min(mask.size(0), tgt_len)\r\n@@ -165,14 +184,15 @@\n         src_key_padding_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n         \"\"\"Forward pass with correct mask handling.\"\"\"\r\n-        if self.debug_counter < self.max_debug_prints:  # Reuse same counter\r\n-            print(f\"\\nDEBUG - Forward shapes (call {self.debug_counter + 1}/{self.max_debug_prints}):\")\r\n-            print(f\"Source shape: {src.shape}\")\r\n-            print(f\"Target shape: {tgt.shape}\")\r\n-            print(f\"Source mask shape: {src_mask.shape if src_mask is not None else None}\")\r\n-            print(f\"Target mask shape: {tgt_mask.shape if tgt_mask is not None else None}\")\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            self.logger.debug(f\"Forward pass details (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"source_shape\": tuple(src.shape),\r\n+                \"target_shape\": tuple(tgt.shape),\r\n+                \"source_mask_shape\": tuple(src_mask.shape) if src_mask is not None else None,\r\n+                \"target_mask_shape\": tuple(tgt_mask.shape) if tgt_mask is not None else None\r\n+            })\r\n         \r\n         # Encode source sequence\r\n         memory = self.encode(src, src_mask, src_key_padding_mask)\r\n         \r\n"
                }
            ],
            "date": 1733089622876,
            "name": "Commit-0",
            "content": "from abc import ABC, abstractmethod\r\nfrom typing import Optional, Dict, Any, Tuple\r\nimport torch\r\nfrom torch import nn, Tensor\r\n\r\nfrom models.base.base_model import BaseModel\r\nfrom models.components.embeddings import CombinedEmbedding\r\nfrom models.components.attention import MultiHeadAttention\r\n\r\n\r\nclass BaseTransformer(BaseModel, ABC):\r\n    def __init__(self, config: Dict[str, Any]):\r\n        super().__init__(config)\r\n\r\n        self.d_model = config['d_model']\r\n        self.n_heads = config['n_heads']\r\n        self.n_encoder_layers = config['n_encoder_layers']\r\n        self.n_decoder_layers = config['n_decoder_layers']\r\n        self.d_ff = config['d_ff']\r\n        self.dropout = config['dropout']\r\n        self.input_features = config['input_features']\r\n        self.batch_first = config.get('batch_first', True)  # Default to batch_first=True\r\n\r\n        # Common components with proper initialization\r\n        self.encoder_embedding = CombinedEmbedding(\r\n            self.d_model,\r\n            self.input_features,\r\n            dropout=self.dropout\r\n        )\r\n        self.decoder_embedding = CombinedEmbedding(\r\n            self.d_model,\r\n            self.input_features,\r\n            dropout=self.dropout\r\n        )\r\n\r\n        # Initialize encoder and decoder layers\r\n        self.encoder_layers = self._create_encoder_layers()\r\n        self.decoder_layers = self._create_decoder_layers()\r\n\r\n        # Output projection with proper initialization\r\n        self.output_projection = nn.Linear(self.d_model, 1)\r\n        nn.init.xavier_uniform_(self.output_projection.weight)\r\n        nn.init.zeros_(self.output_projection.bias)\r\n\r\n    @abstractmethod\r\n    def _create_encoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def _create_decoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n        pass\r\n\r\n    def encode(\r\n        self,\r\n        src: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Encode input sequence.\"\"\"\r\n        # Apply embedding\r\n        src = self.encoder_embedding(src)\r\n        \r\n        # Pass through encoder layers\r\n        for layer in self.encoder_layers:\r\n            src = layer(\r\n                src,\r\n                src_mask=src_mask, \r\n                src_key_padding_mask=src_key_padding_mask\r\n            )\r\n        return src\r\n\r\n    def decode(\r\n        self,\r\n        tgt: Tensor,\r\n        memory: Tensor,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        memory_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n        memory_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Decode target sequence.\"\"\"\r\n        # Apply embedding\r\n        tgt = self.decoder_embedding(tgt)\r\n        \r\n        # Pass through decoder layers\r\n        for layer in self.decoder_layers:\r\n            tgt = layer(\r\n                tgt,\r\n                memory,\r\n                tgt_mask=tgt_mask,\r\n                memory_mask=memory_mask,\r\n                tgt_key_padding_mask=tgt_key_padding_mask,\r\n                memory_key_padding_mask=memory_key_padding_mask\r\n            )\r\n        return tgt\r\n\r\n    def forward(\r\n        self,\r\n        src: Tensor,\r\n        tgt: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Forward pass that returns only the final output.\"\"\"\r\n        # Encode source sequence\r\n        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n        \r\n        # Decode target sequence\r\n        output = self.decode(\r\n            tgt,\r\n            memory,\r\n            tgt_mask=tgt_mask,\r\n            memory_mask=src_mask,\r\n            tgt_key_padding_mask=tgt_key_padding_mask,\r\n            memory_key_padding_mask=src_key_padding_mask\r\n        )\r\n        \r\n        # Project to output dimension\r\n        output = self.output_projection(output)\r\n        \r\n        return output\r\n\r\n    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n        \"\"\"Generate a square mask for the sequence. \r\n        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\r\n        \"\"\"\r\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n        return mask\r\n\r\n    def get_input_dims(self) -> int:\r\n        return self.input_features\r\n\r\n    def get_output_dims(self) -> int:\r\n        return 1"
        }
    ]
}