{
    "sourceFile": "models/architectures/transformers/base_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 11,
            "patches": [
                {
                    "date": 1733264259467,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733264330905,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,5 @@\n-from abc import ABC\r\n+from abc import ABC, abstractmethod\r\n import math\r\n import torch\r\n from torch import nn, Tensor\r\n from typing import Optional, Dict, Any, Tuple, cast\r\n@@ -61,8 +61,18 @@\n             'forward_pass_count': 0,\r\n             'peak_memory': 0.0\r\n         }\r\n \r\n+    @abstractmethod\r\n+    def get_input_dims(self) -> int:\r\n+        \"\"\"Get input dimensions.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def get_output_dims(self) -> int:\r\n+        \"\"\"Get output dimensions.\"\"\"\r\n+        pass\r\n+\r\n     def _setup_model_dimensions(self, config: Dict[str, Any]) -> None:\r\n         \"\"\"Set up model dimensions with optimized defaults for 8GB VRAM.\"\"\"\r\n         self.d_model = config['d_model']\r\n         self.input_features = config['input_features']\r\n"
                },
                {
                    "date": 1733264383569,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,8 +61,32 @@\n             'forward_pass_count': 0,\r\n             'peak_memory': 0.0\r\n         }\r\n \r\n+    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n+        \"\"\"Validate transformer configuration parameters.\"\"\"\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        for key in required_keys:\r\n+            if key not in config:\r\n+                raise ValueError(f\"Missing required config parameter: {key}\")\r\n+                \r\n+        if config['d_model'] % config['n_heads'] != 0:\r\n+            raise ValueError(\r\n+                f\"d_model ({config['d_model']}) must be divisible by \"\r\n+                f\"n_heads ({config['n_heads']})\"\r\n+            )\r\n+            \r\n+        if config['d_model'] <= 0 or config['n_heads'] <= 0:\r\n+            raise ValueError(\r\n+                \"d_model and n_heads must be positive integers\"\r\n+            )\r\n+            \r\n+        if 'dropout' in config and not 0 <= config['dropout'] <= 1:\r\n+            raise ValueError(\"dropout must be between 0 and 1\")\r\n+            \r\n+        if 'attention_dropout' in config and not 0 <= config['attention_dropout'] <= 1:\r\n+            raise ValueError(\"attention_dropout must be between 0 and 1\")\r\n+\r\n     @abstractmethod\r\n     def get_input_dims(self) -> int:\r\n         \"\"\"Get input dimensions.\"\"\"\r\n         pass\r\n"
                },
                {
                    "date": 1733264396100,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,11 +1,11 @@\n from abc import ABC, abstractmethod\r\n+from typing import Optional, Dict, Any, Tuple, cast\r\n import math\r\n import torch\r\n from torch import nn, Tensor\r\n-from typing import Optional, Dict, Any, Tuple, cast\r\n import torch.nn.functional as F\r\n-from torch.utils.checkpoint import checkpoint\r\n+\r\n from models.base.base_model import BaseModel\r\n from models.components.embeddings import CombinedEmbedding\r\n from models.components.attention import MultiHeadAttention\r\n from utils.logging.logger import Logger\r\n@@ -61,42 +61,8 @@\n             'forward_pass_count': 0,\r\n             'peak_memory': 0.0\r\n         }\r\n \r\n-    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n-        \"\"\"Validate transformer configuration parameters.\"\"\"\r\n-        required_keys = ['d_model', 'n_heads', 'input_features']\r\n-        for key in required_keys:\r\n-            if key not in config:\r\n-                raise ValueError(f\"Missing required config parameter: {key}\")\r\n-                \r\n-        if config['d_model'] % config['n_heads'] != 0:\r\n-            raise ValueError(\r\n-                f\"d_model ({config['d_model']}) must be divisible by \"\r\n-                f\"n_heads ({config['n_heads']})\"\r\n-            )\r\n-            \r\n-        if config['d_model'] <= 0 or config['n_heads'] <= 0:\r\n-            raise ValueError(\r\n-                \"d_model and n_heads must be positive integers\"\r\n-            )\r\n-            \r\n-        if 'dropout' in config and not 0 <= config['dropout'] <= 1:\r\n-            raise ValueError(\"dropout must be between 0 and 1\")\r\n-            \r\n-        if 'attention_dropout' in config and not 0 <= config['attention_dropout'] <= 1:\r\n-            raise ValueError(\"attention_dropout must be between 0 and 1\")\r\n-\r\n-    @abstractmethod\r\n-    def get_input_dims(self) -> int:\r\n-        \"\"\"Get input dimensions.\"\"\"\r\n-        pass\r\n-\r\n-    @abstractmethod\r\n-    def get_output_dims(self) -> int:\r\n-        \"\"\"Get output dimensions.\"\"\"\r\n-        pass\r\n-\r\n     def _setup_model_dimensions(self, config: Dict[str, Any]) -> None:\r\n         \"\"\"Set up model dimensions with optimized defaults for 8GB VRAM.\"\"\"\r\n         self.d_model = config['d_model']\r\n         self.input_features = config['input_features']\r\n@@ -123,9 +89,9 @@\n         return CombinedEmbedding(\r\n             d_model=self.d_model,\r\n             input_features=self.input_features,\r\n             dropout=self.dropout,\r\n-            max_seq_len=self.config.get('max_seq_len', 5000)  # Changed parameter name\r\n+            max_sequence_length=self.config.get('max_sequence_length', 5000)\r\n         )\r\n \r\n     def _compute_attention_chunked(\r\n         self, \r\n@@ -133,9 +99,9 @@\n         key: Tensor, \r\n         value: Tensor, \r\n         mask: Optional[Tensor] = None\r\n     ) -> Tensor:\r\n-        \"\"\"Compute attention scores in chunks to save memory.\"\"\" \r\n+        \"\"\"Compute attention scores in chunks to save memory.\"\"\"\r\n         batch_size, seq_len, _ = query.size()\r\n         chunks = math.ceil(seq_len / self.attention_batch_size)\r\n         \r\n         outputs = []\r\n@@ -172,18 +138,20 @@\n         \"\"\"Memory-efficient encoding process.\"\"\"\r\n         if torch.cuda.is_available():\r\n             init_mem = torch.cuda.memory_allocated()\r\n         \r\n+        # Apply embedding\r\n         src = self.encoder_embedding(src)\r\n         \r\n+        # Process encoder layers with checkpointing if enabled\r\n         for i, layer in enumerate(self.encoder_layers):\r\n             if self.use_checkpointing and self.training:\r\n-                src = cast(Tensor, checkpoint(  # Cast checkpoint result to Tensor\r\n+                src = torch.utils.checkpoint.checkpoint(\r\n                     layer,\r\n                     src,\r\n                     src_mask,\r\n                     src_key_padding_mask\r\n-                ))\r\n+                )\r\n             else:\r\n                 src = layer(\r\n                     src,\r\n                     src_mask=src_mask,\r\n@@ -205,21 +173,23 @@\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n         memory_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n         \"\"\"Memory-efficient decoding process.\"\"\"\r\n+        # Apply embedding\r\n         tgt = self.decoder_embedding(tgt)\r\n         \r\n+        # Process decoder layers with optional checkpointing\r\n         for layer in self.decoder_layers:\r\n             if self.use_checkpointing and self.training:\r\n-                tgt = cast(Tensor, checkpoint(  # Cast checkpoint result to Tensor\r\n+                tgt = torch.utils.checkpoint.checkpoint(\r\n                     layer,\r\n                     tgt,\r\n                     memory,\r\n                     tgt_mask,\r\n                     memory_mask,\r\n                     tgt_key_padding_mask,\r\n                     memory_key_padding_mask\r\n-                ))\r\n+                )\r\n             else:\r\n                 tgt = layer(\r\n                     tgt,\r\n                     memory,\r\n"
                },
                {
                    "date": 1733264729914,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,40 +1,59 @@\n from abc import ABC, abstractmethod\r\n-from typing import Optional, Dict, Any, Tuple, cast\r\n+from typing import Optional, Dict, Any, Tuple, cast, Callable\r\n import math\r\n import torch\r\n from torch import nn, Tensor\r\n import torch.nn.functional as F\r\n+from torch.utils.checkpoint import checkpoint\r\n \r\n from models.base.base_model import BaseModel\r\n from models.components.embeddings import CombinedEmbedding\r\n from models.components.attention import MultiHeadAttention\r\n from utils.logging.logger import Logger\r\n from utils.logging.config import LoggerConfig, LogLevel\r\n \r\n class BaseTransformer(BaseModel, ABC):\r\n+    \"\"\"Base class for transformer models with memory-efficient implementation.\r\n+    \r\n+    This implementation is optimized for training on GPUs with limited VRAM (e.g., RTX 4060 8GB)\r\n+    and includes features like gradient checkpointing, Flash Attention when available, and\r\n+    chunked attention computation for memory efficiency.\r\n+    \"\"\"\r\n+\r\n     def __init__(self, config: Dict[str, Any]):\r\n+        \"\"\"Initialize the transformer with the given configuration.\r\n+        \r\n+        Args:\r\n+            config: A dictionary containing model configuration parameters including:\r\n+                - d_model: The model's hidden dimension size\r\n+                - n_heads: Number of attention heads\r\n+                - input_features: Number of input features\r\n+                - dropout: Dropout rate\r\n+                - use_checkpointing: Whether to use gradient checkpointing\r\n+                - use_flash_attention: Whether to use Flash Attention when available\r\n+                - attention_batch_size: Size of chunks for attention computation\r\n+        \"\"\"\r\n         super().__init__(config)\r\n         \r\n-        # Initialize logger with performance monitoring\r\n+        # Initialize logger\r\n         logger_config = LoggerConfig(\r\n             level=LogLevel.INFO,\r\n             component_name=\"BaseTransformer\",\r\n             include_timestamp=True\r\n         )\r\n         self.logger = Logger.get_logger(__name__, logger_config)\r\n         self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n \r\n-        # Validate and set up configuration\r\n+        # Validate configuration\r\n         self._validate_config(config)\r\n         self._setup_model_dimensions(config)\r\n         \r\n         # Memory optimization flags\r\n         self.use_checkpointing = config.get('use_checkpointing', True)\r\n         self.attention_dropout = config.get('attention_dropout', 0.1)\r\n-        self.activation_checkpointing = config.get('activation_checkpointing', True)\r\n         \r\n-        # Initialize embeddings with memory optimization\r\n+        # Initialize embeddings\r\n         self.encoder_embedding = self._create_embedding()\r\n         self.decoder_embedding = self._create_embedding()\r\n         \r\n         # Initialize flash attention if available\r\n@@ -48,9 +67,9 @@\n                 self.has_flash_attention = False\r\n                 self.logger.warning(\"Flash Attention not available, falling back to standard attention\")\r\n \r\n         # Memory-efficient attention settings\r\n-        self.attention_batch_size = config.get('attention_batch_size', 1024)  # For chunked attention computation\r\n+        self.attention_batch_size = config.get('attention_batch_size', 1024)\r\n         \r\n         # Output projection with improved initialization\r\n         self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n         self._initialize_output_projection()\r\n@@ -61,47 +80,113 @@\n             'forward_pass_count': 0,\r\n             'peak_memory': 0.0\r\n         }\r\n \r\n+    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n+        \"\"\"Validate the configuration parameters.\"\"\"\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        missing_keys = [key for key in required_keys if key not in config]\r\n+        if missing_keys:\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n     def _setup_model_dimensions(self, config: Dict[str, Any]) -> None:\r\n-        \"\"\"Set up model dimensions with optimized defaults for 8GB VRAM.\"\"\"\r\n+        \"\"\"Set up model dimensions and validate their compatibility.\"\"\"\r\n         self.d_model = config['d_model']\r\n         self.input_features = config['input_features']\r\n         self.output_features = config.get('output_features', 1)\r\n         \r\n-        # Calculate optimal attention dimensions\r\n+        # Calculate attention dimensions\r\n         self.n_heads = config['n_heads']\r\n         self.head_dim = self.d_model // self.n_heads\r\n         \r\n-        # Memory-efficient settings\r\n+        # Validate dimensions\r\n+        if self.d_model % self.n_heads != 0:\r\n+            raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({self.n_heads})\")\r\n+        \r\n+        # Set dropout rates\r\n         self.dropout = config.get('dropout', 0.1)\r\n         self.attention_dropout = config.get('attention_dropout', 0.1)\r\n-        \r\n-        self.logger.debug(\"Model dimensions initialized\", {\r\n-            \"d_model\": self.d_model,\r\n-            \"input_features\": self.input_features,\r\n-            \"output_features\": self.output_features,\r\n-            \"n_heads\": self.n_heads,\r\n-            \"head_dim\": self.head_dim\r\n-        })\r\n \r\n     def _create_embedding(self) -> CombinedEmbedding:\r\n-        \"\"\"Create memory-efficient embedding layer.\"\"\"\r\n+        \"\"\"Create the embedding layer with proper initialization.\"\"\"\r\n         return CombinedEmbedding(\r\n             d_model=self.d_model,\r\n             input_features=self.input_features,\r\n-            dropout=self.dropout,\r\n-            max_sequence_length=self.config.get('max_sequence_length', 5000)\r\n+            dropout=self.dropout\r\n         )\r\n \r\n+    def _initialize_output_projection(self) -> None:\r\n+        \"\"\"Initialize the output projection layer with Xavier initialization.\"\"\"\r\n+        nn.init.xavier_uniform_(self.output_projection.weight)\r\n+        if self.output_projection.bias is not None:\r\n+            nn.init.zeros_(self.output_projection.bias)\r\n+\r\n+    def _create_encoder_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n+        \"\"\"Create a type-safe forward function for encoder checkpointing.\r\n+        \r\n+        Args:\r\n+            layer: The encoder layer module\r\n+            \r\n+        Returns:\r\n+            A callable that handles the forward pass with proper type safety\r\n+        \"\"\"\r\n+        def custom_forward(*inputs: Any) -> Tensor:\r\n+            if not all(isinstance(x, (torch.Tensor, type(None))) for x in inputs):\r\n+                raise TypeError(\"All inputs must be tensors or None\")\r\n+            \r\n+            src = cast(Tensor, inputs[0])\r\n+            src_mask = inputs[1]  # Optional[Tensor]\r\n+            src_key_padding_mask = inputs[2]  # Optional[Tensor]\r\n+            \r\n+            return layer(\r\n+                src,\r\n+                src_mask=src_mask,\r\n+                src_key_padding_mask=src_key_padding_mask\r\n+            )\r\n+        return custom_forward\r\n+\r\n+    def _create_decoder_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n+        \"\"\"Create a type-safe forward function for decoder checkpointing.\r\n+        \r\n+        Args:\r\n+            layer: The decoder layer module\r\n+            \r\n+        Returns:\r\n+            A callable that handles the forward pass with proper type safety\r\n+        \"\"\"\r\n+        def custom_forward(*inputs: Any) -> Tensor:\r\n+            if not all(isinstance(x, (torch.Tensor, type(None))) for x in inputs):\r\n+                raise TypeError(\"All inputs must be tensors or None\")\r\n+            \r\n+            tgt = cast(Tensor, inputs[0])\r\n+            memory = cast(Tensor, inputs[1])\r\n+            tgt_mask = inputs[2]  # Optional[Tensor]\r\n+            memory_mask = inputs[3]  # Optional[Tensor]\r\n+            tgt_key_padding_mask = inputs[4]  # Optional[Tensor]\r\n+            memory_key_padding_mask = inputs[5]  # Optional[Tensor]\r\n+            \r\n+            return layer(\r\n+                tgt,\r\n+                memory,\r\n+                tgt_mask=tgt_mask,\r\n+                memory_mask=memory_mask,\r\n+                tgt_key_padding_mask=tgt_key_padding_mask,\r\n+                memory_key_padding_mask=memory_key_padding_mask\r\n+            )\r\n+        return custom_forward\r\n+\r\n     def _compute_attention_chunked(\r\n         self, \r\n         query: Tensor, \r\n         key: Tensor, \r\n         value: Tensor, \r\n         mask: Optional[Tensor] = None\r\n     ) -> Tensor:\r\n-        \"\"\"Compute attention scores in chunks to save memory.\"\"\"\r\n+        \"\"\"Compute attention scores in memory-efficient chunks.\r\n+        \r\n+        This method processes attention computation in smaller chunks to reduce memory\r\n+        usage, which is particularly important for long sequences.\r\n+        \"\"\"\r\n         batch_size, seq_len, _ = query.size()\r\n         chunks = math.ceil(seq_len / self.attention_batch_size)\r\n         \r\n         outputs = []\r\n@@ -120,9 +205,9 @@\n                     dropout_p=self.attention_dropout if self.training else 0.0,\r\n                     is_causal=False\r\n                 )\r\n             else:\r\n-                # Standard attention for the chunk\r\n+                # Standard attention computation\r\n                 scores = torch.matmul(chunk_q, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\r\n                 if chunk_mask is not None:\r\n                     scores = scores.masked_fill(chunk_mask == 0, float('-inf'))\r\n                 weights = F.softmax(scores, dim=-1)\r\n@@ -134,24 +219,34 @@\n         return torch.cat(outputs, dim=1)\r\n \r\n     def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n               src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n-        \"\"\"Memory-efficient encoding process.\"\"\"\r\n+        \"\"\"Encode input sequence with memory-efficient processing.\r\n+        \r\n+        Args:\r\n+            src: Source sequence tensor\r\n+            src_mask: Optional mask for source sequence\r\n+            src_key_padding_mask: Optional key padding mask for source sequence\r\n+            \r\n+        Returns:\r\n+            Encoded sequence tensor\r\n+        \"\"\"\r\n         if torch.cuda.is_available():\r\n             init_mem = torch.cuda.memory_allocated()\r\n         \r\n         # Apply embedding\r\n         src = self.encoder_embedding(src)\r\n         \r\n-        # Process encoder layers with checkpointing if enabled\r\n+        # Process encoder layers with optional checkpointing\r\n         for i, layer in enumerate(self.encoder_layers):\r\n             if self.use_checkpointing and self.training:\r\n-                src = torch.utils.checkpoint.checkpoint(\r\n-                    layer,\r\n-                    src,\r\n-                    src_mask,\r\n-                    src_key_padding_mask\r\n+                custom_forward = self._create_encoder_forward(layer)\r\n+                output = checkpoint(\r\n+                    custom_forward,\r\n+                    src, src_mask, src_key_padding_mask,\r\n+                    preserve_rng_state=True\r\n                 )\r\n+                src = cast(Tensor, output)\r\n             else:\r\n                 src = layer(\r\n                     src,\r\n                     src_mask=src_mask,\r\n@@ -172,24 +267,35 @@\n         memory_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n         memory_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n-        \"\"\"Memory-efficient decoding process.\"\"\"\r\n+        \"\"\"Decode target sequence with memory-efficient processing.\r\n+        \r\n+        Args:\r\n+            tgt: Target sequence tensor\r\n+            memory: Memory from encoder\r\n+            tgt_mask: Optional mask for target sequence\r\n+            memory_mask: Optional mask for memory\r\n+            tgt_key_padding_mask: Optional key padding mask for target sequence\r\n+            memory_key_padding_mask: Optional key padding mask for memory\r\n+            \r\n+        Returns:\r\n+            Decoded sequence tensor\r\n+        \"\"\"\r\n         # Apply embedding\r\n         tgt = self.decoder_embedding(tgt)\r\n         \r\n         # Process decoder layers with optional checkpointing\r\n         for layer in self.decoder_layers:\r\n             if self.use_checkpointing and self.training:\r\n-                tgt = torch.utils.checkpoint.checkpoint(\r\n-                    layer,\r\n-                    tgt,\r\n-                    memory,\r\n-                    tgt_mask,\r\n-                    memory_mask,\r\n-                    tgt_key_padding_mask,\r\n-                    memory_key_padding_mask\r\n+                custom_forward = self._create_decoder_forward(layer)\r\n+                output = checkpoint(\r\n+                    custom_forward,\r\n+                    tgt, memory, tgt_mask, memory_mask,\r\n+                    tgt_key_padding_mask, memory_key_padding_mask,\r\n+                    preserve_rng_state=True\r\n                 )\r\n+                tgt = cast(Tensor, output)\r\n             else:\r\n                 tgt = layer(\r\n                     tgt,\r\n                     memory,\r\n@@ -208,9 +314,21 @@\n         tgt_mask: Optional[Tensor] = None,\r\n         src_key_padding_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n-        \"\"\"Optimized forward pass with memory tracking.\"\"\"\r\n+        \"\"\"Forward pass with memory tracking and optimization.\r\n+        \r\n+        Args:\r\n+            src: Source sequence tensor\r\n+            tgt: Target sequence tensor\r\n+            src_mask: Optional mask for source sequence\r\n+            tgt_mask: Optional mask for target sequence\r\n+            src_key_padding_mask: Optional key padding mask for source sequence\r\n+            tgt_key_padding_mask: Optional key padding mask for target sequence\r\n+            \r\n+        Returns:\r\n+            Output tensor from the transformer\r\n+        \"\"\"\r\n         if torch.cuda.is_available():\r\n             torch.cuda.reset_peak_memory_stats()\r\n             \r\n         # Encode source sequence\r\n@@ -220,14 +338,14 @@\n         output = self.decode(\r\n             tgt,\r\n             memory,\r\n             tgt_mask=tgt_mask,\r\n-            memory_mask=None,  # Memory mask not needed in transformer\r\n+            memory_mask=None,\r\n             tgt_key_padding_mask=tgt_key_padding_mask,\r\n             memory_key_padding_mask=src_key_padding_mask\r\n         )\r\n         \r\n-        # Project to output dimension with fused operations\r\n+        # Project to output dimension\r\n         output = self.output_projection(output)\r\n         \r\n         # Track performance metrics\r\n         self.perf_stats['forward_pass_count'] += 1\r\n@@ -237,25 +355,200 @@\n         \r\n         return output\r\n \r\n     def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n-        \"\"\"Generate an optimized attention mask.\"\"\"\r\n+        \"\"\"Generate an optimized attention mask.\r\n+        \r\n+        Args:\r\n+            sz: Size of the square mask\r\n+            \r\n+        Returns:\r\n+            A square mask tensor for attention\r\n+        \"\"\"\r\n         mask = torch.triu(torch.ones(sz, sz), diagonal=1)\r\n         mask = mask.float().masked_fill(mask == 1, float('-inf'))\r\n-        \r\n-        # Move mask to same device as model parameters\r\n         device = next(self.parameters()).device\r\n         return mask.to(device=device)\r\n \r\n-    def _check_memory_usage(self) -> None:\r\n-        \"\"\"Monitor GPU memory usage.\"\"\"\r\n-        if torch.cuda.is_available():\r\n-            current_mem = torch.cuda.memory_allocated() / 1024**2  # MB\r\n-            peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB\r\n\\ No newline at end of file\n-            self.logger.debug(f\"Current GPU memory: {current_mem:.2f}MB, Peak: {peak_mem:.2f}MB\")\r\n+    @abstractmethod\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n \r\n+    @abstractmethod\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n     def get_performance_stats(self) -> Dict[str, float]:\r\n-        \"\"\"Return performance statistics.\"\"\"\r\n+        \"\"\"Return performance statistics for monitoring.\r\n+        \r\n+        Returns:\r\n+            Dictionary containing performance metrics including memory usage,\r\n+            forward pass counts, and average memory consumption per forward pass.\r\n+        \"\"\"\r\n         stats = self.perf_stats.copy()\r\n         if self.perf_stats['forward_pass_count'] > 0:\r\n-            stats['avg_memory_per_forward'] = self.perf_stats['peak_memory'] / self.perf_stats['forward_pass_count']\r\n-        return stats\n+            stats['avg_memory_per_forward'] = (\r\n+                self.perf_stats['peak_memory'] / \r\n+                self.perf_stats['forward_pass_count']\r\n+            )\r\n+        return stats\r\n+    \r\n+    def get_attention_stats(self) -> Dict[str, Any]:\r\n+        \"\"\"Return statistics about attention mechanism usage.\r\n+        \r\n+        Returns:\r\n+            Dictionary containing attention-related metrics including whether\r\n+            Flash Attention is being used and chunk sizes for attention computation.\r\n+        \"\"\"\r\n+        return {\r\n+            'using_flash_attention': self.has_flash_attention,\r\n+            'attention_chunk_size': self.attention_batch_size,\r\n+            'attention_dropout': self.attention_dropout,\r\n+        }\r\n+    \r\n+    def get_memory_usage(self) -> Dict[str, float]:\r\n+        \"\"\"Get current GPU memory usage statistics.\r\n+        \r\n+        Returns:\r\n+            Dictionary containing current memory usage metrics in GB.\r\n+            Returns empty dict if not running on GPU.\r\n+        \"\"\"\r\n+        if not torch.cuda.is_available():\r\n+            return {}\r\n+            \r\n+        return {\r\n+            'current_allocated': torch.cuda.memory_allocated() / 1024**3,\r\n+            'max_allocated': torch.cuda.max_memory_allocated() / 1024**3,\r\n+            'current_reserved': torch.cuda.memory_reserved() / 1024**3,\r\n+            'max_reserved': torch.cuda.max_memory_reserved() / 1024**3\r\n+        }\r\n+\r\n+    def _optimize_memory_usage(self) -> None:\r\n+        \"\"\"Optimize memory usage by clearing unnecessary buffers.\r\n+        \r\n+        This method is called periodically during training to help manage\r\n+        GPU memory usage and prevent OOM errors.\r\n+        \"\"\"\r\n+        if not torch.cuda.is_available():\r\n+            return\r\n+            \r\n+        # Get current memory usage\r\n+        current_mem = torch.cuda.memory_allocated() / 1024**3\r\n+        \r\n+        # If using more than 80% of available memory, try to optimize\r\n+        if current_mem > 6.4:  # 80% of 8GB\r\n+            # Clear cache\r\n+            torch.cuda.empty_cache()\r\n+            \r\n+            # Clear unused buffers\r\n+            for param in self.parameters():\r\n+                if param.grad is not None:\r\n+                    param.grad.detach_()\r\n+                    param.grad.zero_()\r\n+            \r\n+            # Log memory usage after optimization\r\n+            new_mem = torch.cuda.memory_allocated() / 1024**3\r\n+            self.logger.debug(\r\n+                f\"Memory optimization performed: {current_mem:.2f}GB -> {new_mem:.2f}GB\"\r\n+            )\r\n+\r\n+    def get_input_dims(self) -> int:\r\n+        \"\"\"Return the input dimensions of the model.\r\n+        \r\n+        Returns:\r\n+            Number of input features the model expects.\r\n+        \"\"\"\r\n+        return self.input_features\r\n+\r\n+    def get_output_dims(self) -> int:\r\n+        \"\"\"Return the output dimensions of the model.\r\n+        \r\n+        Returns:\r\n+            Number of output features the model produces.\r\n+        \"\"\"\r\n+        return self.output_features\r\n+\r\n+    def should_apply_checkpointing(self) -> bool:\r\n+        \"\"\"Determine if gradient checkpointing should be applied.\r\n+        \r\n+        Returns:\r\n+            Boolean indicating whether gradient checkpointing should be used\r\n+            based on current memory usage and model configuration.\r\n+        \"\"\"\r\n+        if not self.use_checkpointing:\r\n+            return False\r\n+            \r\n+        if not torch.cuda.is_available():\r\n+            return False\r\n+            \r\n+        # Check current memory usage\r\n+        current_mem = torch.cuda.memory_allocated() / 1024**3\r\n+        \r\n+        # Enable checkpointing if memory usage is high\r\n+        return current_mem > 5.0  # 5GB threshold for 8GB GPU\r\n+\r\n+    @property\r\n+    def device(self) -> torch.device:\r\n+        \"\"\"Get the device the model is currently on.\r\n+        \r\n+        Returns:\r\n+            torch.device: The device (CPU/GPU) currently being used.\r\n+        \"\"\"\r\n+        return next(self.parameters()).device\r\n+\r\n+    def to_device(self, device: torch.device) -> None:\r\n+        \"\"\"Move the model to the specified device.\r\n+        \r\n+        This method ensures all model components are moved to the same device\r\n+        and updates internal device tracking.\r\n+        \r\n+        Args:\r\n+            device: The target device to move the model to.\r\n+        \"\"\"\r\n+        self.to(device)\r\n+        \r\n+        # Ensure all components are on the correct device\r\n+        if hasattr(self, 'encoder_embedding'):\r\n+            self.encoder_embedding = self.encoder_embedding.to(device)\r\n+        if hasattr(self, 'decoder_embedding'):\r\n+            self.decoder_embedding = self.decoder_embedding.to(device)\r\n+        if hasattr(self, 'output_projection'):\r\n+            self.output_projection = self.output_projection.to(device)\r\n+            \r\n+        self.logger.debug(f\"Model moved to device: {device}\")\r\n+\r\n+    def train(self, mode: bool = True) -> 'BaseTransformer':\r\n+        \"\"\"Set the model to training mode.\r\n+        \r\n+        Overrides the default train() method to ensure proper setup for\r\n+        training mode changes.\r\n+        \r\n+        Args:\r\n+            mode: Boolean indicating whether to set training mode on (True)\r\n+                 or evaluation mode (False)\r\n+                 \r\n+        Returns:\r\n+            Self for method chaining\r\n+        \"\"\"\r\n+        super().train(mode)\r\n+        if mode:\r\n+            # Reset performance stats when entering training mode\r\n+            self.perf_stats = {\r\n+                'attention_time': 0.0,\r\n+                'forward_pass_count': 0,\r\n+                'peak_memory': 0.0\r\n+            }\r\n+        return self\r\n+\r\n+    def eval(self) -> 'BaseTransformer':\r\n+        \"\"\"Set the model to evaluation mode.\r\n+        \r\n+        Overrides the default eval() method to ensure proper setup for\r\n+        evaluation mode changes.\r\n+        \r\n+        Returns:\r\n+            Self for method chaining\r\n+        \"\"\"\r\n+        super().eval()\r\n+        return self\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733264895066,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,581 @@\n+from abc import ABC, abstractmethod\r\n+from typing import Optional, Dict, Any, Tuple, cast, Callable\r\n+import math\r\n+import torch\r\n+from torch import nn, Tensor\r\n+import torch.nn.functional as F\r\n+from torch.utils.checkpoint import checkpoint\r\n+\r\n+from models.base.base_model import BaseModel\r\n+from models.components.embeddings import CombinedEmbedding\r\n+from models.components.attention import MultiHeadAttention\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n+\r\n+class BaseTransformer(BaseModel, ABC):\r\n+    \"\"\"Base class for transformer models with memory-efficient implementation.\r\n+    \r\n+    This implementation is optimized for training on GPUs with limited VRAM (e.g., RTX 4060 8GB)\r\n+    and includes features like gradient checkpointing, Flash Attention when available, and\r\n+    chunked attention computation for memory efficiency.\r\n+    \"\"\"\r\n+\r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        \"\"\"Initialize the transformer with the given configuration.\r\n+        \r\n+        Args:\r\n+            config: A dictionary containing model configuration parameters including:\r\n+                - d_model: The model's hidden dimension size\r\n+                - n_heads: Number of attention heads\r\n+                - input_features: Number of input features\r\n+                - dropout: Dropout rate\r\n+                - use_checkpointing: Whether to use gradient checkpointing\r\n+                - use_flash_attention: Whether to use Flash Attention when available\r\n+                - attention_batch_size: Size of chunks for attention computation\r\n+        \"\"\"\r\n+        super().__init__(config)\r\n+        \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"BaseTransformer\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n+\r\n+        # Validate configuration\r\n+        self._validate_config(config)\r\n+        self._setup_model_dimensions(config)\r\n+        \r\n+        # Memory optimization flags\r\n+        self.use_checkpointing = config.get('use_checkpointing', True)\r\n+        self.attention_dropout = config.get('attention_dropout', 0.1)\r\n+        \r\n+        # Initialize embeddings\r\n+        self.encoder_embedding = self._create_embedding()\r\n+        self.decoder_embedding = self._create_embedding()\r\n+        \r\n+        # Initialize flash attention if available\r\n+        self.use_flash_attention = config.get('use_flash_attention', True) and torch.cuda.is_available()\r\n+        if self.use_flash_attention:\r\n+            try:\r\n+                from torch.nn.functional import scaled_dot_product_attention\r\n+                self.has_flash_attention = True\r\n+                self.logger.info(\"Flash Attention is enabled\")\r\n+            except ImportError:\r\n+                self.has_flash_attention = False\r\n+                self.logger.warning(\"Flash Attention not available, falling back to standard attention\")\r\n+\r\n+        # Memory-efficient attention settings\r\n+        self.attention_batch_size = config.get('attention_batch_size', 1024)\r\n+        \r\n+        # Output projection with improved initialization\r\n+        self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n+        self._initialize_output_projection()\r\n+        \r\n+        # Initialize performance monitoring\r\n+        self.perf_stats = {\r\n+            'attention_time': 0.0,\r\n+            'forward_pass_count': 0,\r\n+            'peak_memory': 0.0\r\n+        }\r\n+\r\n+    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n+        \"\"\"Validate the configuration parameters.\"\"\"\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        missing_keys = [key for key in required_keys if key not in config]\r\n+        if missing_keys:\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n+    def _setup_model_dimensions(self, config: Dict[str, Any]) -> None:\r\n+        \"\"\"Set up model dimensions and validate their compatibility.\"\"\"\r\n+        self.d_model = config['d_model']\r\n+        self.input_features = config['input_features']\r\n+        self.output_features = config.get('output_features', 1)\r\n+        \r\n+        # Calculate attention dimensions\r\n+        self.n_heads = config['n_heads']\r\n+        self.head_dim = self.d_model // self.n_heads\r\n+        \r\n+        # Validate dimensions\r\n+        if self.d_model % self.n_heads != 0:\r\n+            raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({self.n_heads})\")\r\n+        \r\n+        # Set dropout rates\r\n+        self.dropout = config.get('dropout', 0.1)\r\n+        self.attention_dropout = config.get('attention_dropout', 0.1)\r\n+\r\n+    def _create_embedding(self) -> CombinedEmbedding:\r\n+        \"\"\"Create the embedding layer with proper initialization.\"\"\"\r\n+        return CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=self.dropout\r\n+        )\r\n+\r\n+    def _initialize_output_projection(self) -> None:\r\n+        \"\"\"Initialize the output projection layer with Xavier initialization.\"\"\"\r\n+        nn.init.xavier_uniform_(self.output_projection.weight)\r\n+        if self.output_projection.bias is not None:\r\n+            nn.init.zeros_(self.output_projection.bias)\r\n+\r\n+    def _create_encoder_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n+        \"\"\"Create a type-safe forward function for encoder checkpointing.\r\n+        \r\n+        Args:\r\n+            layer: The encoder layer module\r\n+            \r\n+        Returns:\r\n+            A callable that handles the forward pass with proper type safety\r\n+        \"\"\"\r\n+        def custom_forward(*inputs: Any) -> Tensor:\r\n+            if not all(isinstance(x, (torch.Tensor, type(None))) for x in inputs):\r\n+                raise TypeError(\"All inputs must be tensors or None\")\r\n+            \r\n+            src = cast(Tensor, inputs[0])\r\n+            src_mask = inputs[1]  # Optional[Tensor]\r\n+            src_key_padding_mask = inputs[2]  # Optional[Tensor]\r\n+            \r\n+            return layer(\r\n+                src,\r\n+                src_mask=src_mask,\r\n+                src_key_padding_mask=src_key_padding_mask\r\n+            )\r\n+        return custom_forward\r\n+\r\n+    def _create_decoder_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n+        \"\"\"Create a type-safe forward function for decoder checkpointing.\r\n+        \r\n+        Args:\r\n+            layer: The decoder layer module\r\n+            \r\n+        Returns:\r\n+            A callable that handles the forward pass with proper type safety\r\n+        \"\"\"\r\n+        def custom_forward(*inputs: Any) -> Tensor:\r\n+            if not all(isinstance(x, (torch.Tensor, type(None))) for x in inputs):\r\n+                raise TypeError(\"All inputs must be tensors or None\")\r\n+            \r\n+            tgt = cast(Tensor, inputs[0])\r\n+            memory = cast(Tensor, inputs[1])\r\n+            tgt_mask = inputs[2]  # Optional[Tensor]\r\n+            memory_mask = inputs[3]  # Optional[Tensor]\r\n+            tgt_key_padding_mask = inputs[4]  # Optional[Tensor]\r\n+            memory_key_padding_mask = inputs[5]  # Optional[Tensor]\r\n+            \r\n+            return layer(\r\n+                tgt,\r\n+                memory,\r\n+                tgt_mask=tgt_mask,\r\n+                memory_mask=memory_mask,\r\n+                tgt_key_padding_mask=tgt_key_padding_mask,\r\n+                memory_key_padding_mask=memory_key_padding_mask\r\n+            )\r\n+        return custom_forward\r\n+\r\n+    def _compute_attention_chunked(\r\n+        self, \r\n+        query: Tensor, \r\n+        key: Tensor, \r\n+        value: Tensor, \r\n+        mask: Optional[Tensor] = None\r\n+    ) -> Tensor:\r\n+        \"\"\"Compute attention scores in memory-efficient chunks.\r\n+        \r\n+        This method processes attention computation in smaller chunks to reduce memory\r\n+        usage, which is particularly important for long sequences.\r\n+        \"\"\"\r\n+        batch_size, seq_len, _ = query.size()\r\n+        chunks = math.ceil(seq_len / self.attention_batch_size)\r\n+        \r\n+        outputs = []\r\n+        for i in range(chunks):\r\n+            start_idx = i * self.attention_batch_size\r\n+            end_idx = min((i + 1) * self.attention_batch_size, seq_len)\r\n+            \r\n+            # Process chunk\r\n+            chunk_q = query[:, start_idx:end_idx]\r\n+            chunk_mask = mask[:, start_idx:end_idx] if mask is not None else None\r\n+            \r\n+            if self.has_flash_attention:\r\n+                chunk_output = F.scaled_dot_product_attention(\r\n+                    chunk_q, key, value,\r\n+                    attn_mask=chunk_mask,\r\n+                    dropout_p=self.attention_dropout if self.training else 0.0,\r\n+                    is_causal=False\r\n+                )\r\n+            else:\r\n+                # Standard attention computation\r\n+                scores = torch.matmul(chunk_q, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\r\n+                if chunk_mask is not None:\r\n+                    scores = scores.masked_fill(chunk_mask == 0, float('-inf'))\r\n+                weights = F.softmax(scores, dim=-1)\r\n+                weights = F.dropout(weights, p=self.attention_dropout, training=self.training)\r\n+                chunk_output = torch.matmul(weights, value)\r\n+            \r\n+            outputs.append(chunk_output)\r\n+            \r\n+        return torch.cat(outputs, dim=1)\r\n+\r\n+    def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n+              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n+        \"\"\"Encode input sequence with memory-efficient processing.\r\n+        \r\n+        Args:\r\n+            src: Source sequence tensor\r\n+            src_mask: Optional mask for source sequence\r\n+            src_key_padding_mask: Optional key padding mask for source sequence\r\n+            \r\n+        Returns:\r\n+            Encoded sequence tensor\r\n+        \"\"\"\r\n+        if torch.cuda.is_available():\r\n+            init_mem = torch.cuda.memory_allocated()\r\n+        \r\n+        # Apply embedding\r\n+        src = self.encoder_embedding(src)\r\n+        \r\n+        # Process encoder layers with optional checkpointing\r\n+        for i, layer in enumerate(self.encoder_layers):\r\n+            if self.use_checkpointing and self.training:\r\n+                custom_forward = self._create_encoder_forward(layer)\r\n+                output = checkpoint(\r\n+                    custom_forward,\r\n+                    src, src_mask, src_key_padding_mask,\r\n+                    preserve_rng_state=True\r\n+                )\r\n+                src = cast(Tensor, output)\r\n+            else:\r\n+                src = layer(\r\n+                    src,\r\n+                    src_mask=src_mask,\r\n+                    src_key_padding_mask=src_key_padding_mask\r\n+                )\r\n+        \r\n+        if torch.cuda.is_available():\r\n+            peak_mem = torch.cuda.max_memory_allocated() - init_mem\r\n+            self.perf_stats['peak_memory'] = max(self.perf_stats['peak_memory'], peak_mem)\r\n+            \r\n+        return src\r\n+\r\n+    def decode(\r\n+        self,\r\n+        tgt: Tensor,\r\n+        memory: Tensor,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        memory_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+        memory_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Decode target sequence with memory-efficient processing.\r\n+        \r\n+        Args:\r\n+            tgt: Target sequence tensor\r\n+            memory: Memory from encoder\r\n+            tgt_mask: Optional mask for target sequence\r\n+            memory_mask: Optional mask for memory\r\n+            tgt_key_padding_mask: Optional key padding mask for target sequence\r\n+            memory_key_padding_mask: Optional key padding mask for memory\r\n+            \r\n+        Returns:\r\n+            Decoded sequence tensor\r\n+        \"\"\"\r\n+        # Apply embedding\r\n+        tgt = self.decoder_embedding(tgt)\r\n+        \r\n+        # Process decoder layers with optional checkpointing\r\n+        for layer in self.decoder_layers:\r\n+            if self.use_checkpointing and self.training:\r\n+                custom_forward = self._create_decoder_forward(layer)\r\n+                output = checkpoint(\r\n+                    custom_forward,\r\n+                    tgt, memory, tgt_mask, memory_mask,\r\n+                    tgt_key_padding_mask, memory_key_padding_mask,\r\n+                    preserve_rng_state=True\r\n+                )\r\n+                tgt = cast(Tensor, output)\r\n+            else:\r\n+                tgt = layer(\r\n+                    tgt,\r\n+                    memory,\r\n+                    tgt_mask=tgt_mask,\r\n+                    memory_mask=memory_mask,\r\n+                    tgt_key_padding_mask=tgt_key_padding_mask,\r\n+                    memory_key_padding_mask=memory_key_padding_mask\r\n+                )\r\n+        return tgt\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        tgt: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Forward pass with memory tracking and optimization.\r\n+        \r\n+        Args:\r\n+            src: Source sequence tensor\r\n+            tgt: Target sequence tensor\r\n+            src_mask: Optional mask for source sequence\r\n+            tgt_mask: Optional mask for target sequence\r\n+            src_key_padding_mask: Optional key padding mask for source sequence\r\n+            tgt_key_padding_mask: Optional key padding mask for target sequence\r\n+            \r\n+        Returns:\r\n+            Output tensor from the transformer\r\n+        \"\"\"\r\n+        if torch.cuda.is_available():\r\n+            torch.cuda.reset_peak_memory_stats()\r\n+            \r\n+        # Encode source sequence\r\n+        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n+        \r\n+        # Decode target sequence\r\n+        output = self.decode(\r\n+            tgt,\r\n+            memory,\r\n+            tgt_mask=tgt_mask,\r\n+            memory_mask=None,\r\n+            tgt_key_padding_mask=tgt_key_padding_mask,\r\n+            memory_key_padding_mask=src_key_padding_mask\r\n+        )\r\n+        \r\n+        # Project to output dimension\r\n+        output = self.output_projection(output)\r\n+        \r\n+        # Track performance metrics\r\n+        self.perf_stats['forward_pass_count'] += 1\r\n+        if torch.cuda.is_available():\r\n+            peak_mem = torch.cuda.max_memory_allocated()\r\n+            self.perf_stats['peak_memory'] = max(self.perf_stats['peak_memory'], peak_mem)\r\n+        \r\n+        return output\r\n+\r\n+    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n+        \"\"\"Generate an optimized attention mask.\r\n+        \r\n+        Args:\r\n+            sz: Size of the square mask\r\n+            \r\n+        Returns:\r\n+            A square mask tensor for attention\r\n+        \"\"\"\r\n+        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\r\n+        mask = mask.float().masked_fill(mask == 1, float('-inf'))\r\n+        device = next(self.parameters()).device\r\n+        return mask.to(device=device)\r\n+\r\n+    @abstractmethod\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    def get_performance_stats(self) -> Dict[str, float]:\r\n+        \"\"\"Return performance statistics for monitoring.\r\n+        \r\n+        Returns:\r\n+            Dictionary containing performance metrics including memory usage,\r\n+            forward pass counts, and average memory consumption per forward pass.\r\n+        \"\"\"\r\n+        stats = self.perf_stats.copy()\r\n+        if self.perf_stats['forward_pass_count'] > 0:\r\n+            stats['avg_memory_per_forward'] = (\r\n+                self.perf_stats['peak_memory'] / \r\n+                self.perf_stats['forward_pass_count']\r\n+            )\r\n+        return stats\r\n+    \r\n+    def get_attention_stats(self) -> Dict[str, Any]:\r\n+        \"\"\"Return statistics about attention mechanism usage.\r\n+        \r\n+        Returns:\r\n+            Dictionary containing attention-related metrics including whether\r\n+            Flash Attention is being used and chunk sizes for attention computation.\r\n+        \"\"\"\r\n+        return {\r\n+            'using_flash_attention': self.has_flash_attention,\r\n+            'attention_chunk_size': self.attention_batch_size,\r\n+            'attention_dropout': self.attention_dropout,\r\n+        }\r\n+        \r\n+    def diagnose_gradients(self, name: str, tensor_or_module: Union[torch.Tensor, nn.Module]) -> None:\r\n+        \"\"\"Diagnose gradient setup of tensors or modules.\r\n+        \r\n+        This helper function prints detailed information about gradient settings\r\n+        to help identify where gradients might be lost.\r\n+        \r\n+        Args:\r\n+            name: Identifier for the diagnostic output\r\n+            tensor_or_module: Tensor or module to diagnose\r\n+        \"\"\"\r\n+        if isinstance(tensor_or_module, torch.Tensor):\r\n+            self.logger.debug(f\"Gradient Diagnostic - {name}:\")\r\n+            self.logger.debug(f\"  - requires_grad: {tensor_or_module.requires_grad}\")\r\n+            self.logger.debug(f\"  - is_leaf: {tensor_or_module.is_leaf}\")\r\n+            self.logger.debug(f\"  - dtype: {tensor_or_module.dtype}\")\r\n+            self.logger.debug(f\"  - device: {tensor_or_module.device}\")\r\n+        elif isinstance(tensor_or_module, nn.Module):\r\n+            self.logger.debug(f\"Module Diagnostic - {name}:\")\r\n+            total_params = 0\r\n+            params_with_grad = 0\r\n+            for param in tensor_or_module.parameters():\r\n+                total_params += 1\r\n+                if param.requires_grad:\r\n+                    params_with_grad += 1\r\n+            self.logger.debug(f\"  - Parameters with requires_grad: {params_with_grad}/{total_params}\")\r\n+            self.logger.debug(f\"  - Training mode: {tensor_or_module.training}\")\r\n+    \r\n+    def get_memory_usage(self) -> Dict[str, float]:\r\n+        \"\"\"Get current GPU memory usage statistics.\r\n+        \r\n+        Returns:\r\n+            Dictionary containing current memory usage metrics in GB.\r\n+            Returns empty dict if not running on GPU.\r\n+        \"\"\"\r\n+        if not torch.cuda.is_available():\r\n+            return {}\r\n+            \r\n+        return {\r\n+            'current_allocated': torch.cuda.memory_allocated() / 1024**3,\r\n+            'max_allocated': torch.cuda.max_memory_allocated() / 1024**3,\r\n+            'current_reserved': torch.cuda.memory_reserved() / 1024**3,\r\n+            'max_reserved': torch.cuda.max_memory_reserved() / 1024**3\r\n+        }\r\n+\r\n+    def _optimize_memory_usage(self) -> None:\r\n+        \"\"\"Optimize memory usage by clearing unnecessary buffers.\r\n+        \r\n+        This method is called periodically during training to help manage\r\n+        GPU memory usage and prevent OOM errors.\r\n+        \"\"\"\r\n+        if not torch.cuda.is_available():\r\n+            return\r\n+            \r\n+        # Get current memory usage\r\n+        current_mem = torch.cuda.memory_allocated() / 1024**3\r\n+        \r\n+        # If using more than 80% of available memory, try to optimize\r\n+        if current_mem > 6.4:  # 80% of 8GB\r\n+            # Clear cache\r\n+            torch.cuda.empty_cache()\r\n+            \r\n+            # Clear unused buffers\r\n+            for param in self.parameters():\r\n+                if param.grad is not None:\r\n+                    param.grad.detach_()\r\n+                    param.grad.zero_()\r\n+            \r\n+            # Log memory usage after optimization\r\n+            new_mem = torch.cuda.memory_allocated() / 1024**3\r\n+            self.logger.debug(\r\n+                f\"Memory optimization performed: {current_mem:.2f}GB -> {new_mem:.2f}GB\"\r\n+            )\r\n+\r\n+    def get_input_dims(self) -> int:\r\n+        \"\"\"Return the input dimensions of the model.\r\n+        \r\n+        Returns:\r\n+            Number of input features the model expects.\r\n+        \"\"\"\r\n+        return self.input_features\r\n+\r\n+    def get_output_dims(self) -> int:\r\n+        \"\"\"Return the output dimensions of the model.\r\n+        \r\n+        Returns:\r\n+            Number of output features the model produces.\r\n+        \"\"\"\r\n+        return self.output_features\r\n+\r\n+    def should_apply_checkpointing(self) -> bool:\r\n+        \"\"\"Determine if gradient checkpointing should be applied.\r\n+        \r\n+        Returns:\r\n+            Boolean indicating whether gradient checkpointing should be used\r\n+            based on current memory usage and model configuration.\r\n+        \"\"\"\r\n+        if not self.use_checkpointing:\r\n+            return False\r\n+            \r\n+        if not torch.cuda.is_available():\r\n+            return False\r\n+            \r\n+        # Check current memory usage\r\n+        current_mem = torch.cuda.memory_allocated() / 1024**3\r\n+        \r\n+        # Enable checkpointing if memory usage is high\r\n+        return current_mem > 5.0  # 5GB threshold for 8GB GPU\r\n+\r\n+    @property\r\n+    def device(self) -> torch.device:\r\n+        \"\"\"Get the device the model is currently on.\r\n+        \r\n+        Returns:\r\n+            torch.device: The device (CPU/GPU) currently being used.\r\n+        \"\"\"\r\n+        return next(self.parameters()).device\r\n+\r\n+    def to_device(self, device: torch.device) -> None:\r\n+        \"\"\"Move the model to the specified device.\r\n+        \r\n+        This method ensures all model components are moved to the same device\r\n+        and updates internal device tracking.\r\n+        \r\n+        Args:\r\n+            device: The target device to move the model to.\r\n+        \"\"\"\r\n+        self.to(device)\r\n+        \r\n+        # Ensure all components are on the correct device\r\n+        if hasattr(self, 'encoder_embedding'):\r\n+            self.encoder_embedding = self.encoder_embedding.to(device)\r\n+        if hasattr(self, 'decoder_embedding'):\r\n+            self.decoder_embedding = self.decoder_embedding.to(device)\r\n+        if hasattr(self, 'output_projection'):\r\n+            self.output_projection = self.output_projection.to(device)\r\n+            \r\n+        self.logger.debug(f\"Model moved to device: {device}\")\r\n+\r\n+    def train(self, mode: bool = True) -> 'BaseTransformer':\r\n+        \"\"\"Set the model to training mode.\r\n+        \r\n+        Overrides the default train() method to ensure proper setup for\r\n+        training mode changes.\r\n+        \r\n+        Args:\r\n+            mode: Boolean indicating whether to set training mode on (True)\r\n+                 or evaluation mode (False)\r\n+                 \r\n+        Returns:\r\n+            Self for method chaining\r\n+        \"\"\"\r\n+        super().train(mode)\r\n+        if mode:\r\n+            # Reset performance stats when entering training mode\r\n+            self.perf_stats = {\r\n+                'attention_time': 0.0,\r\n+                'forward_pass_count': 0,\r\n+                'peak_memory': 0.0\r\n+            }\r\n+        return self\r\n+\r\n+    def eval(self) -> 'BaseTransformer':\r\n+        \"\"\"Set the model to evaluation mode.\r\n+        \r\n+        Overrides the default eval() method to ensure proper setup for\r\n+        evaluation mode changes.\r\n+        \r\n+        Returns:\r\n+            Self for method chaining\r\n+        \"\"\"\r\n+        super().eval()\r\n+        return self\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733264915300,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -218,46 +218,50 @@\n             \r\n         return torch.cat(outputs, dim=1)\r\n \r\n     def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n-              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n-        \"\"\"Encode input sequence with memory-efficient processing.\r\n+            src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n+        \"\"\"Encode input sequence with gradient diagnostics.\"\"\"\r\n+        # Check input tensor gradients\r\n+        self.diagnose_gradients(\"Input Tensor\", src)\r\n         \r\n-        Args:\r\n-            src: Source sequence tensor\r\n-            src_mask: Optional mask for source sequence\r\n-            src_key_padding_mask: Optional key padding mask for source sequence\r\n-            \r\n-        Returns:\r\n-            Encoded sequence tensor\r\n-        \"\"\"\r\n-        if torch.cuda.is_available():\r\n-            init_mem = torch.cuda.memory_allocated()\r\n+        # Check model state before encoding\r\n+        self.diagnose_gradients(\"Encoder Embedding\", self.encoder_embedding)\r\n         \r\n         # Apply embedding\r\n         src = self.encoder_embedding(src)\r\n         \r\n+        # Check embedded tensor gradients\r\n+        self.diagnose_gradients(\"After Embedding\", src)\r\n+        \r\n         # Process encoder layers with optional checkpointing\r\n         for i, layer in enumerate(self.encoder_layers):\r\n+            self.diagnose_gradients(f\"Encoder Layer {i}\", layer)\r\n+            \r\n             if self.use_checkpointing and self.training:\r\n                 custom_forward = self._create_encoder_forward(layer)\r\n+                # Ensure inputs require gradients before checkpointing\r\n+                if not src.requires_grad:\r\n+                    src.requires_grad = True\r\n+                    self.logger.warning(f\"Fixed missing requires_grad in layer {i}\")\r\n+                \r\n                 output = checkpoint(\r\n                     custom_forward,\r\n                     src, src_mask, src_key_padding_mask,\r\n-                    preserve_rng_state=True\r\n+                    preserve_rng_state=True,\r\n+                    use_reentrant=False\r\n                 )\r\n                 src = cast(Tensor, output)\r\n             else:\r\n                 src = layer(\r\n                     src,\r\n                     src_mask=src_mask,\r\n                     src_key_padding_mask=src_key_padding_mask\r\n                 )\r\n+            \r\n+            # Check output gradients after each layer\r\n+            self.diagnose_gradients(f\"After Layer {i}\", src)\r\n         \r\n-        if torch.cuda.is_available():\r\n-            peak_mem = torch.cuda.max_memory_allocated() - init_mem\r\n-            self.perf_stats['peak_memory'] = max(self.perf_stats['peak_memory'], peak_mem)\r\n-            \r\n         return src\r\n \r\n     def decode(\r\n         self,\r\n"
                },
                {
                    "date": 1733264925402,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,6 @@\n from abc import ABC, abstractmethod\r\n-from typing import Optional, Dict, Any, Tuple, cast, Callable\r\n+from typing import Optional, Dict, Any, Tuple, Union, cast, Callable\r\n import math\r\n import torch\r\n from torch import nn, Tensor\r\n import torch.nn.functional as F\r\n@@ -581,559 +581,5 @@\n         Returns:\r\n             Self for method chaining\r\n         \"\"\"\r\n         super().eval()\r\n-        return self\n-from abc import ABC, abstractmethod\r\n-from typing import Optional, Dict, Any, Tuple, cast, Callable\r\n-import math\r\n-import torch\r\n-from torch import nn, Tensor\r\n-import torch.nn.functional as F\r\n-from torch.utils.checkpoint import checkpoint\r\n-\r\n-from models.base.base_model import BaseModel\r\n-from models.components.embeddings import CombinedEmbedding\r\n-from models.components.attention import MultiHeadAttention\r\n-from utils.logging.logger import Logger\r\n-from utils.logging.config import LoggerConfig, LogLevel\r\n-\r\n-class BaseTransformer(BaseModel, ABC):\r\n-    \"\"\"Base class for transformer models with memory-efficient implementation.\r\n-    \r\n-    This implementation is optimized for training on GPUs with limited VRAM (e.g., RTX 4060 8GB)\r\n-    and includes features like gradient checkpointing, Flash Attention when available, and\r\n-    chunked attention computation for memory efficiency.\r\n-    \"\"\"\r\n-\r\n-    def __init__(self, config: Dict[str, Any]):\r\n-        \"\"\"Initialize the transformer with the given configuration.\r\n-        \r\n-        Args:\r\n-            config: A dictionary containing model configuration parameters including:\r\n-                - d_model: The model's hidden dimension size\r\n-                - n_heads: Number of attention heads\r\n-                - input_features: Number of input features\r\n-                - dropout: Dropout rate\r\n-                - use_checkpointing: Whether to use gradient checkpointing\r\n-                - use_flash_attention: Whether to use Flash Attention when available\r\n-                - attention_batch_size: Size of chunks for attention computation\r\n-        \"\"\"\r\n-        super().__init__(config)\r\n-        \r\n-        # Initialize logger\r\n-        logger_config = LoggerConfig(\r\n-            level=LogLevel.INFO,\r\n-            component_name=\"BaseTransformer\",\r\n-            include_timestamp=True\r\n-        )\r\n-        self.logger = Logger.get_logger(__name__, logger_config)\r\n-        self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n-\r\n-        # Validate configuration\r\n-        self._validate_config(config)\r\n-        self._setup_model_dimensions(config)\r\n-        \r\n-        # Memory optimization flags\r\n-        self.use_checkpointing = config.get('use_checkpointing', True)\r\n-        self.attention_dropout = config.get('attention_dropout', 0.1)\r\n-        \r\n-        # Initialize embeddings\r\n-        self.encoder_embedding = self._create_embedding()\r\n-        self.decoder_embedding = self._create_embedding()\r\n-        \r\n-        # Initialize flash attention if available\r\n-        self.use_flash_attention = config.get('use_flash_attention', True) and torch.cuda.is_available()\r\n-        if self.use_flash_attention:\r\n-            try:\r\n-                from torch.nn.functional import scaled_dot_product_attention\r\n-                self.has_flash_attention = True\r\n-                self.logger.info(\"Flash Attention is enabled\")\r\n-            except ImportError:\r\n-                self.has_flash_attention = False\r\n-                self.logger.warning(\"Flash Attention not available, falling back to standard attention\")\r\n-\r\n-        # Memory-efficient attention settings\r\n-        self.attention_batch_size = config.get('attention_batch_size', 1024)\r\n-        \r\n-        # Output projection with improved initialization\r\n-        self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n-        self._initialize_output_projection()\r\n-        \r\n-        # Initialize performance monitoring\r\n-        self.perf_stats = {\r\n-            'attention_time': 0.0,\r\n-            'forward_pass_count': 0,\r\n-            'peak_memory': 0.0\r\n-        }\r\n-\r\n-    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n-        \"\"\"Validate the configuration parameters.\"\"\"\r\n-        required_keys = ['d_model', 'n_heads', 'input_features']\r\n-        missing_keys = [key for key in required_keys if key not in config]\r\n-        if missing_keys:\r\n-            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n-\r\n-    def _setup_model_dimensions(self, config: Dict[str, Any]) -> None:\r\n-        \"\"\"Set up model dimensions and validate their compatibility.\"\"\"\r\n-        self.d_model = config['d_model']\r\n-        self.input_features = config['input_features']\r\n-        self.output_features = config.get('output_features', 1)\r\n-        \r\n-        # Calculate attention dimensions\r\n-        self.n_heads = config['n_heads']\r\n-        self.head_dim = self.d_model // self.n_heads\r\n-        \r\n-        # Validate dimensions\r\n-        if self.d_model % self.n_heads != 0:\r\n-            raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({self.n_heads})\")\r\n-        \r\n-        # Set dropout rates\r\n-        self.dropout = config.get('dropout', 0.1)\r\n-        self.attention_dropout = config.get('attention_dropout', 0.1)\r\n-\r\n-    def _create_embedding(self) -> CombinedEmbedding:\r\n-        \"\"\"Create the embedding layer with proper initialization.\"\"\"\r\n-        return CombinedEmbedding(\r\n-            d_model=self.d_model,\r\n-            input_features=self.input_features,\r\n-            dropout=self.dropout\r\n-        )\r\n-\r\n-    def _initialize_output_projection(self) -> None:\r\n-        \"\"\"Initialize the output projection layer with Xavier initialization.\"\"\"\r\n-        nn.init.xavier_uniform_(self.output_projection.weight)\r\n-        if self.output_projection.bias is not None:\r\n-            nn.init.zeros_(self.output_projection.bias)\r\n-\r\n-    def _create_encoder_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n-        \"\"\"Create a type-safe forward function for encoder checkpointing.\r\n-        \r\n-        Args:\r\n-            layer: The encoder layer module\r\n-            \r\n-        Returns:\r\n-            A callable that handles the forward pass with proper type safety\r\n-        \"\"\"\r\n-        def custom_forward(*inputs: Any) -> Tensor:\r\n-            if not all(isinstance(x, (torch.Tensor, type(None))) for x in inputs):\r\n-                raise TypeError(\"All inputs must be tensors or None\")\r\n-            \r\n-            src = cast(Tensor, inputs[0])\r\n-            src_mask = inputs[1]  # Optional[Tensor]\r\n-            src_key_padding_mask = inputs[2]  # Optional[Tensor]\r\n-            \r\n-            return layer(\r\n-                src,\r\n-                src_mask=src_mask,\r\n-                src_key_padding_mask=src_key_padding_mask\r\n-            )\r\n-        return custom_forward\r\n-\r\n-    def _create_decoder_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n-        \"\"\"Create a type-safe forward function for decoder checkpointing.\r\n-        \r\n-        Args:\r\n-            layer: The decoder layer module\r\n-            \r\n-        Returns:\r\n-            A callable that handles the forward pass with proper type safety\r\n-        \"\"\"\r\n-        def custom_forward(*inputs: Any) -> Tensor:\r\n-            if not all(isinstance(x, (torch.Tensor, type(None))) for x in inputs):\r\n-                raise TypeError(\"All inputs must be tensors or None\")\r\n-            \r\n-            tgt = cast(Tensor, inputs[0])\r\n-            memory = cast(Tensor, inputs[1])\r\n-            tgt_mask = inputs[2]  # Optional[Tensor]\r\n-            memory_mask = inputs[3]  # Optional[Tensor]\r\n-            tgt_key_padding_mask = inputs[4]  # Optional[Tensor]\r\n-            memory_key_padding_mask = inputs[5]  # Optional[Tensor]\r\n-            \r\n-            return layer(\r\n-                tgt,\r\n-                memory,\r\n-                tgt_mask=tgt_mask,\r\n-                memory_mask=memory_mask,\r\n-                tgt_key_padding_mask=tgt_key_padding_mask,\r\n-                memory_key_padding_mask=memory_key_padding_mask\r\n-            )\r\n-        return custom_forward\r\n-\r\n-    def _compute_attention_chunked(\r\n-        self, \r\n-        query: Tensor, \r\n-        key: Tensor, \r\n-        value: Tensor, \r\n-        mask: Optional[Tensor] = None\r\n-    ) -> Tensor:\r\n-        \"\"\"Compute attention scores in memory-efficient chunks.\r\n-        \r\n-        This method processes attention computation in smaller chunks to reduce memory\r\n-        usage, which is particularly important for long sequences.\r\n-        \"\"\"\r\n-        batch_size, seq_len, _ = query.size()\r\n-        chunks = math.ceil(seq_len / self.attention_batch_size)\r\n-        \r\n-        outputs = []\r\n-        for i in range(chunks):\r\n-            start_idx = i * self.attention_batch_size\r\n-            end_idx = min((i + 1) * self.attention_batch_size, seq_len)\r\n-            \r\n-            # Process chunk\r\n-            chunk_q = query[:, start_idx:end_idx]\r\n-            chunk_mask = mask[:, start_idx:end_idx] if mask is not None else None\r\n-            \r\n-            if self.has_flash_attention:\r\n-                chunk_output = F.scaled_dot_product_attention(\r\n-                    chunk_q, key, value,\r\n-                    attn_mask=chunk_mask,\r\n-                    dropout_p=self.attention_dropout if self.training else 0.0,\r\n-                    is_causal=False\r\n-                )\r\n-            else:\r\n-                # Standard attention computation\r\n-                scores = torch.matmul(chunk_q, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\r\n-                if chunk_mask is not None:\r\n-                    scores = scores.masked_fill(chunk_mask == 0, float('-inf'))\r\n-                weights = F.softmax(scores, dim=-1)\r\n-                weights = F.dropout(weights, p=self.attention_dropout, training=self.training)\r\n-                chunk_output = torch.matmul(weights, value)\r\n-            \r\n-            outputs.append(chunk_output)\r\n-            \r\n-        return torch.cat(outputs, dim=1)\r\n-\r\n-    def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n-              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n-        \"\"\"Encode input sequence with memory-efficient processing.\r\n-        \r\n-        Args:\r\n-            src: Source sequence tensor\r\n-            src_mask: Optional mask for source sequence\r\n-            src_key_padding_mask: Optional key padding mask for source sequence\r\n-            \r\n-        Returns:\r\n-            Encoded sequence tensor\r\n-        \"\"\"\r\n-        if torch.cuda.is_available():\r\n-            init_mem = torch.cuda.memory_allocated()\r\n-        \r\n-        # Apply embedding\r\n-        src = self.encoder_embedding(src)\r\n-        \r\n-        # Process encoder layers with optional checkpointing\r\n-        for i, layer in enumerate(self.encoder_layers):\r\n-            if self.use_checkpointing and self.training:\r\n-                custom_forward = self._create_encoder_forward(layer)\r\n-                output = checkpoint(\r\n-                    custom_forward,\r\n-                    src, src_mask, src_key_padding_mask,\r\n-                    preserve_rng_state=True\r\n-                )\r\n-                src = cast(Tensor, output)\r\n-            else:\r\n-                src = layer(\r\n-                    src,\r\n-                    src_mask=src_mask,\r\n-                    src_key_padding_mask=src_key_padding_mask\r\n-                )\r\n-        \r\n-        if torch.cuda.is_available():\r\n-            peak_mem = torch.cuda.max_memory_allocated() - init_mem\r\n-            self.perf_stats['peak_memory'] = max(self.perf_stats['peak_memory'], peak_mem)\r\n-            \r\n-        return src\r\n-\r\n-    def decode(\r\n-        self,\r\n-        tgt: Tensor,\r\n-        memory: Tensor,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        memory_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-        memory_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Decode target sequence with memory-efficient processing.\r\n-        \r\n-        Args:\r\n-            tgt: Target sequence tensor\r\n-            memory: Memory from encoder\r\n-            tgt_mask: Optional mask for target sequence\r\n-            memory_mask: Optional mask for memory\r\n-            tgt_key_padding_mask: Optional key padding mask for target sequence\r\n-            memory_key_padding_mask: Optional key padding mask for memory\r\n-            \r\n-        Returns:\r\n-            Decoded sequence tensor\r\n-        \"\"\"\r\n-        # Apply embedding\r\n-        tgt = self.decoder_embedding(tgt)\r\n-        \r\n-        # Process decoder layers with optional checkpointing\r\n-        for layer in self.decoder_layers:\r\n-            if self.use_checkpointing and self.training:\r\n-                custom_forward = self._create_decoder_forward(layer)\r\n-                output = checkpoint(\r\n-                    custom_forward,\r\n-                    tgt, memory, tgt_mask, memory_mask,\r\n-                    tgt_key_padding_mask, memory_key_padding_mask,\r\n-                    preserve_rng_state=True\r\n-                )\r\n-                tgt = cast(Tensor, output)\r\n-            else:\r\n-                tgt = layer(\r\n-                    tgt,\r\n-                    memory,\r\n-                    tgt_mask=tgt_mask,\r\n-                    memory_mask=memory_mask,\r\n-                    tgt_key_padding_mask=tgt_key_padding_mask,\r\n-                    memory_key_padding_mask=memory_key_padding_mask\r\n-                )\r\n-        return tgt\r\n-\r\n-    def forward(\r\n-        self,\r\n-        src: Tensor,\r\n-        tgt: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Forward pass with memory tracking and optimization.\r\n-        \r\n-        Args:\r\n-            src: Source sequence tensor\r\n-            tgt: Target sequence tensor\r\n-            src_mask: Optional mask for source sequence\r\n-            tgt_mask: Optional mask for target sequence\r\n-            src_key_padding_mask: Optional key padding mask for source sequence\r\n-            tgt_key_padding_mask: Optional key padding mask for target sequence\r\n-            \r\n-        Returns:\r\n-            Output tensor from the transformer\r\n-        \"\"\"\r\n-        if torch.cuda.is_available():\r\n-            torch.cuda.reset_peak_memory_stats()\r\n-            \r\n-        # Encode source sequence\r\n-        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n-        \r\n-        # Decode target sequence\r\n-        output = self.decode(\r\n-            tgt,\r\n-            memory,\r\n-            tgt_mask=tgt_mask,\r\n-            memory_mask=None,\r\n-            tgt_key_padding_mask=tgt_key_padding_mask,\r\n-            memory_key_padding_mask=src_key_padding_mask\r\n-        )\r\n-        \r\n-        # Project to output dimension\r\n-        output = self.output_projection(output)\r\n-        \r\n-        # Track performance metrics\r\n-        self.perf_stats['forward_pass_count'] += 1\r\n-        if torch.cuda.is_available():\r\n-            peak_mem = torch.cuda.max_memory_allocated()\r\n-            self.perf_stats['peak_memory'] = max(self.perf_stats['peak_memory'], peak_mem)\r\n-        \r\n-        return output\r\n-\r\n-    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n-        \"\"\"Generate an optimized attention mask.\r\n-        \r\n-        Args:\r\n-            sz: Size of the square mask\r\n-            \r\n-        Returns:\r\n-            A square mask tensor for attention\r\n-        \"\"\"\r\n-        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\r\n-        mask = mask.float().masked_fill(mask == 1, float('-inf'))\r\n-        device = next(self.parameters()).device\r\n-        return mask.to(device=device)\r\n-\r\n-    @abstractmethod\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n-        pass\r\n-\r\n-    @abstractmethod\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n-        pass\r\n-\r\n-    def get_performance_stats(self) -> Dict[str, float]:\r\n-        \"\"\"Return performance statistics for monitoring.\r\n-        \r\n-        Returns:\r\n-            Dictionary containing performance metrics including memory usage,\r\n-            forward pass counts, and average memory consumption per forward pass.\r\n-        \"\"\"\r\n-        stats = self.perf_stats.copy()\r\n-        if self.perf_stats['forward_pass_count'] > 0:\r\n-            stats['avg_memory_per_forward'] = (\r\n-                self.perf_stats['peak_memory'] / \r\n-                self.perf_stats['forward_pass_count']\r\n-            )\r\n-        return stats\r\n-    \r\n-    def get_attention_stats(self) -> Dict[str, Any]:\r\n-        \"\"\"Return statistics about attention mechanism usage.\r\n-        \r\n-        Returns:\r\n-            Dictionary containing attention-related metrics including whether\r\n-            Flash Attention is being used and chunk sizes for attention computation.\r\n-        \"\"\"\r\n-        return {\r\n-            'using_flash_attention': self.has_flash_attention,\r\n-            'attention_chunk_size': self.attention_batch_size,\r\n-            'attention_dropout': self.attention_dropout,\r\n-        }\r\n-    \r\n-    def get_memory_usage(self) -> Dict[str, float]:\r\n-        \"\"\"Get current GPU memory usage statistics.\r\n-        \r\n-        Returns:\r\n-            Dictionary containing current memory usage metrics in GB.\r\n-            Returns empty dict if not running on GPU.\r\n-        \"\"\"\r\n-        if not torch.cuda.is_available():\r\n-            return {}\r\n-            \r\n-        return {\r\n-            'current_allocated': torch.cuda.memory_allocated() / 1024**3,\r\n-            'max_allocated': torch.cuda.max_memory_allocated() / 1024**3,\r\n-            'current_reserved': torch.cuda.memory_reserved() / 1024**3,\r\n-            'max_reserved': torch.cuda.max_memory_reserved() / 1024**3\r\n-        }\r\n-\r\n-    def _optimize_memory_usage(self) -> None:\r\n-        \"\"\"Optimize memory usage by clearing unnecessary buffers.\r\n-        \r\n-        This method is called periodically during training to help manage\r\n-        GPU memory usage and prevent OOM errors.\r\n-        \"\"\"\r\n-        if not torch.cuda.is_available():\r\n-            return\r\n-            \r\n-        # Get current memory usage\r\n-        current_mem = torch.cuda.memory_allocated() / 1024**3\r\n-        \r\n-        # If using more than 80% of available memory, try to optimize\r\n-        if current_mem > 6.4:  # 80% of 8GB\r\n-            # Clear cache\r\n-            torch.cuda.empty_cache()\r\n-            \r\n-            # Clear unused buffers\r\n-            for param in self.parameters():\r\n-                if param.grad is not None:\r\n-                    param.grad.detach_()\r\n-                    param.grad.zero_()\r\n-            \r\n-            # Log memory usage after optimization\r\n-            new_mem = torch.cuda.memory_allocated() / 1024**3\r\n-            self.logger.debug(\r\n-                f\"Memory optimization performed: {current_mem:.2f}GB -> {new_mem:.2f}GB\"\r\n-            )\r\n-\r\n-    def get_input_dims(self) -> int:\r\n-        \"\"\"Return the input dimensions of the model.\r\n-        \r\n-        Returns:\r\n-            Number of input features the model expects.\r\n-        \"\"\"\r\n-        return self.input_features\r\n-\r\n-    def get_output_dims(self) -> int:\r\n-        \"\"\"Return the output dimensions of the model.\r\n-        \r\n-        Returns:\r\n-            Number of output features the model produces.\r\n-        \"\"\"\r\n-        return self.output_features\r\n-\r\n-    def should_apply_checkpointing(self) -> bool:\r\n-        \"\"\"Determine if gradient checkpointing should be applied.\r\n-        \r\n-        Returns:\r\n-            Boolean indicating whether gradient checkpointing should be used\r\n-            based on current memory usage and model configuration.\r\n-        \"\"\"\r\n-        if not self.use_checkpointing:\r\n-            return False\r\n-            \r\n-        if not torch.cuda.is_available():\r\n-            return False\r\n-            \r\n-        # Check current memory usage\r\n-        current_mem = torch.cuda.memory_allocated() / 1024**3\r\n-        \r\n-        # Enable checkpointing if memory usage is high\r\n-        return current_mem > 5.0  # 5GB threshold for 8GB GPU\r\n-\r\n-    @property\r\n-    def device(self) -> torch.device:\r\n-        \"\"\"Get the device the model is currently on.\r\n-        \r\n-        Returns:\r\n-            torch.device: The device (CPU/GPU) currently being used.\r\n-        \"\"\"\r\n-        return next(self.parameters()).device\r\n-\r\n-    def to_device(self, device: torch.device) -> None:\r\n-        \"\"\"Move the model to the specified device.\r\n-        \r\n-        This method ensures all model components are moved to the same device\r\n-        and updates internal device tracking.\r\n-        \r\n-        Args:\r\n-            device: The target device to move the model to.\r\n-        \"\"\"\r\n-        self.to(device)\r\n-        \r\n-        # Ensure all components are on the correct device\r\n-        if hasattr(self, 'encoder_embedding'):\r\n-            self.encoder_embedding = self.encoder_embedding.to(device)\r\n-        if hasattr(self, 'decoder_embedding'):\r\n-            self.decoder_embedding = self.decoder_embedding.to(device)\r\n-        if hasattr(self, 'output_projection'):\r\n-            self.output_projection = self.output_projection.to(device)\r\n-            \r\n-        self.logger.debug(f\"Model moved to device: {device}\")\r\n-\r\n-    def train(self, mode: bool = True) -> 'BaseTransformer':\r\n-        \"\"\"Set the model to training mode.\r\n-        \r\n-        Overrides the default train() method to ensure proper setup for\r\n-        training mode changes.\r\n-        \r\n-        Args:\r\n-            mode: Boolean indicating whether to set training mode on (True)\r\n-                 or evaluation mode (False)\r\n-                 \r\n-        Returns:\r\n-            Self for method chaining\r\n-        \"\"\"\r\n-        super().train(mode)\r\n-        if mode:\r\n-            # Reset performance stats when entering training mode\r\n-            self.perf_stats = {\r\n-                'attention_time': 0.0,\r\n-                'forward_pass_count': 0,\r\n-                'peak_memory': 0.0\r\n-            }\r\n-        return self\r\n-\r\n-    def eval(self) -> 'BaseTransformer':\r\n-        \"\"\"Set the model to evaluation mode.\r\n-        \r\n-        Overrides the default eval() method to ensure proper setup for\r\n-        evaluation mode changes.\r\n-        \r\n-        Returns:\r\n-            Self for method chaining\r\n-        \"\"\"\r\n-        super().eval()\r\n         return self\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733266209806,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,59 +1,40 @@\n from abc import ABC, abstractmethod\r\n-from typing import Optional, Dict, Any, Tuple, Union, cast, Callable\r\n import math\r\n import torch\r\n from torch import nn, Tensor\r\n+from typing import Optional, Dict, Any, Tuple, cast\r\n import torch.nn.functional as F\r\n from torch.utils.checkpoint import checkpoint\r\n-\r\n from models.base.base_model import BaseModel\r\n from models.components.embeddings import CombinedEmbedding\r\n from models.components.attention import MultiHeadAttention\r\n from utils.logging.logger import Logger\r\n from utils.logging.config import LoggerConfig, LogLevel\r\n \r\n class BaseTransformer(BaseModel, ABC):\r\n-    \"\"\"Base class for transformer models with memory-efficient implementation.\r\n-    \r\n-    This implementation is optimized for training on GPUs with limited VRAM (e.g., RTX 4060 8GB)\r\n-    and includes features like gradient checkpointing, Flash Attention when available, and\r\n-    chunked attention computation for memory efficiency.\r\n-    \"\"\"\r\n-\r\n     def __init__(self, config: Dict[str, Any]):\r\n-        \"\"\"Initialize the transformer with the given configuration.\r\n-        \r\n-        Args:\r\n-            config: A dictionary containing model configuration parameters including:\r\n-                - d_model: The model's hidden dimension size\r\n-                - n_heads: Number of attention heads\r\n-                - input_features: Number of input features\r\n-                - dropout: Dropout rate\r\n-                - use_checkpointing: Whether to use gradient checkpointing\r\n-                - use_flash_attention: Whether to use Flash Attention when available\r\n-                - attention_batch_size: Size of chunks for attention computation\r\n-        \"\"\"\r\n         super().__init__(config)\r\n         \r\n-        # Initialize logger\r\n+        # Initialize logger with performance monitoring\r\n         logger_config = LoggerConfig(\r\n             level=LogLevel.INFO,\r\n             component_name=\"BaseTransformer\",\r\n             include_timestamp=True\r\n         )\r\n         self.logger = Logger.get_logger(__name__, logger_config)\r\n         self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n \r\n-        # Validate configuration\r\n+        # Validate and set up configuration\r\n         self._validate_config(config)\r\n         self._setup_model_dimensions(config)\r\n         \r\n         # Memory optimization flags\r\n         self.use_checkpointing = config.get('use_checkpointing', True)\r\n         self.attention_dropout = config.get('attention_dropout', 0.1)\r\n+        self.activation_checkpointing = config.get('activation_checkpointing', True)\r\n         \r\n-        # Initialize embeddings\r\n+        # Initialize embeddings with memory optimization\r\n         self.encoder_embedding = self._create_embedding()\r\n         self.decoder_embedding = self._create_embedding()\r\n         \r\n         # Initialize flash attention if available\r\n@@ -67,9 +48,9 @@\n                 self.has_flash_attention = False\r\n                 self.logger.warning(\"Flash Attention not available, falling back to standard attention\")\r\n \r\n         # Memory-efficient attention settings\r\n-        self.attention_batch_size = config.get('attention_batch_size', 1024)\r\n+        self.attention_batch_size = config.get('attention_batch_size', 1024)  # For chunked attention computation\r\n         \r\n         # Output projection with improved initialization\r\n         self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n         self._initialize_output_projection()\r\n@@ -81,112 +62,80 @@\n             'peak_memory': 0.0\r\n         }\r\n \r\n     def _validate_config(self, config: Dict[str, Any]) -> None:\r\n-        \"\"\"Validate the configuration parameters.\"\"\"\r\n+        \"\"\"Validate transformer configuration parameters.\"\"\"\r\n         required_keys = ['d_model', 'n_heads', 'input_features']\r\n-        missing_keys = [key for key in required_keys if key not in config]\r\n-        if missing_keys:\r\n-            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+        for key in required_keys:\r\n+            if key not in config:\r\n+                raise ValueError(f\"Missing required config parameter: {key}\")\r\n+                \r\n+        if config['d_model'] % config['n_heads'] != 0:\r\n+            raise ValueError(\r\n+                f\"d_model ({config['d_model']}) must be divisible by \"\r\n+                f\"n_heads ({config['n_heads']})\"\r\n+            )\r\n+            \r\n+        if config['d_model'] <= 0 or config['n_heads'] <= 0:\r\n+            raise ValueError(\r\n+                \"d_model and n_heads must be positive integers\"\r\n+            )\r\n+            \r\n+        if 'dropout' in config and not 0 <= config['dropout'] <= 1:\r\n+            raise ValueError(\"dropout must be between 0 and 1\")\r\n+            \r\n+        if 'attention_dropout' in config and not 0 <= config['attention_dropout'] <= 1:\r\n+            raise ValueError(\"attention_dropout must be between 0 and 1\")\r\n \r\n+    @abstractmethod\r\n+    def get_input_dims(self) -> int:\r\n+        \"\"\"Get input dimensions.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def get_output_dims(self) -> int:\r\n+        \"\"\"Get output dimensions.\"\"\"\r\n+        pass\r\n+\r\n     def _setup_model_dimensions(self, config: Dict[str, Any]) -> None:\r\n-        \"\"\"Set up model dimensions and validate their compatibility.\"\"\"\r\n+        \"\"\"Set up model dimensions with optimized defaults for 8GB VRAM.\"\"\"\r\n         self.d_model = config['d_model']\r\n         self.input_features = config['input_features']\r\n         self.output_features = config.get('output_features', 1)\r\n         \r\n-        # Calculate attention dimensions\r\n+        # Calculate optimal attention dimensions\r\n         self.n_heads = config['n_heads']\r\n         self.head_dim = self.d_model // self.n_heads\r\n         \r\n-        # Validate dimensions\r\n-        if self.d_model % self.n_heads != 0:\r\n-            raise ValueError(f\"d_model ({self.d_model}) must be divisible by n_heads ({self.n_heads})\")\r\n-        \r\n-        # Set dropout rates\r\n+        # Memory-efficient settings\r\n         self.dropout = config.get('dropout', 0.1)\r\n         self.attention_dropout = config.get('attention_dropout', 0.1)\r\n+        \r\n+        self.logger.debug(\"Model dimensions initialized\", {\r\n+            \"d_model\": self.d_model,\r\n+            \"input_features\": self.input_features,\r\n+            \"output_features\": self.output_features,\r\n+            \"n_heads\": self.n_heads,\r\n+            \"head_dim\": self.head_dim\r\n+        })\r\n \r\n     def _create_embedding(self) -> CombinedEmbedding:\r\n-        \"\"\"Create the embedding layer with proper initialization.\"\"\"\r\n+        \"\"\"Create memory-efficient embedding layer.\"\"\"\r\n         return CombinedEmbedding(\r\n             d_model=self.d_model,\r\n             input_features=self.input_features,\r\n-            dropout=self.dropout\r\n+            dropout=self.dropout,\r\n+            max_seq_len=self.config.get('max_seq_len', 5000)  # Changed parameter name\r\n         )\r\n \r\n-    def _initialize_output_projection(self) -> None:\r\n-        \"\"\"Initialize the output projection layer with Xavier initialization.\"\"\"\r\n-        nn.init.xavier_uniform_(self.output_projection.weight)\r\n-        if self.output_projection.bias is not None:\r\n-            nn.init.zeros_(self.output_projection.bias)\r\n-\r\n-    def _create_encoder_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n-        \"\"\"Create a type-safe forward function for encoder checkpointing.\r\n-        \r\n-        Args:\r\n-            layer: The encoder layer module\r\n-            \r\n-        Returns:\r\n-            A callable that handles the forward pass with proper type safety\r\n-        \"\"\"\r\n-        def custom_forward(*inputs: Any) -> Tensor:\r\n-            if not all(isinstance(x, (torch.Tensor, type(None))) for x in inputs):\r\n-                raise TypeError(\"All inputs must be tensors or None\")\r\n-            \r\n-            src = cast(Tensor, inputs[0])\r\n-            src_mask = inputs[1]  # Optional[Tensor]\r\n-            src_key_padding_mask = inputs[2]  # Optional[Tensor]\r\n-            \r\n-            return layer(\r\n-                src,\r\n-                src_mask=src_mask,\r\n-                src_key_padding_mask=src_key_padding_mask\r\n-            )\r\n-        return custom_forward\r\n-\r\n-    def _create_decoder_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n-        \"\"\"Create a type-safe forward function for decoder checkpointing.\r\n-        \r\n-        Args:\r\n-            layer: The decoder layer module\r\n-            \r\n-        Returns:\r\n-            A callable that handles the forward pass with proper type safety\r\n-        \"\"\"\r\n-        def custom_forward(*inputs: Any) -> Tensor:\r\n-            if not all(isinstance(x, (torch.Tensor, type(None))) for x in inputs):\r\n-                raise TypeError(\"All inputs must be tensors or None\")\r\n-            \r\n-            tgt = cast(Tensor, inputs[0])\r\n-            memory = cast(Tensor, inputs[1])\r\n-            tgt_mask = inputs[2]  # Optional[Tensor]\r\n-            memory_mask = inputs[3]  # Optional[Tensor]\r\n-            tgt_key_padding_mask = inputs[4]  # Optional[Tensor]\r\n-            memory_key_padding_mask = inputs[5]  # Optional[Tensor]\r\n-            \r\n-            return layer(\r\n-                tgt,\r\n-                memory,\r\n-                tgt_mask=tgt_mask,\r\n-                memory_mask=memory_mask,\r\n-                tgt_key_padding_mask=tgt_key_padding_mask,\r\n-                memory_key_padding_mask=memory_key_padding_mask\r\n-            )\r\n-        return custom_forward\r\n-\r\n     def _compute_attention_chunked(\r\n         self, \r\n         query: Tensor, \r\n         key: Tensor, \r\n         value: Tensor, \r\n         mask: Optional[Tensor] = None\r\n     ) -> Tensor:\r\n-        \"\"\"Compute attention scores in memory-efficient chunks.\r\n-        \r\n-        This method processes attention computation in smaller chunks to reduce memory\r\n-        usage, which is particularly important for long sequences.\r\n-        \"\"\"\r\n+        \"\"\"Compute attention scores in chunks to save memory.\"\"\" \r\n         batch_size, seq_len, _ = query.size()\r\n         chunks = math.ceil(seq_len / self.attention_batch_size)\r\n         \r\n         outputs = []\r\n@@ -205,9 +154,9 @@\n                     dropout_p=self.attention_dropout if self.training else 0.0,\r\n                     is_causal=False\r\n                 )\r\n             else:\r\n-                # Standard attention computation\r\n+                # Standard attention for the chunk\r\n                 scores = torch.matmul(chunk_q, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\r\n                 if chunk_mask is not None:\r\n                     scores = scores.masked_fill(chunk_mask == 0, float('-inf'))\r\n                 weights = F.softmax(scores, dim=-1)\r\n@@ -218,50 +167,34 @@\n             \r\n         return torch.cat(outputs, dim=1)\r\n \r\n     def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n-            src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n-        \"\"\"Encode input sequence with gradient diagnostics.\"\"\"\r\n-        # Check input tensor gradients\r\n-        self.diagnose_gradients(\"Input Tensor\", src)\r\n+              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n+        \"\"\"Memory-efficient encoding process.\"\"\"\r\n+        if torch.cuda.is_available():\r\n+            init_mem = torch.cuda.memory_allocated()\r\n         \r\n-        # Check model state before encoding\r\n-        self.diagnose_gradients(\"Encoder Embedding\", self.encoder_embedding)\r\n-        \r\n-        # Apply embedding\r\n         src = self.encoder_embedding(src)\r\n         \r\n-        # Check embedded tensor gradients\r\n-        self.diagnose_gradients(\"After Embedding\", src)\r\n-        \r\n-        # Process encoder layers with optional checkpointing\r\n         for i, layer in enumerate(self.encoder_layers):\r\n-            self.diagnose_gradients(f\"Encoder Layer {i}\", layer)\r\n-            \r\n             if self.use_checkpointing and self.training:\r\n-                custom_forward = self._create_encoder_forward(layer)\r\n-                # Ensure inputs require gradients before checkpointing\r\n-                if not src.requires_grad:\r\n-                    src.requires_grad = True\r\n-                    self.logger.warning(f\"Fixed missing requires_grad in layer {i}\")\r\n-                \r\n-                output = checkpoint(\r\n-                    custom_forward,\r\n-                    src, src_mask, src_key_padding_mask,\r\n-                    preserve_rng_state=True,\r\n-                    use_reentrant=False\r\n-                )\r\n-                src = cast(Tensor, output)\r\n+                src = cast(Tensor, checkpoint(  # Cast checkpoint result to Tensor\r\n+                    layer,\r\n+                    src,\r\n+                    src_mask,\r\n+                    src_key_padding_mask\r\n+                ))\r\n             else:\r\n                 src = layer(\r\n                     src,\r\n                     src_mask=src_mask,\r\n                     src_key_padding_mask=src_key_padding_mask\r\n                 )\r\n+        \r\n+        if torch.cuda.is_available():\r\n+            peak_mem = torch.cuda.max_memory_allocated() - init_mem\r\n+            self.perf_stats['peak_memory'] = max(self.perf_stats['peak_memory'], peak_mem)\r\n             \r\n-            # Check output gradients after each layer\r\n-            self.diagnose_gradients(f\"After Layer {i}\", src)\r\n-        \r\n         return src\r\n \r\n     def decode(\r\n         self,\r\n@@ -271,35 +204,22 @@\n         memory_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n         memory_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n-        \"\"\"Decode target sequence with memory-efficient processing.\r\n-        \r\n-        Args:\r\n-            tgt: Target sequence tensor\r\n-            memory: Memory from encoder\r\n-            tgt_mask: Optional mask for target sequence\r\n-            memory_mask: Optional mask for memory\r\n-            tgt_key_padding_mask: Optional key padding mask for target sequence\r\n-            memory_key_padding_mask: Optional key padding mask for memory\r\n-            \r\n-        Returns:\r\n-            Decoded sequence tensor\r\n-        \"\"\"\r\n-        # Apply embedding\r\n+        \"\"\"Memory-efficient decoding process.\"\"\"\r\n         tgt = self.decoder_embedding(tgt)\r\n         \r\n-        # Process decoder layers with optional checkpointing\r\n         for layer in self.decoder_layers:\r\n             if self.use_checkpointing and self.training:\r\n-                custom_forward = self._create_decoder_forward(layer)\r\n-                output = checkpoint(\r\n-                    custom_forward,\r\n-                    tgt, memory, tgt_mask, memory_mask,\r\n-                    tgt_key_padding_mask, memory_key_padding_mask,\r\n-                    preserve_rng_state=True\r\n-                )\r\n-                tgt = cast(Tensor, output)\r\n+                tgt = cast(Tensor, checkpoint(  # Cast checkpoint result to Tensor\r\n+                    layer,\r\n+                    tgt,\r\n+                    memory,\r\n+                    tgt_mask,\r\n+                    memory_mask,\r\n+                    tgt_key_padding_mask,\r\n+                    memory_key_padding_mask\r\n+                ))\r\n             else:\r\n                 tgt = layer(\r\n                     tgt,\r\n                     memory,\r\n@@ -318,21 +238,9 @@\n         tgt_mask: Optional[Tensor] = None,\r\n         src_key_padding_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n-        \"\"\"Forward pass with memory tracking and optimization.\r\n-        \r\n-        Args:\r\n-            src: Source sequence tensor\r\n-            tgt: Target sequence tensor\r\n-            src_mask: Optional mask for source sequence\r\n-            tgt_mask: Optional mask for target sequence\r\n-            src_key_padding_mask: Optional key padding mask for source sequence\r\n-            tgt_key_padding_mask: Optional key padding mask for target sequence\r\n-            \r\n-        Returns:\r\n-            Output tensor from the transformer\r\n-        \"\"\"\r\n+        \"\"\"Optimized forward pass with memory tracking.\"\"\"\r\n         if torch.cuda.is_available():\r\n             torch.cuda.reset_peak_memory_stats()\r\n             \r\n         # Encode source sequence\r\n@@ -342,14 +250,14 @@\n         output = self.decode(\r\n             tgt,\r\n             memory,\r\n             tgt_mask=tgt_mask,\r\n-            memory_mask=None,\r\n+            memory_mask=None,  # Memory mask not needed in transformer\r\n             tgt_key_padding_mask=tgt_key_padding_mask,\r\n             memory_key_padding_mask=src_key_padding_mask\r\n         )\r\n         \r\n-        # Project to output dimension\r\n+        # Project to output dimension with fused operations\r\n         output = self.output_projection(output)\r\n         \r\n         # Track performance metrics\r\n         self.perf_stats['forward_pass_count'] += 1\r\n@@ -359,227 +267,25 @@\n         \r\n         return output\r\n \r\n     def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n-        \"\"\"Generate an optimized attention mask.\r\n-        \r\n-        Args:\r\n-            sz: Size of the square mask\r\n-            \r\n-        Returns:\r\n-            A square mask tensor for attention\r\n-        \"\"\"\r\n+        \"\"\"Generate an optimized attention mask.\"\"\"\r\n         mask = torch.triu(torch.ones(sz, sz), diagonal=1)\r\n         mask = mask.float().masked_fill(mask == 1, float('-inf'))\r\n+        \r\n+        # Move mask to same device as model parameters\r\n         device = next(self.parameters()).device\r\n         return mask.to(device=device)\r\n \r\n-    @abstractmethod\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n-        pass\r\n+    def _check_memory_usage(self) -> None:\r\n+        \"\"\"Monitor GPU memory usage.\"\"\"\r\n+        if torch.cuda.is_available():\r\n+            current_mem = torch.cuda.memory_allocated() / 1024**2  # MB\r\n+            peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB\r\n+            self.logger.debug(f\"Current GPU memory: {current_mem:.2f}MB, Peak: {peak_mem:.2f}MB\")\r\n \r\n-    @abstractmethod\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n-        pass\r\n-\r\n     def get_performance_stats(self) -> Dict[str, float]:\r\n-        \"\"\"Return performance statistics for monitoring.\r\n-        \r\n-        Returns:\r\n-            Dictionary containing performance metrics including memory usage,\r\n-            forward pass counts, and average memory consumption per forward pass.\r\n-        \"\"\"\r\n+        \"\"\"Return performance statistics.\"\"\"\r\n         stats = self.perf_stats.copy()\r\n         if self.perf_stats['forward_pass_count'] > 0:\r\n-            stats['avg_memory_per_forward'] = (\r\n-                self.perf_stats['peak_memory'] / \r\n-                self.perf_stats['forward_pass_count']\r\n-            )\r\n-        return stats\r\n-    \r\n-    def get_attention_stats(self) -> Dict[str, Any]:\r\n-        \"\"\"Return statistics about attention mechanism usage.\r\n-        \r\n-        Returns:\r\n-            Dictionary containing attention-related metrics including whether\r\n-            Flash Attention is being used and chunk sizes for attention computation.\r\n-        \"\"\"\r\n-        return {\r\n-            'using_flash_attention': self.has_flash_attention,\r\n-            'attention_chunk_size': self.attention_batch_size,\r\n-            'attention_dropout': self.attention_dropout,\r\n-        }\r\n-        \r\n-    def diagnose_gradients(self, name: str, tensor_or_module: Union[torch.Tensor, nn.Module]) -> None:\r\n-        \"\"\"Diagnose gradient setup of tensors or modules.\r\n-        \r\n-        This helper function prints detailed information about gradient settings\r\n-        to help identify where gradients might be lost.\r\n-        \r\n-        Args:\r\n-            name: Identifier for the diagnostic output\r\n-            tensor_or_module: Tensor or module to diagnose\r\n-        \"\"\"\r\n-        if isinstance(tensor_or_module, torch.Tensor):\r\n-            self.logger.debug(f\"Gradient Diagnostic - {name}:\")\r\n-            self.logger.debug(f\"  - requires_grad: {tensor_or_module.requires_grad}\")\r\n-            self.logger.debug(f\"  - is_leaf: {tensor_or_module.is_leaf}\")\r\n-            self.logger.debug(f\"  - dtype: {tensor_or_module.dtype}\")\r\n-            self.logger.debug(f\"  - device: {tensor_or_module.device}\")\r\n-        elif isinstance(tensor_or_module, nn.Module):\r\n-            self.logger.debug(f\"Module Diagnostic - {name}:\")\r\n-            total_params = 0\r\n-            params_with_grad = 0\r\n-            for param in tensor_or_module.parameters():\r\n-                total_params += 1\r\n-                if param.requires_grad:\r\n-                    params_with_grad += 1\r\n-            self.logger.debug(f\"  - Parameters with requires_grad: {params_with_grad}/{total_params}\")\r\n-            self.logger.debug(f\"  - Training mode: {tensor_or_module.training}\")\r\n-    \r\n-    def get_memory_usage(self) -> Dict[str, float]:\r\n-        \"\"\"Get current GPU memory usage statistics.\r\n-        \r\n-        Returns:\r\n-            Dictionary containing current memory usage metrics in GB.\r\n-            Returns empty dict if not running on GPU.\r\n-        \"\"\"\r\n-        if not torch.cuda.is_available():\r\n-            return {}\r\n-            \r\n-        return {\r\n-            'current_allocated': torch.cuda.memory_allocated() / 1024**3,\r\n-            'max_allocated': torch.cuda.max_memory_allocated() / 1024**3,\r\n-            'current_reserved': torch.cuda.memory_reserved() / 1024**3,\r\n-            'max_reserved': torch.cuda.max_memory_reserved() / 1024**3\r\n-        }\r\n-\r\n-    def _optimize_memory_usage(self) -> None:\r\n-        \"\"\"Optimize memory usage by clearing unnecessary buffers.\r\n-        \r\n-        This method is called periodically during training to help manage\r\n-        GPU memory usage and prevent OOM errors.\r\n-        \"\"\"\r\n-        if not torch.cuda.is_available():\r\n-            return\r\n-            \r\n-        # Get current memory usage\r\n-        current_mem = torch.cuda.memory_allocated() / 1024**3\r\n-        \r\n-        # If using more than 80% of available memory, try to optimize\r\n-        if current_mem > 6.4:  # 80% of 8GB\r\n-            # Clear cache\r\n-            torch.cuda.empty_cache()\r\n-            \r\n-            # Clear unused buffers\r\n-            for param in self.parameters():\r\n-                if param.grad is not None:\r\n-                    param.grad.detach_()\r\n-                    param.grad.zero_()\r\n-            \r\n-            # Log memory usage after optimization\r\n-            new_mem = torch.cuda.memory_allocated() / 1024**3\r\n-            self.logger.debug(\r\n-                f\"Memory optimization performed: {current_mem:.2f}GB -> {new_mem:.2f}GB\"\r\n-            )\r\n-\r\n-    def get_input_dims(self) -> int:\r\n-        \"\"\"Return the input dimensions of the model.\r\n-        \r\n-        Returns:\r\n-            Number of input features the model expects.\r\n-        \"\"\"\r\n-        return self.input_features\r\n-\r\n-    def get_output_dims(self) -> int:\r\n-        \"\"\"Return the output dimensions of the model.\r\n-        \r\n-        Returns:\r\n-            Number of output features the model produces.\r\n-        \"\"\"\r\n-        return self.output_features\r\n-\r\n-    def should_apply_checkpointing(self) -> bool:\r\n-        \"\"\"Determine if gradient checkpointing should be applied.\r\n-        \r\n-        Returns:\r\n-            Boolean indicating whether gradient checkpointing should be used\r\n-            based on current memory usage and model configuration.\r\n-        \"\"\"\r\n-        if not self.use_checkpointing:\r\n-            return False\r\n-            \r\n-        if not torch.cuda.is_available():\r\n-            return False\r\n-            \r\n-        # Check current memory usage\r\n-        current_mem = torch.cuda.memory_allocated() / 1024**3\r\n-        \r\n-        # Enable checkpointing if memory usage is high\r\n-        return current_mem > 5.0  # 5GB threshold for 8GB GPU\r\n-\r\n-    @property\r\n-    def device(self) -> torch.device:\r\n-        \"\"\"Get the device the model is currently on.\r\n-        \r\n-        Returns:\r\n-            torch.device: The device (CPU/GPU) currently being used.\r\n-        \"\"\"\r\n-        return next(self.parameters()).device\r\n-\r\n-    def to_device(self, device: torch.device) -> None:\r\n-        \"\"\"Move the model to the specified device.\r\n-        \r\n-        This method ensures all model components are moved to the same device\r\n-        and updates internal device tracking.\r\n-        \r\n-        Args:\r\n-            device: The target device to move the model to.\r\n-        \"\"\"\r\n-        self.to(device)\r\n-        \r\n-        # Ensure all components are on the correct device\r\n-        if hasattr(self, 'encoder_embedding'):\r\n-            self.encoder_embedding = self.encoder_embedding.to(device)\r\n-        if hasattr(self, 'decoder_embedding'):\r\n-            self.decoder_embedding = self.decoder_embedding.to(device)\r\n-        if hasattr(self, 'output_projection'):\r\n-            self.output_projection = self.output_projection.to(device)\r\n-            \r\n-        self.logger.debug(f\"Model moved to device: {device}\")\r\n-\r\n-    def train(self, mode: bool = True) -> 'BaseTransformer':\r\n-        \"\"\"Set the model to training mode.\r\n-        \r\n-        Overrides the default train() method to ensure proper setup for\r\n-        training mode changes.\r\n-        \r\n-        Args:\r\n-            mode: Boolean indicating whether to set training mode on (True)\r\n-                 or evaluation mode (False)\r\n-                 \r\n-        Returns:\r\n-            Self for method chaining\r\n-        \"\"\"\r\n-        super().train(mode)\r\n-        if mode:\r\n-            # Reset performance stats when entering training mode\r\n-            self.perf_stats = {\r\n-                'attention_time': 0.0,\r\n-                'forward_pass_count': 0,\r\n-                'peak_memory': 0.0\r\n-            }\r\n-        return self\r\n-\r\n-    def eval(self) -> 'BaseTransformer':\r\n\\ No newline at end of file\n-        \"\"\"Set the model to evaluation mode.\r\n-        \r\n-        Overrides the default eval() method to ensure proper setup for\r\n-        evaluation mode changes.\r\n-        \r\n-        Returns:\r\n-            Self for method chaining\r\n-        \"\"\"\r\n-        super().eval()\r\n-        return self\n+            stats['avg_memory_per_forward'] = self.perf_stats['peak_memory'] / self.perf_stats['forward_pass_count']\r\n+        return stats\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733266236263,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,225 @@\n+from abc import ABC, abstractmethod\r\n+from typing import Optional, Dict, Any, Tuple\r\n+import torch\r\n+from torch import nn, Tensor\r\n+\r\n+from models.base.base_model import BaseModel\r\n+from models.components.embeddings import CombinedEmbedding\r\n+from models.components.attention import MultiHeadAttention\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n+\r\n+class BaseTransformer(BaseModel, ABC):\r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        super().__init__(config)\r\n+        \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"BaseTransformer\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n+\r\n+        # Validate configuration\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        if missing_keys := [key for key in required_keys if key not in config]:\r\n+            self.logger.error(\"Missing required configuration keys\", {\"missing_keys\": missing_keys})\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n+        self.d_model = config['d_model']\r\n+        self.input_features = config['input_features']\r\n+        self.output_features = config.get('output_features', 1)\r\n+        \r\n+        self.logger.debug(\"Model dimensions initialized\", {\r\n+            \"d_model\": self.d_model,\r\n+            \"input_features\": self.input_features,\r\n+            \"output_features\": self.output_features\r\n+        })\r\n+\r\n+        # Initialize embeddings\r\n+        self.logger.debug(\"Creating encoder embedding\")\r\n+        self.encoder_embedding = CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=config['dropout']\r\n+        )\r\n+\r\n+        self.logger.debug(\"Creating decoder embedding\")\r\n+        self.decoder_embedding = CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=config['dropout']\r\n+        )\r\n+\r\n+        # Initialize output projection\r\n+        self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n+        self.logger.debug(\"Initialized output projection layer\")\r\n+        \r\n+        # Initialize output projection weights\r\n+        self._initialize_output_projection()\r\n+\r\n+        self.debug_counter = 0\r\n+        self.max_debug_prints = 2\r\n+\r\n+    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n+        \"\"\"Validate configuration parameters.\"\"\"\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        if missing_keys := [key for key in required_keys if key not in config]:\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n+        if config['d_model'] % config['n_heads'] != 0:\r\n+            raise ValueError(\r\n+                f\"d_model ({config['d_model']}) must be divisible by \"\r\n+                f\"n_heads ({config['n_heads']})\"\r\n+            )\r\n+\r\n+    def _create_embedding(self) -> CombinedEmbedding:\r\n+        \"\"\"Create embedding layer with proper initialization.\"\"\"\r\n+        return CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=self.dropout\r\n+        )\r\n+\r\n+    def _initialize_output_projection(self) -> None:\r\n+        \"\"\"Initialize output projection layer weights.\"\"\"\r\n+        # Initialize weights with Xavier/Glorot initialization\r\n+        nn.init.xavier_uniform_(self.output_projection.weight)\r\n+        if self.output_projection.bias is not None:\r\n+            nn.init.zeros_(self.output_projection.bias)\r\n+\r\n+    @abstractmethod\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n+              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n+        \"\"\"Encode input sequence.\"\"\"\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            self.logger.debug(f\"Encoder device status (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"source_device\": str(src.device),\r\n+                \"source_mask_device\": str(src_mask.device) if src_mask is not None else \"None\",\r\n+                \"model_device\": str(next(self.parameters()).device)\r\n+            })\r\n+            self.debug_counter += 1\r\n+        \r\n+        # Move masks to correct device\r\n+        if src_mask is not None:\r\n+            src_mask = src_mask.to(src.device)\r\n+        if src_key_padding_mask is not None:\r\n+            src_key_padding_mask = src_key_padding_mask.to(src.device)\r\n+\r\n+        # Apply embedding\r\n+        src = self.encoder_embedding(src)\r\n+        \r\n+        # Pass through encoder layers\r\n+        for i, layer in enumerate(self.encoder_layers):\r\n+            self.logger.debug(\"Encoder layer processing\", {\r\n+                \"layer\": i,\r\n+                \"input_device\": str(src.device)\r\n+            })\r\n+            src = layer(src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\r\n+        return src\r\n+\r\n+    def decode(\r\n+        self,\r\n+        tgt: Tensor,\r\n+        memory: Tensor,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        memory_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+        memory_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Decode target sequence.\"\"\"\r\n+        # Apply embedding\r\n+        tgt = self.decoder_embedding(tgt)\r\n+        \r\n+        # Create or adjust masks to match sequence dimensions\r\n+        if memory_mask is not None:\r\n+            # Adjust memory mask shape to match [tgt_len, src_len]\r\n+            memory_mask = self._adjust_memory_mask(memory_mask, tgt.size(1), memory.size(1))\r\n+\r\n+        # Pass through decoder layers\r\n+        for layer in self.decoder_layers:\r\n+            tgt = layer(\r\n+                tgt,\r\n+                memory,\r\n+                tgt_mask=tgt_mask,\r\n+                memory_mask=memory_mask,\r\n+                tgt_key_padding_mask=tgt_key_padding_mask,\r\n+                memory_key_padding_mask=memory_key_padding_mask\r\n+            )\r\n+        return tgt\r\n+\r\n+    def _adjust_memory_mask(self, mask: Tensor, tgt_len: int, src_len: int) -> Tensor:\r\n+        \"\"\"Adjust memory mask to match required dimensions.\"\"\"\r\n+        if mask.size(0) != tgt_len or mask.size(1) != src_len:\r\n+            self.logger.debug(\"Adjusting memory mask dimensions\", {\r\n+                \"original_shape\": tuple(mask.shape),\r\n+                \"new_shape\": (tgt_len, src_len)\r\n+            })\r\n+            # Create new mask with correct dimensions\r\n+            new_mask = torch.zeros((tgt_len, src_len), device=mask.device)\r\n+            # Copy values where possible\r\n+            min_rows = min(mask.size(0), tgt_len)\r\n+            min_cols = min(mask.size(1), src_len)\r\n+            new_mask[:min_rows, :min_cols] = mask[:min_rows, :min_cols]\r\n+            return new_mask\r\n+        return mask\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        tgt: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Forward pass with correct mask handling.\"\"\"\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            self.logger.debug(f\"Forward pass details (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"source_shape\": tuple(src.shape),\r\n+                \"target_shape\": tuple(tgt.shape),\r\n+                \"source_mask_shape\": tuple(src_mask.shape) if src_mask is not None else None,\r\n+                \"target_mask_shape\": tuple(tgt_mask.shape) if tgt_mask is not None else None\r\n+            })\r\n+        \r\n+        # Encode source sequence\r\n+        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n+        \r\n+        # Decode target sequence with adjusted masks\r\n+        output = self.decode(\r\n+            tgt,\r\n+            memory,\r\n+            tgt_mask=tgt_mask,\r\n+            memory_mask=src_mask,  # This will be adjusted in decode()\r\n+            tgt_key_padding_mask=tgt_key_padding_mask,\r\n+            memory_key_padding_mask=src_key_padding_mask\r\n+        )\r\n+        \r\n+        # Project to output dimension\r\n+        output = self.output_projection(output)\r\n+        \r\n+        return output\r\n+\r\n+    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n+        \"\"\"Generate a square mask for the sequence.\"\"\"\r\n+        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n+        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n+        # Move mask to same device as model\r\n+        return mask.to(next(self.parameters()).device)\r\n+    \r\n+    def get_input_dims(self) -> int:\r\n+        return self.input_features\r\n+\r\n+    def get_output_dims(self) -> int:\r\n+        return 1\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733266284288,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,225 @@\n+from abc import ABC, abstractmethod\r\n+from typing import Optional, Dict, Any, Tuple\r\n+import torch\r\n+from torch import nn, Tensor\r\n+\r\n+from models.base.base_model import BaseModel\r\n+from models.components.embeddings import CombinedEmbedding\r\n+from models.components.attention import MultiHeadAttention\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n+\r\n+class BaseTransformer(BaseModel, ABC):\r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        super().__init__(config)\r\n+        \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"BaseTransformer\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n+\r\n+        # Validate configuration\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        if missing_keys := [key for key in required_keys if key not in config]:\r\n+            self.logger.error(\"Missing required configuration keys\", {\"missing_keys\": missing_keys})\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n+        self.d_model = config['d_model']\r\n+        self.input_features = config['input_features']\r\n+        self.output_features = config.get('output_features', 1)\r\n+        \r\n+        self.logger.debug(\"Model dimensions initialized\", {\r\n+            \"d_model\": self.d_model,\r\n+            \"input_features\": self.input_features,\r\n+            \"output_features\": self.output_features\r\n+        })\r\n+\r\n+        # Initialize embeddings\r\n+        self.logger.debug(\"Creating encoder embedding\")\r\n+        self.encoder_embedding = CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=config['dropout']\r\n+        )\r\n+\r\n+        self.logger.debug(\"Creating decoder embedding\")\r\n+        self.decoder_embedding = CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=config['dropout']\r\n+        )\r\n+\r\n+        # Initialize output projection\r\n+        self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n+        self.logger.debug(\"Initialized output projection layer\")\r\n+        \r\n+        # Initialize output projection weights\r\n+        self._initialize_output_projection()\r\n+\r\n+        self.debug_counter = 0\r\n+        self.max_debug_prints = 2\r\n+\r\n+    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n+        \"\"\"Validate configuration parameters.\"\"\"\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        if missing_keys := [key for key in required_keys if key not in config]:\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n+        if config['d_model'] % config['n_heads'] != 0:\r\n+            raise ValueError(\r\n+                f\"d_model ({config['d_model']}) must be divisible by \"\r\n+                f\"n_heads ({config['n_heads']})\"\r\n+            )\r\n+\r\n+    def _create_embedding(self) -> CombinedEmbedding:\r\n+        \"\"\"Create embedding layer with proper initialization.\"\"\"\r\n+        return CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=self.dropout\r\n+        )\r\n+\r\n+    def _initialize_output_projection(self) -> None:\r\n+        \"\"\"Initialize output projection layer weights.\"\"\"\r\n+        # Initialize weights with Xavier/Glorot initialization\r\n+        nn.init.xavier_uniform_(self.output_projection.weight)\r\n+        if self.output_projection.bias is not None:\r\n+            nn.init.zeros_(self.output_projection.bias)\r\n+\r\n+    @abstractmethod\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n+              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n+        \"\"\"Encode input sequence.\"\"\"\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            self.logger.debug(f\"Encoder device status (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"source_device\": str(src.device),\r\n+                \"source_mask_device\": str(src_mask.device) if src_mask is not None else \"None\",\r\n+                \"model_device\": str(next(self.parameters()).device)\r\n+            })\r\n+            self.debug_counter += 1\r\n+        \r\n+        # Move masks to correct device\r\n+        if src_mask is not None:\r\n+            src_mask = src_mask.to(src.device)\r\n+        if src_key_padding_mask is not None:\r\n+            src_key_padding_mask = src_key_padding_mask.to(src.device)\r\n+\r\n+        # Apply embedding\r\n+        src = self.encoder_embedding(src)\r\n+        \r\n+        # Pass through encoder layers\r\n+        for i, layer in enumerate(self.encoder_layers):\r\n+            self.logger.debug(\"Encoder layer processing\", {\r\n+                \"layer\": i,\r\n+                \"input_device\": str(src.device)\r\n+            })\r\n+            src = layer(src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\r\n+        return src\r\n+\r\n+    def decode(\r\n+        self,\r\n+        tgt: Tensor,\r\n+        memory: Tensor,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        memory_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+        memory_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Decode target sequence.\"\"\"\r\n+        # Apply embedding\r\n+        tgt = self.decoder_embedding(tgt)\r\n+        \r\n+        # Create or adjust masks to match sequence dimensions\r\n+        if memory_mask is not None:\r\n+            # Adjust memory mask shape to match [tgt_len, src_len]\r\n+            memory_mask = self._adjust_memory_mask(memory_mask, tgt.size(1), memory.size(1))\r\n+\r\n+        # Pass through decoder layers\r\n+        for layer in self.decoder_layers:\r\n+            tgt = layer(\r\n+                tgt,\r\n+                memory,\r\n+                tgt_mask=tgt_mask,\r\n+                memory_mask=memory_mask,\r\n+                tgt_key_padding_mask=tgt_key_padding_mask,\r\n+                memory_key_padding_mask=memory_key_padding_mask\r\n+            )\r\n+        return tgt\r\n+\r\n+    def _adjust_memory_mask(self, mask: Tensor, tgt_len: int, src_len: int) -> Tensor:\r\n+        \"\"\"Adjust memory mask to match required dimensions.\"\"\"\r\n+        if mask.size(0) != tgt_len or mask.size(1) != src_len:\r\n+            self.logger.debug(\"Adjusting memory mask dimensions\", {\r\n+                \"original_shape\": tuple(mask.shape),\r\n+                \"new_shape\": (tgt_len, src_len)\r\n+            })\r\n+            # Create new mask with correct dimensions\r\n+            new_mask = torch.zeros((tgt_len, src_len), device=mask.device)\r\n+            # Copy values where possible\r\n+            min_rows = min(mask.size(0), tgt_len)\r\n+            min_cols = min(mask.size(1), src_len)\r\n+            new_mask[:min_rows, :min_cols] = mask[:min_rows, :min_cols]\r\n+            return new_mask\r\n+        return mask\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        tgt: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Forward pass with correct mask handling.\"\"\"\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            self.logger.debug(f\"Forward pass details (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"source_shape\": tuple(src.shape),\r\n+                \"target_shape\": tuple(tgt.shape),\r\n+                \"source_mask_shape\": tuple(src_mask.shape) if src_mask is not None else None,\r\n+                \"target_mask_shape\": tuple(tgt_mask.shape) if tgt_mask is not None else None\r\n+            })\r\n+        \r\n+        # Encode source sequence\r\n+        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n+        \r\n+        # Decode target sequence with adjusted masks\r\n+        output = self.decode(\r\n+            tgt,\r\n+            memory,\r\n+            tgt_mask=tgt_mask,\r\n+            memory_mask=src_mask,  # This will be adjusted in decode()\r\n+            tgt_key_padding_mask=tgt_key_padding_mask,\r\n+            memory_key_padding_mask=src_key_padding_mask\r\n+        )\r\n+        \r\n+        # Project to output dimension\r\n+        output = self.output_projection(output)\r\n+        \r\n+        return output\r\n+\r\n+    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n+        \"\"\"Generate a square mask for the sequence.\"\"\"\r\n+        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n+        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n+        # Move mask to same device as model\r\n+        return mask.to(next(self.parameters()).device)\r\n+    \r\n+    def get_input_dims(self) -> int:\r\n+        return self.input_features\r\n+\r\n+    def get_output_dims(self) -> int:\r\n+        return 1\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733266295481,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,225 @@\n+from abc import ABC, abstractmethod\r\n+from typing import Optional, Dict, Any, Tuple\r\n+import torch\r\n+from torch import nn, Tensor\r\n+\r\n+from models.base.base_model import BaseModel\r\n+from models.components.embeddings import CombinedEmbedding\r\n+from models.components.attention import MultiHeadAttention\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n+\r\n+class BaseTransformer(BaseModel, ABC):\r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        super().__init__(config)\r\n+        \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"BaseTransformer\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n+\r\n+        # Validate configuration\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        if missing_keys := [key for key in required_keys if key not in config]:\r\n+            self.logger.error(\"Missing required configuration keys\", {\"missing_keys\": missing_keys})\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n+        self.d_model = config['d_model']\r\n+        self.input_features = config['input_features']\r\n+        self.output_features = config.get('output_features', 1)\r\n+        \r\n+        self.logger.debug(\"Model dimensions initialized\", {\r\n+            \"d_model\": self.d_model,\r\n+            \"input_features\": self.input_features,\r\n+            \"output_features\": self.output_features\r\n+        })\r\n+\r\n+        # Initialize embeddings\r\n+        self.logger.debug(\"Creating encoder embedding\")\r\n+        self.encoder_embedding = CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=config['dropout']\r\n+        )\r\n+\r\n+        self.logger.debug(\"Creating decoder embedding\")\r\n+        self.decoder_embedding = CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=config['dropout']\r\n+        )\r\n+\r\n+        # Initialize output projection\r\n+        self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n+        self.logger.debug(\"Initialized output projection layer\")\r\n+        \r\n+        # Initialize output projection weights\r\n+        self._initialize_output_projection()\r\n+\r\n+        self.debug_counter = 0\r\n+        self.max_debug_prints = 2\r\n+\r\n+    def _validate_config(self, config: Dict[str, Any]) -> None:\r\n+        \"\"\"Validate configuration parameters.\"\"\"\r\n+        required_keys = ['d_model', 'n_heads', 'input_features']\r\n+        if missing_keys := [key for key in required_keys if key not in config]:\r\n+            raise ValueError(f\"Missing required configuration keys: {missing_keys}\")\r\n+\r\n+        if config['d_model'] % config['n_heads'] != 0:\r\n+            raise ValueError(\r\n+                f\"d_model ({config['d_model']}) must be divisible by \"\r\n+                f\"n_heads ({config['n_heads']})\"\r\n+            )\r\n+\r\n+    def _create_embedding(self) -> CombinedEmbedding:\r\n+        \"\"\"Create embedding layer with proper initialization.\"\"\"\r\n+        return CombinedEmbedding(\r\n+            d_model=self.d_model,\r\n+            input_features=self.input_features,\r\n+            dropout=self.dropout\r\n+        )\r\n+\r\n+    def _initialize_output_projection(self) -> None:\r\n+        \"\"\"Initialize output projection layer weights.\"\"\"\r\n+        # Initialize weights with Xavier/Glorot initialization\r\n+        nn.init.xavier_uniform_(self.output_projection.weight)\r\n+        if self.output_projection.bias is not None:\r\n+            nn.init.zeros_(self.output_projection.bias)\r\n+\r\n+    @abstractmethod\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n+        pass\r\n+\r\n+    def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n+              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n+        \"\"\"Encode input sequence.\"\"\"\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            self.logger.debug(f\"Encoder device status (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"source_device\": str(src.device),\r\n+                \"source_mask_device\": str(src_mask.device) if src_mask is not None else \"None\",\r\n+                \"model_device\": str(next(self.parameters()).device)\r\n+            })\r\n+            self.debug_counter += 1\r\n+        \r\n+        # Move masks to correct device\r\n+        if src_mask is not None:\r\n+            src_mask = src_mask.to(src.device)\r\n+        if src_key_padding_mask is not None:\r\n+            src_key_padding_mask = src_key_padding_mask.to(src.device)\r\n+\r\n+        # Apply embedding\r\n+        src = self.encoder_embedding(src)\r\n+        \r\n+        # Pass through encoder layers\r\n+        for i, layer in enumerate(self.encoder_layers):\r\n+            self.logger.debug(\"Encoder layer processing\", {\r\n+                \"layer\": i,\r\n+                \"input_device\": str(src.device)\r\n+            })\r\n+            src = layer(src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask)\r\n+        return src\r\n+\r\n+    def decode(\r\n+        self,\r\n+        tgt: Tensor,\r\n+        memory: Tensor,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        memory_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+        memory_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Decode target sequence.\"\"\"\r\n+        # Apply embedding\r\n+        tgt = self.decoder_embedding(tgt)\r\n+        \r\n+        # Create or adjust masks to match sequence dimensions\r\n+        if memory_mask is not None:\r\n+            # Adjust memory mask shape to match [tgt_len, src_len]\r\n+            memory_mask = self._adjust_memory_mask(memory_mask, tgt.size(1), memory.size(1))\r\n+\r\n+        # Pass through decoder layers\r\n+        for layer in self.decoder_layers:\r\n+            tgt = layer(\r\n+                tgt,\r\n+                memory,\r\n+                tgt_mask=tgt_mask,\r\n+                memory_mask=memory_mask,\r\n+                tgt_key_padding_mask=tgt_key_padding_mask,\r\n+                memory_key_padding_mask=memory_key_padding_mask\r\n+            )\r\n+        return tgt\r\n+\r\n+    def _adjust_memory_mask(self, mask: Tensor, tgt_len: int, src_len: int) -> Tensor:\r\n+        \"\"\"Adjust memory mask to match required dimensions.\"\"\"\r\n+        if mask.size(0) != tgt_len or mask.size(1) != src_len:\r\n+            self.logger.debug(\"Adjusting memory mask dimensions\", {\r\n+                \"original_shape\": tuple(mask.shape),\r\n+                \"new_shape\": (tgt_len, src_len)\r\n+            })\r\n+            # Create new mask with correct dimensions\r\n+            new_mask = torch.zeros((tgt_len, src_len), device=mask.device)\r\n+            # Copy values where possible\r\n+            min_rows = min(mask.size(0), tgt_len)\r\n+            min_cols = min(mask.size(1), src_len)\r\n+            new_mask[:min_rows, :min_cols] = mask[:min_rows, :min_cols]\r\n+            return new_mask\r\n+        return mask\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        tgt: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Forward pass with correct mask handling.\"\"\"\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            self.logger.debug(f\"Forward pass details (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"source_shape\": tuple(src.shape),\r\n+                \"target_shape\": tuple(tgt.shape),\r\n+                \"source_mask_shape\": tuple(src_mask.shape) if src_mask is not None else None,\r\n+                \"target_mask_shape\": tuple(tgt_mask.shape) if tgt_mask is not None else None\r\n+            })\r\n+        \r\n+        # Encode source sequence\r\n+        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n+        \r\n+        # Decode target sequence with adjusted masks\r\n+        output = self.decode(\r\n+            tgt,\r\n+            memory,\r\n+            tgt_mask=tgt_mask,\r\n+            memory_mask=src_mask,  # This will be adjusted in decode()\r\n+            tgt_key_padding_mask=tgt_key_padding_mask,\r\n+            memory_key_padding_mask=src_key_padding_mask\r\n+        )\r\n+        \r\n+        # Project to output dimension\r\n+        output = self.output_projection(output)\r\n+        \r\n+        return output\r\n+\r\n+    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n+        \"\"\"Generate a square mask for the sequence.\"\"\"\r\n+        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n+        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n+        # Move mask to same device as model\r\n+        return mask.to(next(self.parameters()).device)\r\n+    \r\n+    def get_input_dims(self) -> int:\r\n+        return self.input_features\r\n+\r\n+    def get_output_dims(self) -> int:\r\n+        return 1\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733264259467,
            "name": "Commit-0",
            "content": "from abc import ABC\r\nimport math\r\nimport torch\r\nfrom torch import nn, Tensor\r\nfrom typing import Optional, Dict, Any, Tuple, cast\r\nimport torch.nn.functional as F\r\nfrom torch.utils.checkpoint import checkpoint\r\nfrom models.base.base_model import BaseModel\r\nfrom models.components.embeddings import CombinedEmbedding\r\nfrom models.components.attention import MultiHeadAttention\r\nfrom utils.logging.logger import Logger\r\nfrom utils.logging.config import LoggerConfig, LogLevel\r\n\r\nclass BaseTransformer(BaseModel, ABC):\r\n    def __init__(self, config: Dict[str, Any]):\r\n        super().__init__(config)\r\n        \r\n        # Initialize logger with performance monitoring\r\n        logger_config = LoggerConfig(\r\n            level=LogLevel.INFO,\r\n            component_name=\"BaseTransformer\",\r\n            include_timestamp=True\r\n        )\r\n        self.logger = Logger.get_logger(__name__, logger_config)\r\n        self.logger.debug(\"Initializing BaseTransformer\", {\"config\": config})\r\n\r\n        # Validate and set up configuration\r\n        self._validate_config(config)\r\n        self._setup_model_dimensions(config)\r\n        \r\n        # Memory optimization flags\r\n        self.use_checkpointing = config.get('use_checkpointing', True)\r\n        self.attention_dropout = config.get('attention_dropout', 0.1)\r\n        self.activation_checkpointing = config.get('activation_checkpointing', True)\r\n        \r\n        # Initialize embeddings with memory optimization\r\n        self.encoder_embedding = self._create_embedding()\r\n        self.decoder_embedding = self._create_embedding()\r\n        \r\n        # Initialize flash attention if available\r\n        self.use_flash_attention = config.get('use_flash_attention', True) and torch.cuda.is_available()\r\n        if self.use_flash_attention:\r\n            try:\r\n                from torch.nn.functional import scaled_dot_product_attention\r\n                self.has_flash_attention = True\r\n                self.logger.info(\"Flash Attention is enabled\")\r\n            except ImportError:\r\n                self.has_flash_attention = False\r\n                self.logger.warning(\"Flash Attention not available, falling back to standard attention\")\r\n\r\n        # Memory-efficient attention settings\r\n        self.attention_batch_size = config.get('attention_batch_size', 1024)  # For chunked attention computation\r\n        \r\n        # Output projection with improved initialization\r\n        self.output_projection = nn.Linear(self.d_model, self.output_features)\r\n        self._initialize_output_projection()\r\n        \r\n        # Initialize performance monitoring\r\n        self.perf_stats = {\r\n            'attention_time': 0.0,\r\n            'forward_pass_count': 0,\r\n            'peak_memory': 0.0\r\n        }\r\n\r\n    def _setup_model_dimensions(self, config: Dict[str, Any]) -> None:\r\n        \"\"\"Set up model dimensions with optimized defaults for 8GB VRAM.\"\"\"\r\n        self.d_model = config['d_model']\r\n        self.input_features = config['input_features']\r\n        self.output_features = config.get('output_features', 1)\r\n        \r\n        # Calculate optimal attention dimensions\r\n        self.n_heads = config['n_heads']\r\n        self.head_dim = self.d_model // self.n_heads\r\n        \r\n        # Memory-efficient settings\r\n        self.dropout = config.get('dropout', 0.1)\r\n        self.attention_dropout = config.get('attention_dropout', 0.1)\r\n        \r\n        self.logger.debug(\"Model dimensions initialized\", {\r\n            \"d_model\": self.d_model,\r\n            \"input_features\": self.input_features,\r\n            \"output_features\": self.output_features,\r\n            \"n_heads\": self.n_heads,\r\n            \"head_dim\": self.head_dim\r\n        })\r\n\r\n    def _create_embedding(self) -> CombinedEmbedding:\r\n        \"\"\"Create memory-efficient embedding layer.\"\"\"\r\n        return CombinedEmbedding(\r\n            d_model=self.d_model,\r\n            input_features=self.input_features,\r\n            dropout=self.dropout,\r\n            max_seq_len=self.config.get('max_seq_len', 5000)  # Changed parameter name\r\n        )\r\n\r\n    def _compute_attention_chunked(\r\n        self, \r\n        query: Tensor, \r\n        key: Tensor, \r\n        value: Tensor, \r\n        mask: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        \"\"\"Compute attention scores in chunks to save memory.\"\"\" \r\n        batch_size, seq_len, _ = query.size()\r\n        chunks = math.ceil(seq_len / self.attention_batch_size)\r\n        \r\n        outputs = []\r\n        for i in range(chunks):\r\n            start_idx = i * self.attention_batch_size\r\n            end_idx = min((i + 1) * self.attention_batch_size, seq_len)\r\n            \r\n            # Process chunk\r\n            chunk_q = query[:, start_idx:end_idx]\r\n            chunk_mask = mask[:, start_idx:end_idx] if mask is not None else None\r\n            \r\n            if self.has_flash_attention:\r\n                chunk_output = F.scaled_dot_product_attention(\r\n                    chunk_q, key, value,\r\n                    attn_mask=chunk_mask,\r\n                    dropout_p=self.attention_dropout if self.training else 0.0,\r\n                    is_causal=False\r\n                )\r\n            else:\r\n                # Standard attention for the chunk\r\n                scores = torch.matmul(chunk_q, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\r\n                if chunk_mask is not None:\r\n                    scores = scores.masked_fill(chunk_mask == 0, float('-inf'))\r\n                weights = F.softmax(scores, dim=-1)\r\n                weights = F.dropout(weights, p=self.attention_dropout, training=self.training)\r\n                chunk_output = torch.matmul(weights, value)\r\n            \r\n            outputs.append(chunk_output)\r\n            \r\n        return torch.cat(outputs, dim=1)\r\n\r\n    def encode(self, src: Tensor, src_mask: Optional[Tensor] = None,\r\n              src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\r\n        \"\"\"Memory-efficient encoding process.\"\"\"\r\n        if torch.cuda.is_available():\r\n            init_mem = torch.cuda.memory_allocated()\r\n        \r\n        src = self.encoder_embedding(src)\r\n        \r\n        for i, layer in enumerate(self.encoder_layers):\r\n            if self.use_checkpointing and self.training:\r\n                src = cast(Tensor, checkpoint(  # Cast checkpoint result to Tensor\r\n                    layer,\r\n                    src,\r\n                    src_mask,\r\n                    src_key_padding_mask\r\n                ))\r\n            else:\r\n                src = layer(\r\n                    src,\r\n                    src_mask=src_mask,\r\n                    src_key_padding_mask=src_key_padding_mask\r\n                )\r\n        \r\n        if torch.cuda.is_available():\r\n            peak_mem = torch.cuda.max_memory_allocated() - init_mem\r\n            self.perf_stats['peak_memory'] = max(self.perf_stats['peak_memory'], peak_mem)\r\n            \r\n        return src\r\n\r\n    def decode(\r\n        self,\r\n        tgt: Tensor,\r\n        memory: Tensor,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        memory_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n        memory_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Memory-efficient decoding process.\"\"\"\r\n        tgt = self.decoder_embedding(tgt)\r\n        \r\n        for layer in self.decoder_layers:\r\n            if self.use_checkpointing and self.training:\r\n                tgt = cast(Tensor, checkpoint(  # Cast checkpoint result to Tensor\r\n                    layer,\r\n                    tgt,\r\n                    memory,\r\n                    tgt_mask,\r\n                    memory_mask,\r\n                    tgt_key_padding_mask,\r\n                    memory_key_padding_mask\r\n                ))\r\n            else:\r\n                tgt = layer(\r\n                    tgt,\r\n                    memory,\r\n                    tgt_mask=tgt_mask,\r\n                    memory_mask=memory_mask,\r\n                    tgt_key_padding_mask=tgt_key_padding_mask,\r\n                    memory_key_padding_mask=memory_key_padding_mask\r\n                )\r\n        return tgt\r\n\r\n    def forward(\r\n        self,\r\n        src: Tensor,\r\n        tgt: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Optimized forward pass with memory tracking.\"\"\"\r\n        if torch.cuda.is_available():\r\n            torch.cuda.reset_peak_memory_stats()\r\n            \r\n        # Encode source sequence\r\n        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n        \r\n        # Decode target sequence\r\n        output = self.decode(\r\n            tgt,\r\n            memory,\r\n            tgt_mask=tgt_mask,\r\n            memory_mask=None,  # Memory mask not needed in transformer\r\n            tgt_key_padding_mask=tgt_key_padding_mask,\r\n            memory_key_padding_mask=src_key_padding_mask\r\n        )\r\n        \r\n        # Project to output dimension with fused operations\r\n        output = self.output_projection(output)\r\n        \r\n        # Track performance metrics\r\n        self.perf_stats['forward_pass_count'] += 1\r\n        if torch.cuda.is_available():\r\n            peak_mem = torch.cuda.max_memory_allocated()\r\n            self.perf_stats['peak_memory'] = max(self.perf_stats['peak_memory'], peak_mem)\r\n        \r\n        return output\r\n\r\n    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n        \"\"\"Generate an optimized attention mask.\"\"\"\r\n        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\r\n        mask = mask.float().masked_fill(mask == 1, float('-inf'))\r\n        \r\n        # Move mask to same device as model parameters\r\n        device = next(self.parameters()).device\r\n        return mask.to(device=device)\r\n\r\n    def _check_memory_usage(self) -> None:\r\n        \"\"\"Monitor GPU memory usage.\"\"\"\r\n        if torch.cuda.is_available():\r\n            current_mem = torch.cuda.memory_allocated() / 1024**2  # MB\r\n            peak_mem = torch.cuda.max_memory_allocated() / 1024**2  # MB\r\n            self.logger.debug(f\"Current GPU memory: {current_mem:.2f}MB, Peak: {peak_mem:.2f}MB\")\r\n\r\n    def get_performance_stats(self) -> Dict[str, float]:\r\n        \"\"\"Return performance statistics.\"\"\"\r\n        stats = self.perf_stats.copy()\r\n        if self.perf_stats['forward_pass_count'] > 0:\r\n            stats['avg_memory_per_forward'] = self.perf_stats['peak_memory'] / self.perf_stats['forward_pass_count']\r\n        return stats"
        }
    ]
}