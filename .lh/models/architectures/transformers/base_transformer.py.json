{
    "sourceFile": "models/architectures/transformers/base_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1733089622876,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1733089622876,
            "name": "Commit-0",
            "content": "from abc import ABC, abstractmethod\r\nfrom typing import Optional, Dict, Any, Tuple\r\nimport torch\r\nfrom torch import nn, Tensor\r\n\r\nfrom models.base.base_model import BaseModel\r\nfrom models.components.embeddings import CombinedEmbedding\r\nfrom models.components.attention import MultiHeadAttention\r\n\r\n\r\nclass BaseTransformer(BaseModel, ABC):\r\n    def __init__(self, config: Dict[str, Any]):\r\n        super().__init__(config)\r\n\r\n        self.d_model = config['d_model']\r\n        self.n_heads = config['n_heads']\r\n        self.n_encoder_layers = config['n_encoder_layers']\r\n        self.n_decoder_layers = config['n_decoder_layers']\r\n        self.d_ff = config['d_ff']\r\n        self.dropout = config['dropout']\r\n        self.input_features = config['input_features']\r\n        self.batch_first = config.get('batch_first', True)  # Default to batch_first=True\r\n\r\n        # Common components with proper initialization\r\n        self.encoder_embedding = CombinedEmbedding(\r\n            self.d_model,\r\n            self.input_features,\r\n            dropout=self.dropout\r\n        )\r\n        self.decoder_embedding = CombinedEmbedding(\r\n            self.d_model,\r\n            self.input_features,\r\n            dropout=self.dropout\r\n        )\r\n\r\n        # Initialize encoder and decoder layers\r\n        self.encoder_layers = self._create_encoder_layers()\r\n        self.decoder_layers = self._create_decoder_layers()\r\n\r\n        # Output projection with proper initialization\r\n        self.output_projection = nn.Linear(self.d_model, 1)\r\n        nn.init.xavier_uniform_(self.output_projection.weight)\r\n        nn.init.zeros_(self.output_projection.bias)\r\n\r\n    @abstractmethod\r\n    def _create_encoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create encoder layers specific to the transformer variant.\"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def _create_decoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create decoder layers specific to the transformer variant.\"\"\"\r\n        pass\r\n\r\n    def encode(\r\n        self,\r\n        src: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Encode input sequence.\"\"\"\r\n        # Apply embedding\r\n        src = self.encoder_embedding(src)\r\n        \r\n        # Pass through encoder layers\r\n        for layer in self.encoder_layers:\r\n            src = layer(\r\n                src,\r\n                src_mask=src_mask, \r\n                src_key_padding_mask=src_key_padding_mask\r\n            )\r\n        return src\r\n\r\n    def decode(\r\n        self,\r\n        tgt: Tensor,\r\n        memory: Tensor,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        memory_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n        memory_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Decode target sequence.\"\"\"\r\n        # Apply embedding\r\n        tgt = self.decoder_embedding(tgt)\r\n        \r\n        # Pass through decoder layers\r\n        for layer in self.decoder_layers:\r\n            tgt = layer(\r\n                tgt,\r\n                memory,\r\n                tgt_mask=tgt_mask,\r\n                memory_mask=memory_mask,\r\n                tgt_key_padding_mask=tgt_key_padding_mask,\r\n                memory_key_padding_mask=memory_key_padding_mask\r\n            )\r\n        return tgt\r\n\r\n    def forward(\r\n        self,\r\n        src: Tensor,\r\n        tgt: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Forward pass that returns only the final output.\"\"\"\r\n        # Encode source sequence\r\n        memory = self.encode(src, src_mask, src_key_padding_mask)\r\n        \r\n        # Decode target sequence\r\n        output = self.decode(\r\n            tgt,\r\n            memory,\r\n            tgt_mask=tgt_mask,\r\n            memory_mask=src_mask,\r\n            tgt_key_padding_mask=tgt_key_padding_mask,\r\n            memory_key_padding_mask=src_key_padding_mask\r\n        )\r\n        \r\n        # Project to output dimension\r\n        output = self.output_projection(output)\r\n        \r\n        return output\r\n\r\n    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n        \"\"\"Generate a square mask for the sequence. \r\n        The masked positions are filled with float('-inf'). Unmasked positions are filled with float(0.0).\r\n        \"\"\"\r\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n        return mask\r\n\r\n    def get_input_dims(self) -> int:\r\n        return self.input_features\r\n\r\n    def get_output_dims(self) -> int:\r\n        return 1"
        }
    ]
}