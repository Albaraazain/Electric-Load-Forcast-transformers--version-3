{
    "sourceFile": "models/architectures/transformers/conv_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1733090852317,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733091185507,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,15 +1,17 @@\n-from typing import Dict, Any, Optional\r\n+from typing import Dict, Any, Optional, Tuple\r\n import torch\r\n from torch import nn, Tensor\r\n+from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\r\n \r\n from models.registry.factory import ModelFactory\r\n from models.registry.model_types import ModelType\r\n from models.components.embeddings import TotalEmbedding\r\n-from models.components.attention import ConvolutionalMultiheadAttention\r\n+from models.components.attention import MultiHeadAttention\r\n+from models.base.base_model import BaseModel\r\n \r\n \r\n-class ConvEncoderLayer(nn.Module):\r\n+class ConvEncoderLayer(TransformerEncoderLayer):\r\n     \"\"\"Transformer encoder layer with convolutional attention.\"\"\"\r\n     \r\n     def __init__(\r\n         self, \r\n@@ -20,55 +22,56 @@\n         dropout: float = 0.1,\r\n         activation: str = \"relu\",\r\n         batch_first: bool = True\r\n     ):\r\n-        super().__init__()\r\n+        super().__init__(\r\n+            d_model=d_model,\r\n+            nhead=n_heads,\r\n+            dim_feedforward=d_ff,\r\n+            dropout=dropout,\r\n+            activation=activation,\r\n+            batch_first=batch_first\r\n+        )\r\n         \r\n-        # Convolutional self-attention\r\n-        self.self_attn = ConvolutionalMultiheadAttention(\r\n+        # Replace standard attention with convolutional attention\r\n+        self.self_attn = MultiHeadAttention(\r\n             embed_dim=d_model,\r\n             num_heads=n_heads,\r\n-            kernel_size=kernel_size,\r\n             dropout=dropout,\r\n             batch_first=batch_first\r\n         )\r\n         \r\n-        # Feed forward network\r\n-        self.feed_forward = nn.Sequential(\r\n-            nn.Linear(d_model, d_ff),\r\n-            nn.ReLU() if activation == \"relu\" else nn.GELU(),\r\n-            nn.Dropout(dropout),\r\n-            nn.Linear(d_ff, d_model)\r\n-        )\r\n-        \r\n-        # Layer normalization and dropout\r\n-        self.norm1 = nn.LayerNorm(d_model)\r\n-        self.norm2 = nn.LayerNorm(d_model)\r\n-        self.dropout1 = nn.Dropout(dropout)\r\n-        self.dropout2 = nn.Dropout(dropout)\r\n+        # Add convolutional layers\r\n+        padding = (kernel_size - 1) // 2\r\n+        self.conv_q = nn.Conv1d(d_model, d_model, kernel_size, padding=padding)\r\n+        self.conv_k = nn.Conv1d(d_model, d_model, kernel_size, padding=padding)\r\n \r\n     def forward(\r\n         self,\r\n         src: Tensor,\r\n         src_mask: Optional[Tensor] = None,\r\n         src_key_padding_mask: Optional[Tensor] = None\r\n     ) -> Tensor:\r\n-        # Self attention block\r\n-        src2, _ = self.self_attn(\r\n-            src, src, src,\r\n-            attn_mask=src_mask,\r\n-            key_padding_mask=src_key_padding_mask\r\n-        )\r\n-        src = self.norm1(src + self.dropout1(src2))\r\n+        # Apply convolutions to queries and keys\r\n+        q = self.conv_q(src.transpose(1, 2)).transpose(1, 2)\r\n+        k = self.conv_k(src.transpose(1, 2)).transpose(1, 2)\r\n         \r\n+        # Self attention block with convolutional features\r\n+        src2, _ = self.self_attn(q, k, src,\r\n+                              attn_mask=src_mask,\r\n+                              key_padding_mask=src_key_padding_mask)\r\n+        src = src + self.dropout1(src2)\r\n+        src = self.norm1(src)\r\n+        \r\n         # Feed forward block\r\n-        src2 = self.feed_forward(src)\r\n-        src = self.norm2(src + self.dropout2(src2))\r\n+        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\r\n+        src = src + self.dropout2(src2)\r\n+        src = self.norm2(src)\r\n         \r\n         return src\r\n \r\n \r\n-class ConvDecoderLayer(nn.Module):\r\n+class ConvDecoderLayer(TransformerDecoderLayer):\r\n     \"\"\"Transformer decoder layer with convolutional attention.\"\"\"\r\n     \r\n     def __init__(\r\n         self,\r\n@@ -79,41 +82,35 @@\n         dropout: float = 0.1,\r\n         activation: str = \"relu\",\r\n         batch_first: bool = True\r\n     ):\r\n-        super().__init__()\r\n+        super().__init__(\r\n+            d_model=d_model,\r\n+            nhead=n_heads,\r\n+            dim_feedforward=d_ff,\r\n+            dropout=dropout,\r\n+            activation=activation,\r\n+            batch_first=batch_first\r\n+        )\r\n         \r\n-        # Convolutional self and cross attention\r\n-        self.self_attn = ConvolutionalMultiheadAttention(\r\n+        # Replace standard attention with convolutional attention\r\n+        self.self_attn = MultiHeadAttention(\r\n             embed_dim=d_model,\r\n             num_heads=n_heads,\r\n-            kernel_size=kernel_size,\r\n             dropout=dropout,\r\n             batch_first=batch_first\r\n         )\r\n-        self.cross_attn = ConvolutionalMultiheadAttention(\r\n+        self.multihead_attn = MultiHeadAttention(\r\n             embed_dim=d_model,\r\n             num_heads=n_heads,\r\n-            kernel_size=kernel_size,\r\n             dropout=dropout,\r\n             batch_first=batch_first\r\n         )\r\n         \r\n-        # Feed forward network\r\n-        self.feed_forward = nn.Sequential(\r\n-            nn.Linear(d_model, d_ff),\r\n-            nn.ReLU() if activation == \"relu\" else nn.GELU(),\r\n-            nn.Dropout(dropout),\r\n-            nn.Linear(d_ff, d_model)\r\n-        )\r\n-        \r\n-        # Layer normalization and dropout\r\n-        self.norm1 = nn.LayerNorm(d_model)\r\n-        self.norm2 = nn.LayerNorm(d_model)\r\n-        self.norm3 = nn.LayerNorm(d_model)\r\n-        self.dropout1 = nn.Dropout(dropout)\r\n-        self.dropout2 = nn.Dropout(dropout)\r\n-        self.dropout3 = nn.Dropout(dropout)\r\n+        # Add convolutional layers\r\n+        padding = (kernel_size - 1) // 2\r\n+        self.conv_q = nn.Conv1d(d_model, d_model, kernel_size, padding=padding)\r\n+        self.conv_k = nn.Conv1d(d_model, d_model, kernel_size, padding=padding)\r\n \r\n     def forward(\r\n         self,\r\n         tgt: Tensor,\r\n@@ -122,37 +119,41 @@\n         memory_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n         memory_key_padding_mask: Optional[Tensor] = None\r\n     ) -> Tensor:\r\n-        # Self attention block\r\n-        tgt2, _ = self.self_attn(\r\n-            tgt, tgt, tgt,\r\n-            attn_mask=tgt_mask,\r\n-            key_padding_mask=tgt_key_padding_mask\r\n-        )\r\n-        tgt = self.norm1(tgt + self.dropout1(tgt2))\r\n+        # Apply convolutions\r\n+        q_tgt = self.conv_q(tgt.transpose(1, 2)).transpose(1, 2)\r\n+        k_tgt = self.conv_k(tgt.transpose(1, 2)).transpose(1, 2)\r\n+        q_mem = self.conv_q(memory.transpose(1, 2)).transpose(1, 2)\r\n         \r\n-        # Cross attention block\r\n-        tgt2, _ = self.cross_attn(\r\n-            tgt, memory, memory,\r\n-            attn_mask=memory_mask,\r\n-            key_padding_mask=memory_key_padding_mask\r\n-        )\r\n-        tgt = self.norm2(tgt + self.dropout2(tgt2))\r\n+        # Self attention\r\n+        tgt2, _ = self.self_attn(q_tgt, k_tgt, tgt,\r\n+                              attn_mask=tgt_mask,\r\n+                              key_padding_mask=tgt_key_padding_mask)\r\n+        tgt = tgt + self.dropout1(tgt2)\r\n+        tgt = self.norm1(tgt)\r\n         \r\n-        # Feed forward block\r\n-        tgt2 = self.feed_forward(tgt)\r\n-        tgt = self.norm3(tgt + self.dropout3(tgt2))\r\n+        # Cross attention\r\n+        tgt2, _ = self.multihead_attn(tgt, q_mem, memory,\r\n+                                   attn_mask=memory_mask,\r\n+                                   key_padding_mask=memory_key_padding_mask)\r\n+        tgt = tgt + self.dropout2(tgt2)\r\n+        tgt = self.norm2(tgt)\r\n         \r\n+        # Feed forward\r\n+        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\r\n+        tgt = tgt + self.dropout3(tgt2)\r\n+        tgt = self.norm3(tgt)\r\n+        \r\n         return tgt\r\n \r\n \r\n @ModelFactory.register(ModelType.CONV_TRANSFORMER)\r\n-class ConvolutionalTransformer(nn.Module):\r\n+class ConvolutionalTransformer(BaseModel):\r\n     \"\"\"Transformer with convolutional attention mechanism.\"\"\"\r\n \r\n     def __init__(self, config: Dict[str, Any]):\r\n-        super().__init__()\r\n+        super().__init__(config)  # Fix: Pass config to parent class\r\n         \r\n         # Extract configuration\r\n         self.d_model = config['d_model']\r\n         self.n_heads = config['n_heads']\r\n"
                }
            ],
            "date": 1733090852317,
            "name": "Commit-0",
            "content": "from typing import Dict, Any, Optional\r\nimport torch\r\nfrom torch import nn, Tensor\r\n\r\nfrom models.registry.factory import ModelFactory\r\nfrom models.registry.model_types import ModelType\r\nfrom models.components.embeddings import TotalEmbedding\r\nfrom models.components.attention import ConvolutionalMultiheadAttention\r\n\r\n\r\nclass ConvEncoderLayer(nn.Module):\r\n    \"\"\"Transformer encoder layer with convolutional attention.\"\"\"\r\n    \r\n    def __init__(\r\n        self, \r\n        d_model: int,\r\n        n_heads: int,\r\n        d_ff: int,\r\n        kernel_size: int,\r\n        dropout: float = 0.1,\r\n        activation: str = \"relu\",\r\n        batch_first: bool = True\r\n    ):\r\n        super().__init__()\r\n        \r\n        # Convolutional self-attention\r\n        self.self_attn = ConvolutionalMultiheadAttention(\r\n            embed_dim=d_model,\r\n            num_heads=n_heads,\r\n            kernel_size=kernel_size,\r\n            dropout=dropout,\r\n            batch_first=batch_first\r\n        )\r\n        \r\n        # Feed forward network\r\n        self.feed_forward = nn.Sequential(\r\n            nn.Linear(d_model, d_ff),\r\n            nn.ReLU() if activation == \"relu\" else nn.GELU(),\r\n            nn.Dropout(dropout),\r\n            nn.Linear(d_ff, d_model)\r\n        )\r\n        \r\n        # Layer normalization and dropout\r\n        self.norm1 = nn.LayerNorm(d_model)\r\n        self.norm2 = nn.LayerNorm(d_model)\r\n        self.dropout1 = nn.Dropout(dropout)\r\n        self.dropout2 = nn.Dropout(dropout)\r\n\r\n    def forward(\r\n        self,\r\n        src: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        # Self attention block\r\n        src2, _ = self.self_attn(\r\n            src, src, src,\r\n            attn_mask=src_mask,\r\n            key_padding_mask=src_key_padding_mask\r\n        )\r\n        src = self.norm1(src + self.dropout1(src2))\r\n        \r\n        # Feed forward block\r\n        src2 = self.feed_forward(src)\r\n        src = self.norm2(src + self.dropout2(src2))\r\n        \r\n        return src\r\n\r\n\r\nclass ConvDecoderLayer(nn.Module):\r\n    \"\"\"Transformer decoder layer with convolutional attention.\"\"\"\r\n    \r\n    def __init__(\r\n        self,\r\n        d_model: int,\r\n        n_heads: int,\r\n        d_ff: int,\r\n        kernel_size: int,\r\n        dropout: float = 0.1,\r\n        activation: str = \"relu\",\r\n        batch_first: bool = True\r\n    ):\r\n        super().__init__()\r\n        \r\n        # Convolutional self and cross attention\r\n        self.self_attn = ConvolutionalMultiheadAttention(\r\n            embed_dim=d_model,\r\n            num_heads=n_heads,\r\n            kernel_size=kernel_size,\r\n            dropout=dropout,\r\n            batch_first=batch_first\r\n        )\r\n        self.cross_attn = ConvolutionalMultiheadAttention(\r\n            embed_dim=d_model,\r\n            num_heads=n_heads,\r\n            kernel_size=kernel_size,\r\n            dropout=dropout,\r\n            batch_first=batch_first\r\n        )\r\n        \r\n        # Feed forward network\r\n        self.feed_forward = nn.Sequential(\r\n            nn.Linear(d_model, d_ff),\r\n            nn.ReLU() if activation == \"relu\" else nn.GELU(),\r\n            nn.Dropout(dropout),\r\n            nn.Linear(d_ff, d_model)\r\n        )\r\n        \r\n        # Layer normalization and dropout\r\n        self.norm1 = nn.LayerNorm(d_model)\r\n        self.norm2 = nn.LayerNorm(d_model)\r\n        self.norm3 = nn.LayerNorm(d_model)\r\n        self.dropout1 = nn.Dropout(dropout)\r\n        self.dropout2 = nn.Dropout(dropout)\r\n        self.dropout3 = nn.Dropout(dropout)\r\n\r\n    def forward(\r\n        self,\r\n        tgt: Tensor,\r\n        memory: Tensor,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        memory_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n        memory_key_padding_mask: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        # Self attention block\r\n        tgt2, _ = self.self_attn(\r\n            tgt, tgt, tgt,\r\n            attn_mask=tgt_mask,\r\n            key_padding_mask=tgt_key_padding_mask\r\n        )\r\n        tgt = self.norm1(tgt + self.dropout1(tgt2))\r\n        \r\n        # Cross attention block\r\n        tgt2, _ = self.cross_attn(\r\n            tgt, memory, memory,\r\n            attn_mask=memory_mask,\r\n            key_padding_mask=memory_key_padding_mask\r\n        )\r\n        tgt = self.norm2(tgt + self.dropout2(tgt2))\r\n        \r\n        # Feed forward block\r\n        tgt2 = self.feed_forward(tgt)\r\n        tgt = self.norm3(tgt + self.dropout3(tgt2))\r\n        \r\n        return tgt\r\n\r\n\r\n@ModelFactory.register(ModelType.CONV_TRANSFORMER)\r\nclass ConvolutionalTransformer(nn.Module):\r\n    \"\"\"Transformer with convolutional attention mechanism.\"\"\"\r\n\r\n    def __init__(self, config: Dict[str, Any]):\r\n        super().__init__()\r\n        \r\n        # Extract configuration\r\n        self.d_model = config['d_model']\r\n        self.n_heads = config['n_heads']\r\n        self.n_encoder_layers = config['n_encoder_layers']\r\n        self.n_decoder_layers = config['n_decoder_layers']\r\n        self.d_ff = config['d_ff']\r\n        self.dropout = config.get('dropout', 0.1)\r\n        self.input_features = config['input_features']\r\n        self.kernel_size = config.get('kernel_size', 3)\r\n        \r\n        # Create encoder\r\n        encoder_layer = ConvEncoderLayer(\r\n            d_model=self.d_model,\r\n            n_heads=self.n_heads,\r\n            d_ff=self.d_ff,\r\n            kernel_size=self.kernel_size,\r\n            dropout=self.dropout\r\n        )\r\n        encoder_norm = nn.LayerNorm(self.d_model)\r\n        self.encoder = nn.TransformerEncoder(encoder_layer, self.n_encoder_layers, encoder_norm)\r\n        \r\n        # Create decoder\r\n        decoder_layer = ConvDecoderLayer(\r\n            d_model=self.d_model,\r\n            n_heads=self.n_heads,\r\n            d_ff=self.d_ff,\r\n            kernel_size=self.kernel_size,\r\n            dropout=self.dropout\r\n        )\r\n        decoder_norm = nn.LayerNorm(self.d_model)\r\n        self.decoder = nn.TransformerDecoder(decoder_layer, self.n_decoder_layers, decoder_norm)\r\n\r\n        # Initialize embeddings\r\n        self.encoder_embedding = TotalEmbedding(\r\n            d_model=self.d_model,\r\n            value_features=1,\r\n            time_features=self.input_features - 1,\r\n            dropout=self.dropout\r\n        )\r\n        self.decoder_embedding = TotalEmbedding(\r\n            d_model=self.d_model,\r\n            value_features=1,\r\n            time_features=self.input_features - 1,\r\n            dropout=self.dropout\r\n        )\r\n\r\n        # Output projection\r\n        self.projection = nn.Linear(self.d_model, 1, bias=True)\r\n        self.relu = nn.ReLU()\r\n\r\n    def forward(\r\n        self,\r\n        x_enc: Tensor,\r\n        x_dec: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        \"\"\"\r\n        Forward pass with convolutional attention.\r\n\r\n        Args:\r\n            x_enc: Encoder input [batch_size, seq_enc_length, features]\r\n            x_dec: Decoder input [batch_size, seq_dec_length, features]\r\n            src_mask: Optional mask for encoder\r\n            tgt_mask: Optional mask for decoder\r\n        \"\"\"\r\n        # Apply embeddings\r\n        enc_embedding = self.encoder_embedding(x_enc)\r\n        dec_embedding = self.decoder_embedding(x_dec)\r\n\r\n        # Run through encoder and decoder\r\n        memory = self.encoder(enc_embedding, mask=src_mask)\r\n        output = self.decoder(dec_embedding, memory, tgt_mask=tgt_mask)\r\n\r\n        # Project to output dimension\r\n        return self.projection(self.relu(output))\r\n\r\n    def create_masks(self, src_len: int, tgt_len: int) -> Tuple[Optional[Tensor], Tensor]:\r\n        \"\"\"Create appropriate masks for training.\"\"\"\r\n        src_mask = None\r\n        tgt_mask = self.generate_square_subsequent_mask(tgt_len)\r\n        return src_mask, tgt_mask\r\n\r\n    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\r\n        \"\"\"Generate causal mask for decoder.\"\"\"\r\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n        return mask"
        }
    ]
}