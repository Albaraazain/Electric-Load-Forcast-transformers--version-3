{
    "sourceFile": "models/architectures/transformers/resolution_specific/base_resolution_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1733137615140,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733137868488,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,10 +2,11 @@\n from typing import Dict, Any, Optional\r\n import torch\r\n from torch import nn, Tensor\r\n \r\n+from data_loading.types.interval_types import TimeInterval\r\n+\r\n from ..base_transformer import BaseTransformer\r\n-from ....data_loading.base.interval_types import TimeInterval\r\n \r\n class BaseResolutionTransformer(BaseTransformer):\r\n     \"\"\"Base class for all resolution-specific transformer implementations.\"\"\"\r\n \r\n"
                },
                {
                    "date": 1733172736479,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -96,19 +96,17 @@\n             src_mask = self._get_positional_encoding(src.size(1))\r\n         if tgt_mask is None:\r\n             tgt_mask = self._get_positional_encoding(tgt.size(1))\r\n \r\n-        # Standard transformer forward pass\r\n-        output = super().forward(\r\n-            src, tgt,\r\n+        return super().forward(\r\n+            src,\r\n+            tgt,\r\n             src_mask=src_mask,\r\n             tgt_mask=tgt_mask,\r\n             src_key_padding_mask=src_key_padding_mask,\r\n-            tgt_key_padding_mask=tgt_key_padding_mask\r\n+            tgt_key_padding_mask=tgt_key_padding_mask,\r\n         )\r\n \r\n-        return output\r\n-\r\n     @classmethod\r\n     def get_resolution_type(cls) -> TimeInterval:\r\n         \"\"\"Get the time interval type this transformer is designed for.\"\"\"\r\n         raise NotImplementedError(\r\n"
                },
                {
                    "date": 1733238734561,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,20 +10,22 @@\n class BaseResolutionTransformer(BaseTransformer):\r\n     \"\"\"Base class for all resolution-specific transformer implementations.\"\"\"\r\n \r\n     def __init__(self, config: Dict[str, Any]):\r\n+        \"\"\"Initialize the resolution-aware transformer.\"\"\"\r\n+        # Call parent initialization first\r\n         super().__init__(config)\r\n         \r\n+        print(\"DEBUG: BaseResolutionTransformer initialization\")\r\n+        \r\n         # Resolution-specific configuration\r\n         self.input_resolution = config['input_resolution_minutes']\r\n         self.forecast_resolution = config['forecast_resolution_minutes']\r\n-        self.lookback_periods = config['lookback_periods']\r\n-        self.forecast_periods = config['forecast_periods']\r\n+        self.minutes_per_step = self.input_resolution\r\n         \r\n-        # Additional components for resolution handling\r\n+        # Initialize resampling if needed\r\n         self.resampling_needed = self.input_resolution != self.forecast_resolution\r\n         if self.resampling_needed:\r\n-            self.resampling_factor = self.forecast_resolution / self.input_resolution\r\n             self._setup_resampling_layer()\r\n \r\n     def _setup_resampling_layer(self) -> None:\r\n         \"\"\"Setup resampling layer if input and forecast resolutions differ.\"\"\"\r\n"
                }
            ],
            "date": 1733137615140,
            "name": "Commit-0",
            "content": "# models/architectures/transformers/resolution_specific/base_resolution_transformer.py\r\nfrom typing import Dict, Any, Optional\r\nimport torch\r\nfrom torch import nn, Tensor\r\n\r\nfrom ..base_transformer import BaseTransformer\r\nfrom ....data_loading.base.interval_types import TimeInterval\r\n\r\nclass BaseResolutionTransformer(BaseTransformer):\r\n    \"\"\"Base class for all resolution-specific transformer implementations.\"\"\"\r\n\r\n    def __init__(self, config: Dict[str, Any]):\r\n        super().__init__(config)\r\n        \r\n        # Resolution-specific configuration\r\n        self.input_resolution = config['input_resolution_minutes']\r\n        self.forecast_resolution = config['forecast_resolution_minutes']\r\n        self.lookback_periods = config['lookback_periods']\r\n        self.forecast_periods = config['forecast_periods']\r\n        \r\n        # Additional components for resolution handling\r\n        self.resampling_needed = self.input_resolution != self.forecast_resolution\r\n        if self.resampling_needed:\r\n            self.resampling_factor = self.forecast_resolution / self.input_resolution\r\n            self._setup_resampling_layer()\r\n\r\n    def _setup_resampling_layer(self) -> None:\r\n        \"\"\"Setup resampling layer if input and forecast resolutions differ.\"\"\"\r\n        if self.resampling_needed:\r\n            # If downsampling (e.g., 15min -> 1hour), use average pooling\r\n            if self.resampling_factor > 1:\r\n                pool_size = int(self.resampling_factor)\r\n                self.resampling_layer = nn.AvgPool1d(\r\n                    kernel_size=pool_size,\r\n                    stride=pool_size\r\n                )\r\n            # If upsampling (rare case), use linear interpolation\r\n            else:\r\n                self.resampling_layer = nn.Upsample(\r\n                    scale_factor=1/self.resampling_factor,\r\n                    mode='linear'\r\n                )\r\n\r\n    def _resample_if_needed(self, x: Tensor) -> Tensor:\r\n        \"\"\"Resample input data if resolutions differ.\"\"\"\r\n        if not self.resampling_needed:\r\n            return x\r\n            \r\n        # Reshape for resampling [batch, seq, features] -> [batch, features, seq]\r\n        x = x.transpose(1, 2)\r\n        \r\n        # Apply resampling\r\n        x = self.resampling_layer(x)\r\n        \r\n        # Reshape back [batch, features, seq] -> [batch, seq, features]\r\n        return x.transpose(1, 2)\r\n\r\n    def _get_positional_encoding(self, sequence_length: int) -> Tensor:\r\n        \"\"\"Get positional encoding appropriate for the resolution.\"\"\"\r\n        # Override in specific implementations if needed\r\n        return super().generate_square_subsequent_mask(sequence_length)\r\n\r\n    def _adjust_attention_for_resolution(\r\n        self,\r\n        attention_weights: Tensor,\r\n        resolution_minutes: int\r\n    ) -> Tensor:\r\n        \"\"\"Adjust attention weights based on resolution.\"\"\"\r\n        # For longer resolutions, we might want to enforce longer-term dependencies\r\n        if resolution_minutes >= 1440:  # daily or longer\r\n            # Enhance attention to seasonal patterns\r\n            attention_weights = attention_weights * (1 + torch.cos(\r\n                torch.arange(attention_weights.size(-1)) * 2 * torch.pi / (7 * 24 * 60 / resolution_minutes)\r\n            ).to(attention_weights.device))\r\n        \r\n        return attention_weights\r\n\r\n    def forward(\r\n        self,\r\n        src: Tensor,\r\n        tgt: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Forward pass with resolution handling.\"\"\"\r\n        # Resample input if needed\r\n        if self.resampling_needed:\r\n            src = self._resample_if_needed(src)\r\n            tgt = self._resample_if_needed(tgt)\r\n\r\n        # Get resolution-appropriate masks if none provided\r\n        if src_mask is None:\r\n            src_mask = self._get_positional_encoding(src.size(1))\r\n        if tgt_mask is None:\r\n            tgt_mask = self._get_positional_encoding(tgt.size(1))\r\n\r\n        # Standard transformer forward pass\r\n        output = super().forward(\r\n            src, tgt,\r\n            src_mask=src_mask,\r\n            tgt_mask=tgt_mask,\r\n            src_key_padding_mask=src_key_padding_mask,\r\n            tgt_key_padding_mask=tgt_key_padding_mask\r\n        )\r\n\r\n        return output\r\n\r\n    @classmethod\r\n    def get_resolution_type(cls) -> TimeInterval:\r\n        \"\"\"Get the time interval type this transformer is designed for.\"\"\"\r\n        raise NotImplementedError(\r\n            \"Resolution-specific transformers must implement get_resolution_type\"\r\n        )\r\n\r\n    def _validate_resolution(self) -> None:\r\n        \"\"\"Validate that the configured resolution matches the transformer type.\"\"\"\r\n        expected_type = self.get_resolution_type()\r\n        if self.forecast_resolution > expected_type.value:\r\n            raise ValueError(\r\n                f\"Forecast resolution ({self.forecast_resolution} minutes) exceeds \"\r\n                f\"maximum for {expected_type.name} transformer ({expected_type.value} minutes)\"\r\n            )"
        }
    ]
}