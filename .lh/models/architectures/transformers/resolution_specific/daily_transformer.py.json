{
    "sourceFile": "models/architectures/transformers/resolution_specific/daily_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1733171992084,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733172205151,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n import torch\r\n from torch import nn, Tensor\r\n import math\r\n \r\n-from ....data_loading.base.interval_types import TimeInterval\r\n+from data_loading.types.interval_types import TimeInterval\r\n from .base_resolution_transformer import BaseResolutionTransformer\r\n from ....registry.factory import ModelFactory\r\n from ....registry.model_types import ModelType\r\n from ....components.layers import EncoderLayer, DecoderLayer\r\n"
                },
                {
                    "date": 1733172431098,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,8 +2,10 @@\n from typing import Dict, Any, Optional\r\n import torch\r\n from torch import nn, Tensor\r\n import math\r\n+from datetime import datetime\r\n+import pandas as pd\r\n \r\n from data_loading.types.interval_types import TimeInterval\r\n from .base_resolution_transformer import BaseResolutionTransformer\r\n from ....registry.factory import ModelFactory\r\n@@ -130,19 +132,27 @@\n         ])\r\n \r\n     def _get_calendar_features(\r\n         self,\r\n-        timestamps: Tensor\r\n-    ) -> Tensor:\r\n+        timestamps: torch.Tensor  # This should be a tensor of unix timestamps\r\n+    ) -> torch.Tensor:\r\n         \"\"\"Extract and combine calendar features.\"\"\"\r\n-        weekday_emb = self.weekday_embedding(timestamps.dayofweek)\r\n-        month_emb = self.month_embedding(timestamps.month - 1)\r\n-        season_emb = self.season_embedding((timestamps.month - 1) // 3)\r\n+        # Convert tensor timestamps to pandas datetime\r\n+        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n         \r\n+        # Extract calendar features\r\n+        weekday = torch.tensor(dates.dayofweek.values, device=timestamps.device)\r\n+        month = torch.tensor(dates.month.values - 1, device=timestamps.device)\r\n+        season = torch.tensor((dates.month.values - 1) // 3, device=timestamps.device)\r\n+        \r\n         # Holiday features (placeholder - would need actual holiday data)\r\n-        is_holiday = torch.zeros_like(timestamps.dayofweek)\r\n-        is_special = torch.zeros_like(timestamps.dayofweek)\r\n+        is_holiday = torch.zeros_like(weekday)\r\n+        is_special = torch.zeros_like(weekday)\r\n         \r\n+        # Get embeddings\r\n+        weekday_emb = self.weekday_embedding(weekday)\r\n+        month_emb = self.month_embedding(month)\r\n+        season_emb = self.season_embedding(season)\r\n         holiday_emb = self.is_holiday_embedding(is_holiday)\r\n         special_emb = self.is_special_day_embedding(is_special)\r\n         \r\n         # Combine all calendar features\r\n"
                },
                {
                    "date": 1733179687100,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,11 +7,11 @@\n import pandas as pd\r\n \r\n from data_loading.types.interval_types import TimeInterval\r\n from .base_resolution_transformer import BaseResolutionTransformer\r\n-from ....registry.factory import ModelFactory\r\n-from ....registry.model_types import ModelType\r\n-from ....components.layers import EncoderLayer, DecoderLayer\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.components.layers import EncoderLayer, DecoderLayer\r\n \r\n @ModelFactory.register(ModelType.DAILY_TRANSFORMER)\r\n class DailyTransformer(BaseResolutionTransformer):\r\n     \"\"\"Transformer optimized for daily predictions.\"\"\"\r\n"
                },
                {
                    "date": 1733179872796,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,262 +1,38 @@\n # models/architectures/transformers/resolution_specific/daily_transformer.py\r\n from typing import Dict, Any, Optional\r\n import torch\r\n from torch import nn, Tensor\r\n-import math\r\n-from datetime import datetime\r\n import pandas as pd\r\n \r\n from data_loading.types.interval_types import TimeInterval\r\n from .base_resolution_transformer import BaseResolutionTransformer\r\n-from models.registry.factory import ModelFactory\r\n-from models.registry.model_types import ModelType\r\n from models.components.layers import EncoderLayer, DecoderLayer\r\n \r\n-@ModelFactory.register(ModelType.DAILY_TRANSFORMER)\r\n class DailyTransformer(BaseResolutionTransformer):\r\n     \"\"\"Transformer optimized for daily predictions.\"\"\"\r\n-\r\n+    \r\n     def __init__(self, config: Dict[str, Any]):\r\n-        # Validate resolution\r\n-        if not (1440 <= config.get('forecast_resolution_minutes', 1440) <= 2880):\r\n-            raise ValueError(\"DailyTransformer is designed for daily resolutions (1440-2880 minutes)\")\r\n-        \r\n+        # Validate resolution before initialization\r\n+        if config.get('forecast_resolution_minutes', 1440) > 2880:\r\n+            raise ValueError(\"DailyTransformer cannot handle resolutions > 2 days\")\r\n+            \r\n         super().__init__(config)\r\n         \r\n         # Daily-specific configurations\r\n-        self.weekly_pattern_features = self.d_model // 2\r\n-        self.seasonal_pattern_features = self.d_model // 4\r\n+        self.minutes_per_step = config.get('forecast_resolution_minutes', 1440)\r\n         \r\n-        # Calendar components\r\n-        self._setup_calendar_components()\r\n-        \r\n-        # Pattern recognition components\r\n-        self._setup_pattern_components()\r\n-        \r\n-        # Long-term dependencies\r\n-        self._setup_long_term_components()\r\n-\r\n-    def _setup_calendar_components(self) -> None:\r\n-        \"\"\"Setup calendar-aware components.\"\"\"\r\n-        # Calendar embeddings\r\n-        self.weekday_embedding = nn.Embedding(7, self.d_model)\r\n-        self.month_embedding = nn.Embedding(12, self.d_model)\r\n-        self.season_embedding = nn.Embedding(4, self.d_model)\r\n-        \r\n-        # Holiday and special day handling\r\n-        self.is_holiday_embedding = nn.Embedding(2, self.d_model)\r\n-        self.is_special_day_embedding = nn.Embedding(2, self.d_model)\r\n-        \r\n-        # Calendar fusion layer\r\n-        self.calendar_fusion = nn.Sequential(\r\n-            nn.Linear(self.d_model * 5, self.d_model),\r\n-            nn.LayerNorm(self.d_model),\r\n-            nn.ReLU(),\r\n-            nn.Dropout(self.dropout)\r\n+        # Enhanced pattern recognition for daily data\r\n+        self.daily_pattern_conv = nn.Conv1d(\r\n+            in_channels=self.d_model,\r\n+            out_channels=self.d_model,\r\n+            kernel_size=7,  # Capture weekly patterns\r\n+            padding=3,\r\n+            groups=self.d_model\r\n         )\r\n \r\n-    def _setup_pattern_components(self) -> None:\r\n-        \"\"\"Setup pattern recognition components.\"\"\"\r\n-        # Weekly pattern recognition\r\n-        self.weekly_pattern_layer = nn.Sequential(\r\n-            nn.Linear(self.d_model, self.weekly_pattern_features),\r\n-            nn.ReLU(),\r\n-            nn.Linear(self.weekly_pattern_features, self.d_model),\r\n-            nn.Dropout(self.dropout)\r\n-        )\r\n-        \r\n-        # Seasonal pattern recognition\r\n-        self.seasonal_pattern_layer = nn.Sequential(\r\n-            nn.Linear(self.d_model, self.seasonal_pattern_features),\r\n-            nn.ReLU(),\r\n-            nn.Linear(self.seasonal_pattern_features, self.d_model),\r\n-            nn.Dropout(self.dropout)\r\n-        )\r\n-        \r\n-        # Pattern attention\r\n-        self.pattern_attention = nn.MultiheadAttention(\r\n-            embed_dim=self.d_model,\r\n-            num_heads=self.n_heads,\r\n-            dropout=self.dropout,\r\n-            batch_first=True\r\n-        )\r\n-\r\n-    def _setup_long_term_components(self) -> None:\r\n-        \"\"\"Setup components for handling long-term dependencies.\"\"\"\r\n-        # Long-term memory\r\n-        self.long_term_memory_size = 28  # 4 weeks\r\n-        self.long_term_memory = nn.Parameter(\r\n-            torch.randn(1, self.long_term_memory_size, self.d_model)\r\n-        )\r\n-        \r\n-        # Long-term attention\r\n-        self.long_term_attention = nn.MultiheadAttention(\r\n-            embed_dim=self.d_model,\r\n-            num_heads=self.n_heads,\r\n-            dropout=self.dropout,\r\n-            batch_first=True\r\n-        )\r\n-        \r\n-        # Trend analysis\r\n-        self.trend_layer = nn.GRU(\r\n-            input_size=self.d_model,\r\n-            hidden_size=self.d_model,\r\n-            num_layers=1,\r\n-            batch_first=True\r\n-        )\r\n-\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create encoder layers optimized for daily patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            EncoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"gelu\"\r\n-            ) for _ in range(self.n_encoder_layers)\r\n-        ])\r\n-\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create decoder layers optimized for daily patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            DecoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"gelu\"\r\n-            ) for _ in range(self.n_decoder_layers)\r\n-        ])\r\n-\r\n-    def _get_calendar_features(\r\n-        self,\r\n-        timestamps: torch.Tensor  # This should be a tensor of unix timestamps\r\n-    ) -> torch.Tensor:\r\n-        \"\"\"Extract and combine calendar features.\"\"\"\r\n-        # Convert tensor timestamps to pandas datetime\r\n-        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n-        \r\n-        # Extract calendar features\r\n-        weekday = torch.tensor(dates.dayofweek.values, device=timestamps.device)\r\n-        month = torch.tensor(dates.month.values - 1, device=timestamps.device)\r\n-        season = torch.tensor((dates.month.values - 1) // 3, device=timestamps.device)\r\n-        \r\n-        # Holiday features (placeholder - would need actual holiday data)\r\n-        is_holiday = torch.zeros_like(weekday)\r\n-        is_special = torch.zeros_like(weekday)\r\n-        \r\n-        # Get embeddings\r\n-        weekday_emb = self.weekday_embedding(weekday)\r\n-        month_emb = self.month_embedding(month)\r\n-        season_emb = self.season_embedding(season)\r\n-        holiday_emb = self.is_holiday_embedding(is_holiday)\r\n-        special_emb = self.is_special_day_embedding(is_special)\r\n-        \r\n-        # Combine all calendar features\r\n-        calendar_features = torch.cat([\r\n-            weekday_emb, month_emb, season_emb,\r\n-            holiday_emb, special_emb\r\n-        ], dim=-1)\r\n-        \r\n-        return self.calendar_fusion(calendar_features)\r\n-\r\n-    def encode(\r\n-        self,\r\n-        src: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        timestamps: Optional[Tensor] = None\r\n-    ) -> Tensor:\r\n-        \"\"\"Enhanced encoding with daily pattern recognition.\"\"\"\r\n-        # Standard embedding\r\n-        src = self.encoder_embedding(src)\r\n-        \r\n-        if timestamps is not None:\r\n-            # Add calendar features\r\n-            calendar_features = self._get_calendar_features(timestamps)\r\n-            src = src + calendar_features\r\n-            \r\n-            # Extract and enhance patterns\r\n-            weekly_patterns = self.weekly_pattern_layer(src)\r\n-            seasonal_patterns = self.seasonal_pattern_layer(src)\r\n-            \r\n-            src = src + weekly_patterns + seasonal_patterns\r\n-        \r\n-        # Apply pattern attention\r\n-        pattern_attn_out, _ = self.pattern_attention(src, src, src)\r\n-        src = src + pattern_attn_out\r\n-        \r\n-        # Apply long-term attention\r\n-        batch_size = src.size(0)\r\n-        long_term_memory = self.long_term_memory.expand(batch_size, -1, -1)\r\n-        long_term_out, _ = self.long_term_attention(src, long_term_memory, long_term_memory)\r\n-        src = src + long_term_out\r\n-        \r\n-        # Extract trend information\r\n-        trend_out, _ = self.trend_layer(src)\r\n-        src = src + trend_out\r\n-        \r\n-        # Pass through encoder layers\r\n-        for layer in self.encoder_layers:\r\n-            src = layer(\r\n-                src,\r\n-                src_mask=src_mask,\r\n-                src_key_padding_mask=src_key_padding_mask\r\n-            )\r\n-        return src\r\n-\r\n-    def _adjust_attention_for_resolution(\r\n-        self,\r\n-        attention_weights: Tensor,\r\n-        resolution_minutes: int\r\n-    ) -> Tensor:\r\n-        \"\"\"Adjust attention weights for daily patterns.\"\"\"\r\n-        seq_len = attention_weights.size(-1)\r\n-        \r\n-        # Weekly pattern emphasis\r\n-        days = torch.arange(seq_len, device=attention_weights.device) % 7\r\n-        week_weights = torch.cos(2 * math.pi * days / 7)\r\n-        attention_weights = attention_weights * (1 + 0.2 * week_weights.unsqueeze(0).unsqueeze(0))\r\n-        \r\n-        # Enhanced weekend attention\r\n-        is_weekend = (days >= 5).float()\r\n-        attention_weights = attention_weights * (1 + 0.1 * is_weekend.unsqueeze(0).unsqueeze(0))\r\n-        \r\n-        return attention_weights\r\n-\r\n     @classmethod\r\n     def get_resolution_type(cls) -> TimeInterval:\r\n         \"\"\"Get the time interval type for daily transformer.\"\"\"\r\n         return TimeInterval.DAILY\r\n \r\n-    def forward(\r\n-        self,\r\n-        src: Tensor,\r\n-        tgt: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-        timestamps: Optional[Tensor] = None\r\n-    ) -> Tensor:\r\n-        \"\"\"Forward pass with enhanced daily pattern recognition.\"\"\"\r\n-        memory = self.encode(\r\n-            src,\r\n-            src_mask=src_mask,\r\n-            src_key_padding_mask=src_key_padding_mask,\r\n\\ No newline at end of file\n-            timestamps=timestamps\r\n-        )\r\n-        \r\n-        output = self.decode(\r\n-            tgt,\r\n-            memory,\r\n-            tgt_mask=tgt_mask,\r\n-            memory_mask=None,\r\n-            tgt_key_padding_mask=tgt_key_padding_mask,\r\n-            memory_key_padding_mask=src_key_padding_mask\r\n-        )\r\n-        \r\n-        return self.output_projection(output)\n+    # ...rest of implementation similar to SubhourlyTransformer...\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733171992084,
            "name": "Commit-0",
            "content": "# models/architectures/transformers/resolution_specific/daily_transformer.py\r\nfrom typing import Dict, Any, Optional\r\nimport torch\r\nfrom torch import nn, Tensor\r\nimport math\r\n\r\nfrom ....data_loading.base.interval_types import TimeInterval\r\nfrom .base_resolution_transformer import BaseResolutionTransformer\r\nfrom ....registry.factory import ModelFactory\r\nfrom ....registry.model_types import ModelType\r\nfrom ....components.layers import EncoderLayer, DecoderLayer\r\n\r\n@ModelFactory.register(ModelType.DAILY_TRANSFORMER)\r\nclass DailyTransformer(BaseResolutionTransformer):\r\n    \"\"\"Transformer optimized for daily predictions.\"\"\"\r\n\r\n    def __init__(self, config: Dict[str, Any]):\r\n        # Validate resolution\r\n        if not (1440 <= config.get('forecast_resolution_minutes', 1440) <= 2880):\r\n            raise ValueError(\"DailyTransformer is designed for daily resolutions (1440-2880 minutes)\")\r\n        \r\n        super().__init__(config)\r\n        \r\n        # Daily-specific configurations\r\n        self.weekly_pattern_features = self.d_model // 2\r\n        self.seasonal_pattern_features = self.d_model // 4\r\n        \r\n        # Calendar components\r\n        self._setup_calendar_components()\r\n        \r\n        # Pattern recognition components\r\n        self._setup_pattern_components()\r\n        \r\n        # Long-term dependencies\r\n        self._setup_long_term_components()\r\n\r\n    def _setup_calendar_components(self) -> None:\r\n        \"\"\"Setup calendar-aware components.\"\"\"\r\n        # Calendar embeddings\r\n        self.weekday_embedding = nn.Embedding(7, self.d_model)\r\n        self.month_embedding = nn.Embedding(12, self.d_model)\r\n        self.season_embedding = nn.Embedding(4, self.d_model)\r\n        \r\n        # Holiday and special day handling\r\n        self.is_holiday_embedding = nn.Embedding(2, self.d_model)\r\n        self.is_special_day_embedding = nn.Embedding(2, self.d_model)\r\n        \r\n        # Calendar fusion layer\r\n        self.calendar_fusion = nn.Sequential(\r\n            nn.Linear(self.d_model * 5, self.d_model),\r\n            nn.LayerNorm(self.d_model),\r\n            nn.ReLU(),\r\n            nn.Dropout(self.dropout)\r\n        )\r\n\r\n    def _setup_pattern_components(self) -> None:\r\n        \"\"\"Setup pattern recognition components.\"\"\"\r\n        # Weekly pattern recognition\r\n        self.weekly_pattern_layer = nn.Sequential(\r\n            nn.Linear(self.d_model, self.weekly_pattern_features),\r\n            nn.ReLU(),\r\n            nn.Linear(self.weekly_pattern_features, self.d_model),\r\n            nn.Dropout(self.dropout)\r\n        )\r\n        \r\n        # Seasonal pattern recognition\r\n        self.seasonal_pattern_layer = nn.Sequential(\r\n            nn.Linear(self.d_model, self.seasonal_pattern_features),\r\n            nn.ReLU(),\r\n            nn.Linear(self.seasonal_pattern_features, self.d_model),\r\n            nn.Dropout(self.dropout)\r\n        )\r\n        \r\n        # Pattern attention\r\n        self.pattern_attention = nn.MultiheadAttention(\r\n            embed_dim=self.d_model,\r\n            num_heads=self.n_heads,\r\n            dropout=self.dropout,\r\n            batch_first=True\r\n        )\r\n\r\n    def _setup_long_term_components(self) -> None:\r\n        \"\"\"Setup components for handling long-term dependencies.\"\"\"\r\n        # Long-term memory\r\n        self.long_term_memory_size = 28  # 4 weeks\r\n        self.long_term_memory = nn.Parameter(\r\n            torch.randn(1, self.long_term_memory_size, self.d_model)\r\n        )\r\n        \r\n        # Long-term attention\r\n        self.long_term_attention = nn.MultiheadAttention(\r\n            embed_dim=self.d_model,\r\n            num_heads=self.n_heads,\r\n            dropout=self.dropout,\r\n            batch_first=True\r\n        )\r\n        \r\n        # Trend analysis\r\n        self.trend_layer = nn.GRU(\r\n            input_size=self.d_model,\r\n            hidden_size=self.d_model,\r\n            num_layers=1,\r\n            batch_first=True\r\n        )\r\n\r\n    def _create_encoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create encoder layers optimized for daily patterns.\"\"\"\r\n        return nn.ModuleList([\r\n            EncoderLayer(\r\n                d_model=self.d_model,\r\n                n_heads=self.n_heads,\r\n                d_ff=self.d_ff,\r\n                dropout=self.dropout,\r\n                attention_type=\"standard\",\r\n                activation=\"gelu\"\r\n            ) for _ in range(self.n_encoder_layers)\r\n        ])\r\n\r\n    def _create_decoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create decoder layers optimized for daily patterns.\"\"\"\r\n        return nn.ModuleList([\r\n            DecoderLayer(\r\n                d_model=self.d_model,\r\n                n_heads=self.n_heads,\r\n                d_ff=self.d_ff,\r\n                dropout=self.dropout,\r\n                attention_type=\"standard\",\r\n                activation=\"gelu\"\r\n            ) for _ in range(self.n_decoder_layers)\r\n        ])\r\n\r\n    def _get_calendar_features(\r\n        self,\r\n        timestamps: Tensor\r\n    ) -> Tensor:\r\n        \"\"\"Extract and combine calendar features.\"\"\"\r\n        weekday_emb = self.weekday_embedding(timestamps.dayofweek)\r\n        month_emb = self.month_embedding(timestamps.month - 1)\r\n        season_emb = self.season_embedding((timestamps.month - 1) // 3)\r\n        \r\n        # Holiday features (placeholder - would need actual holiday data)\r\n        is_holiday = torch.zeros_like(timestamps.dayofweek)\r\n        is_special = torch.zeros_like(timestamps.dayofweek)\r\n        \r\n        holiday_emb = self.is_holiday_embedding(is_holiday)\r\n        special_emb = self.is_special_day_embedding(is_special)\r\n        \r\n        # Combine all calendar features\r\n        calendar_features = torch.cat([\r\n            weekday_emb, month_emb, season_emb,\r\n            holiday_emb, special_emb\r\n        ], dim=-1)\r\n        \r\n        return self.calendar_fusion(calendar_features)\r\n\r\n    def encode(\r\n        self,\r\n        src: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        timestamps: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        \"\"\"Enhanced encoding with daily pattern recognition.\"\"\"\r\n        # Standard embedding\r\n        src = self.encoder_embedding(src)\r\n        \r\n        if timestamps is not None:\r\n            # Add calendar features\r\n            calendar_features = self._get_calendar_features(timestamps)\r\n            src = src + calendar_features\r\n            \r\n            # Extract and enhance patterns\r\n            weekly_patterns = self.weekly_pattern_layer(src)\r\n            seasonal_patterns = self.seasonal_pattern_layer(src)\r\n            \r\n            src = src + weekly_patterns + seasonal_patterns\r\n        \r\n        # Apply pattern attention\r\n        pattern_attn_out, _ = self.pattern_attention(src, src, src)\r\n        src = src + pattern_attn_out\r\n        \r\n        # Apply long-term attention\r\n        batch_size = src.size(0)\r\n        long_term_memory = self.long_term_memory.expand(batch_size, -1, -1)\r\n        long_term_out, _ = self.long_term_attention(src, long_term_memory, long_term_memory)\r\n        src = src + long_term_out\r\n        \r\n        # Extract trend information\r\n        trend_out, _ = self.trend_layer(src)\r\n        src = src + trend_out\r\n        \r\n        # Pass through encoder layers\r\n        for layer in self.encoder_layers:\r\n            src = layer(\r\n                src,\r\n                src_mask=src_mask,\r\n                src_key_padding_mask=src_key_padding_mask\r\n            )\r\n        return src\r\n\r\n    def _adjust_attention_for_resolution(\r\n        self,\r\n        attention_weights: Tensor,\r\n        resolution_minutes: int\r\n    ) -> Tensor:\r\n        \"\"\"Adjust attention weights for daily patterns.\"\"\"\r\n        seq_len = attention_weights.size(-1)\r\n        \r\n        # Weekly pattern emphasis\r\n        days = torch.arange(seq_len, device=attention_weights.device) % 7\r\n        week_weights = torch.cos(2 * math.pi * days / 7)\r\n        attention_weights = attention_weights * (1 + 0.2 * week_weights.unsqueeze(0).unsqueeze(0))\r\n        \r\n        # Enhanced weekend attention\r\n        is_weekend = (days >= 5).float()\r\n        attention_weights = attention_weights * (1 + 0.1 * is_weekend.unsqueeze(0).unsqueeze(0))\r\n        \r\n        return attention_weights\r\n\r\n    @classmethod\r\n    def get_resolution_type(cls) -> TimeInterval:\r\n        \"\"\"Get the time interval type for daily transformer.\"\"\"\r\n        return TimeInterval.DAILY\r\n\r\n    def forward(\r\n        self,\r\n        src: Tensor,\r\n        tgt: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n        timestamps: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        \"\"\"Forward pass with enhanced daily pattern recognition.\"\"\"\r\n        memory = self.encode(\r\n            src,\r\n            src_mask=src_mask,\r\n            src_key_padding_mask=src_key_padding_mask,\r\n            timestamps=timestamps\r\n        )\r\n        \r\n        output = self.decode(\r\n            tgt,\r\n            memory,\r\n            tgt_mask=tgt_mask,\r\n            memory_mask=None,\r\n            tgt_key_padding_mask=tgt_key_padding_mask,\r\n            memory_key_padding_mask=src_key_padding_mask\r\n        )\r\n        \r\n        return self.output_projection(output)"
        }
    ]
}