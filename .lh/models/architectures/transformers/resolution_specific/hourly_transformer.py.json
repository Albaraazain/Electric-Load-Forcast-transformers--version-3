{
    "sourceFile": "models/architectures/transformers/resolution_specific/hourly_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1733171896331,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733171906939,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,10 @@\n import torch\r\n from torch import nn, Tensor\r\n import math\r\n \r\n-from ....data_loading.base.interval_types import TimeInterval\r\n+from data_loading.types.interval_types import TimeInterval\r\n+\r\n from .base_resolution_transformer import BaseResolutionTransformer\r\n from ....registry.factory import ModelFactory\r\n from ....registry.model_types import ModelType\r\n from ....components.layers import EncoderLayer, DecoderLayer\r\n"
                },
                {
                    "date": 1733172736435,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,12 +1,12 @@\n-# models/architectures/transformers/resolution_specific/hourly_transformer.py\r\n from typing import Dict, Any, Optional\r\n+from datetime import datetime\r\n+import pandas as pd\r\n import torch\r\n from torch import nn, Tensor\r\n import math\r\n \r\n from data_loading.types.interval_types import TimeInterval\r\n-\r\n from .base_resolution_transformer import BaseResolutionTransformer\r\n from ....registry.factory import ModelFactory\r\n from ....registry.model_types import ModelType\r\n from ....components.layers import EncoderLayer, DecoderLayer\r\n@@ -80,15 +80,18 @@\n                 activation=\"gelu\"\r\n             ) for _ in range(self.n_decoder_layers)\r\n         ])\r\n \r\n-    def _add_temporal_embeddings(self, x: Tensor, timestamps: Tensor) -> Tensor:\r\n+    def _add_temporal_embeddings(self, x: Tensor, timestamps: torch.Tensor) -> Tensor:\r\n         \"\"\"Add temporal embeddings to input tensor.\"\"\"\r\n         batch_size, seq_len = x.shape[:2]\r\n         \r\n+        # Convert tensor timestamps to pandas datetime\r\n+        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n+        \r\n         # Extract hour and day information\r\n-        hours = timestamps.hour\r\n-        days = timestamps.dayofweek\r\n+        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n+        days = torch.tensor(dates.dayofweek.values, device=timestamps.device)\r\n         \r\n         # Get embeddings\r\n         hour_emb = self.hour_embedding(hours)\r\n         day_emb = self.workday_embedding(days)\r\n"
                },
                {
                    "date": 1733179726770,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,12 +5,11 @@\n from torch import nn, Tensor\r\n import math\r\n \r\n from data_loading.types.interval_types import TimeInterval\r\n-from .base_resolution_transformer import BaseResolutionTransformer\r\n-from ....registry.factory import ModelFactory\r\n-from ....registry.model_types import ModelType\r\n-from ....components.layers import EncoderLayer, DecoderLayer\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.components.layers import EncoderLayer, DecoderLayer\r\n \r\n @ModelFactory.register(ModelType.HOURLY_TRANSFORMER)\r\n class HourlyTransformer(BaseResolutionTransformer):\r\n     \"\"\"Transformer optimized for hourly predictions.\"\"\"\r\n"
                },
                {
                    "date": 1733179868782,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,191 +1,37 @@\n from typing import Dict, Any, Optional\r\n-from datetime import datetime\r\n-import pandas as pd\r\n import torch\r\n from torch import nn, Tensor\r\n-import math\r\n+import pandas as pd\r\n \r\n from data_loading.types.interval_types import TimeInterval\r\n-from models.registry.factory import ModelFactory\r\n-from models.registry.model_types import ModelType\r\n+from .base_resolution_transformer import BaseResolutionTransformer\r\n from models.components.layers import EncoderLayer, DecoderLayer\r\n \r\n-@ModelFactory.register(ModelType.HOURLY_TRANSFORMER)\r\n class HourlyTransformer(BaseResolutionTransformer):\r\n     \"\"\"Transformer optimized for hourly predictions.\"\"\"\r\n-\r\n+    \r\n     def __init__(self, config: Dict[str, Any]):\r\n-        # Validate resolution\r\n-        if not (60 <= config.get('forecast_resolution_minutes', 60) <= 180):\r\n-            raise ValueError(\"HourlyTransformer is designed for 60-180 minute resolutions\")\r\n-        \r\n+        # Validate resolution before initialization\r\n+        if config.get('forecast_resolution_minutes', 60) > 180:\r\n+            raise ValueError(\"HourlyTransformer cannot handle resolutions > 180 minutes\")\r\n+            \r\n         super().__init__(config)\r\n         \r\n         # Hourly-specific configurations\r\n-        self.hours_per_day = 24\r\n-        self.daily_pattern_features = self.d_model // 4  # Dedicated features for daily patterns\r\n+        self.minutes_per_step = config.get('forecast_resolution_minutes', 60)\r\n         \r\n-        # Daily pattern recognition component\r\n-        self.daily_pattern_layer = nn.Sequential(\r\n-            nn.Linear(self.d_model, self.daily_pattern_features),\r\n-            nn.ReLU(),\r\n-            nn.Linear(self.daily_pattern_features, self.d_model),\r\n-            nn.Dropout(self.dropout)\r\n+        # Enhanced pattern recognition for hourly data\r\n+        self.hourly_pattern_conv = nn.Conv1d(\r\n+            in_channels=self.d_model,\r\n+            out_channels=self.d_model,\r\n+            kernel_size=4,  # Capture 4-hour patterns\r\n+            padding=2,\r\n+            groups=self.d_model\r\n         )\r\n-        \r\n-        # Hour-of-day embedding\r\n-        self.hour_embedding = nn.Embedding(24, self.d_model)\r\n-        \r\n-        # Additional time-aware components\r\n-        self._setup_temporal_components()\r\n \r\n-    def _setup_temporal_components(self) -> None:\r\n-        \"\"\"Setup components for temporal pattern recognition.\"\"\"\r\n-        self.workday_embedding = nn.Embedding(7, self.d_model)  # Day of week\r\n-        \r\n-        # Hour-aware attention scaling\r\n-        self.hour_importance = nn.Parameter(torch.ones(24))\r\n-        \r\n-        # Daily cycle attention\r\n-        self.daily_cycle_attention = nn.MultiheadAttention(\r\n-            embed_dim=self.d_model,\r\n-            num_heads=self.n_heads,\r\n-            dropout=self.dropout,\r\n-            batch_first=True\r\n-        )\r\n-\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create encoder layers optimized for hourly patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            EncoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"gelu\"\r\n-            ) for _ in range(self.n_encoder_layers)\r\n-        ])\r\n-\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create decoder layers optimized for hourly patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            DecoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"gelu\"\r\n-            ) for _ in range(self.n_decoder_layers)\r\n-        ])\r\n-\r\n-    def _add_temporal_embeddings(self, x: Tensor, timestamps: torch.Tensor) -> Tensor:\r\n-        \"\"\"Add temporal embeddings to input tensor.\"\"\"\r\n-        batch_size, seq_len = x.shape[:2]\r\n-        \r\n-        # Convert tensor timestamps to pandas datetime\r\n-        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n-        \r\n-        # Extract hour and day information\r\n-        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n-        days = torch.tensor(dates.dayofweek.values, device=timestamps.device)\r\n-        \r\n-        # Get embeddings\r\n-        hour_emb = self.hour_embedding(hours)\r\n-        day_emb = self.workday_embedding(days)\r\n-        \r\n-        # Combine embeddings\r\n-        temporal_embedding = hour_emb + day_emb\r\n-        \r\n-        return x + temporal_embedding\r\n-\r\n-    def encode(\r\n-        self,\r\n-        src: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        timestamps: Optional[Tensor] = None\r\n-    ) -> Tensor:\r\n-        \"\"\"Enhanced encoding with hourly pattern recognition.\"\"\"\r\n-        # Standard embedding\r\n-        src = self.encoder_embedding(src)\r\n-        \r\n-        if timestamps is not None:\r\n-            src = self._add_temporal_embeddings(src, timestamps)\r\n-        \r\n-        # Extract and enhance daily patterns\r\n-        daily_patterns = self.daily_pattern_layer(src)\r\n-        src = src + daily_patterns\r\n-        \r\n-        # Apply daily cycle attention\r\n-        daily_attn_out, _ = self.daily_cycle_attention(src, src, src)\r\n-        src = src + daily_attn_out\r\n-        \r\n-        # Pass through encoder layers\r\n-        for layer in self.encoder_layers:\r\n-            src = layer(\r\n-                src,\r\n-                src_mask=src_mask,\r\n-                src_key_padding_mask=src_key_padding_mask\r\n-            )\r\n-        return src\r\n-\r\n-    def _adjust_attention_for_resolution(\r\n-        self,\r\n-        attention_weights: Tensor,\r\n-        resolution_minutes: int\r\n-    ) -> Tensor:\r\n-        \"\"\"Adjust attention weights for hourly patterns.\"\"\"\r\n-        # Get sequence length\r\n-        seq_len = attention_weights.size(-1)\r\n-        \r\n-        # Create hour-based importance weights\r\n-        hours = torch.arange(seq_len, device=attention_weights.device) % 24\r\n-        hour_weights = self.hour_importance[hours]\r\n-        \r\n-        # Apply hour importance to attention weights\r\n-        attention_weights = attention_weights * hour_weights.unsqueeze(0).unsqueeze(0)\r\n-        \r\n-        # Add daily pattern emphasis\r\n-        daily_pattern = torch.cos(2 * math.pi * hours / 24)\r\n-        attention_weights = attention_weights * (1 + 0.1 * daily_pattern.unsqueeze(0).unsqueeze(0))\r\n-        \r\n-        return attention_weights\r\n-\r\n     @classmethod\r\n     def get_resolution_type(cls) -> TimeInterval:\r\n         \"\"\"Get the time interval type for hourly transformer.\"\"\"\r\n         return TimeInterval.HOURLY\r\n \r\n-    def forward(\r\n-        self,\r\n-        src: Tensor,\r\n-        tgt: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-        timestamps: Optional[Tensor] = None\r\n-    ) -> Tensor:\r\n-        \"\"\"Forward pass with enhanced hourly pattern recognition.\"\"\"\r\n-        # Encode with temporal information\r\n-        memory = self.encode(\r\n-            src,\r\n-            src_mask=src_mask,\r\n-            src_key_padding_mask=src_key_padding_mask,\r\n\\ No newline at end of file\n-            timestamps=timestamps\r\n-        )\r\n-        \r\n-        # Decode with temporal awareness\r\n-        output = self.decode(\r\n-            tgt,\r\n-            memory,\r\n-            tgt_mask=tgt_mask,\r\n-            memory_mask=None,\r\n-            tgt_key_padding_mask=tgt_key_padding_mask,\r\n-            memory_key_padding_mask=src_key_padding_mask\r\n-        )\r\n-        \r\n-        # Final projection\r\n-        return self.output_projection(output)\n+    # ...rest of implementation similar to SubhourlyTransformer...\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733171896331,
            "name": "Commit-0",
            "content": "# models/architectures/transformers/resolution_specific/hourly_transformer.py\r\nfrom typing import Dict, Any, Optional\r\nimport torch\r\nfrom torch import nn, Tensor\r\nimport math\r\n\r\nfrom ....data_loading.base.interval_types import TimeInterval\r\nfrom .base_resolution_transformer import BaseResolutionTransformer\r\nfrom ....registry.factory import ModelFactory\r\nfrom ....registry.model_types import ModelType\r\nfrom ....components.layers import EncoderLayer, DecoderLayer\r\n\r\n@ModelFactory.register(ModelType.HOURLY_TRANSFORMER)\r\nclass HourlyTransformer(BaseResolutionTransformer):\r\n    \"\"\"Transformer optimized for hourly predictions.\"\"\"\r\n\r\n    def __init__(self, config: Dict[str, Any]):\r\n        # Validate resolution\r\n        if not (60 <= config.get('forecast_resolution_minutes', 60) <= 180):\r\n            raise ValueError(\"HourlyTransformer is designed for 60-180 minute resolutions\")\r\n        \r\n        super().__init__(config)\r\n        \r\n        # Hourly-specific configurations\r\n        self.hours_per_day = 24\r\n        self.daily_pattern_features = self.d_model // 4  # Dedicated features for daily patterns\r\n        \r\n        # Daily pattern recognition component\r\n        self.daily_pattern_layer = nn.Sequential(\r\n            nn.Linear(self.d_model, self.daily_pattern_features),\r\n            nn.ReLU(),\r\n            nn.Linear(self.daily_pattern_features, self.d_model),\r\n            nn.Dropout(self.dropout)\r\n        )\r\n        \r\n        # Hour-of-day embedding\r\n        self.hour_embedding = nn.Embedding(24, self.d_model)\r\n        \r\n        # Additional time-aware components\r\n        self._setup_temporal_components()\r\n\r\n    def _setup_temporal_components(self) -> None:\r\n        \"\"\"Setup components for temporal pattern recognition.\"\"\"\r\n        self.workday_embedding = nn.Embedding(7, self.d_model)  # Day of week\r\n        \r\n        # Hour-aware attention scaling\r\n        self.hour_importance = nn.Parameter(torch.ones(24))\r\n        \r\n        # Daily cycle attention\r\n        self.daily_cycle_attention = nn.MultiheadAttention(\r\n            embed_dim=self.d_model,\r\n            num_heads=self.n_heads,\r\n            dropout=self.dropout,\r\n            batch_first=True\r\n        )\r\n\r\n    def _create_encoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create encoder layers optimized for hourly patterns.\"\"\"\r\n        return nn.ModuleList([\r\n            EncoderLayer(\r\n                d_model=self.d_model,\r\n                n_heads=self.n_heads,\r\n                d_ff=self.d_ff,\r\n                dropout=self.dropout,\r\n                attention_type=\"standard\",\r\n                activation=\"gelu\"\r\n            ) for _ in range(self.n_encoder_layers)\r\n        ])\r\n\r\n    def _create_decoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create decoder layers optimized for hourly patterns.\"\"\"\r\n        return nn.ModuleList([\r\n            DecoderLayer(\r\n                d_model=self.d_model,\r\n                n_heads=self.n_heads,\r\n                d_ff=self.d_ff,\r\n                dropout=self.dropout,\r\n                attention_type=\"standard\",\r\n                activation=\"gelu\"\r\n            ) for _ in range(self.n_decoder_layers)\r\n        ])\r\n\r\n    def _add_temporal_embeddings(self, x: Tensor, timestamps: Tensor) -> Tensor:\r\n        \"\"\"Add temporal embeddings to input tensor.\"\"\"\r\n        batch_size, seq_len = x.shape[:2]\r\n        \r\n        # Extract hour and day information\r\n        hours = timestamps.hour\r\n        days = timestamps.dayofweek\r\n        \r\n        # Get embeddings\r\n        hour_emb = self.hour_embedding(hours)\r\n        day_emb = self.workday_embedding(days)\r\n        \r\n        # Combine embeddings\r\n        temporal_embedding = hour_emb + day_emb\r\n        \r\n        return x + temporal_embedding\r\n\r\n    def encode(\r\n        self,\r\n        src: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        timestamps: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        \"\"\"Enhanced encoding with hourly pattern recognition.\"\"\"\r\n        # Standard embedding\r\n        src = self.encoder_embedding(src)\r\n        \r\n        if timestamps is not None:\r\n            src = self._add_temporal_embeddings(src, timestamps)\r\n        \r\n        # Extract and enhance daily patterns\r\n        daily_patterns = self.daily_pattern_layer(src)\r\n        src = src + daily_patterns\r\n        \r\n        # Apply daily cycle attention\r\n        daily_attn_out, _ = self.daily_cycle_attention(src, src, src)\r\n        src = src + daily_attn_out\r\n        \r\n        # Pass through encoder layers\r\n        for layer in self.encoder_layers:\r\n            src = layer(\r\n                src,\r\n                src_mask=src_mask,\r\n                src_key_padding_mask=src_key_padding_mask\r\n            )\r\n        return src\r\n\r\n    def _adjust_attention_for_resolution(\r\n        self,\r\n        attention_weights: Tensor,\r\n        resolution_minutes: int\r\n    ) -> Tensor:\r\n        \"\"\"Adjust attention weights for hourly patterns.\"\"\"\r\n        # Get sequence length\r\n        seq_len = attention_weights.size(-1)\r\n        \r\n        # Create hour-based importance weights\r\n        hours = torch.arange(seq_len, device=attention_weights.device) % 24\r\n        hour_weights = self.hour_importance[hours]\r\n        \r\n        # Apply hour importance to attention weights\r\n        attention_weights = attention_weights * hour_weights.unsqueeze(0).unsqueeze(0)\r\n        \r\n        # Add daily pattern emphasis\r\n        daily_pattern = torch.cos(2 * math.pi * hours / 24)\r\n        attention_weights = attention_weights * (1 + 0.1 * daily_pattern.unsqueeze(0).unsqueeze(0))\r\n        \r\n        return attention_weights\r\n\r\n    @classmethod\r\n    def get_resolution_type(cls) -> TimeInterval:\r\n        \"\"\"Get the time interval type for hourly transformer.\"\"\"\r\n        return TimeInterval.HOURLY\r\n\r\n    def forward(\r\n        self,\r\n        src: Tensor,\r\n        tgt: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n        timestamps: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        \"\"\"Forward pass with enhanced hourly pattern recognition.\"\"\"\r\n        # Encode with temporal information\r\n        memory = self.encode(\r\n            src,\r\n            src_mask=src_mask,\r\n            src_key_padding_mask=src_key_padding_mask,\r\n            timestamps=timestamps\r\n        )\r\n        \r\n        # Decode with temporal awareness\r\n        output = self.decode(\r\n            tgt,\r\n            memory,\r\n            tgt_mask=tgt_mask,\r\n            memory_mask=None,\r\n            tgt_key_padding_mask=tgt_key_padding_mask,\r\n            memory_key_padding_mask=src_key_padding_mask\r\n        )\r\n        \r\n        # Final projection\r\n        return self.output_projection(output)"
        }
    ]
}