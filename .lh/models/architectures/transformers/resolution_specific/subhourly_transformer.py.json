{
    "sourceFile": "models/architectures/transformers/resolution_specific/subhourly_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1733264330899,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733264394230,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,170 @@\n+from typing import Dict, Any, Optional\r\n+import torch\r\n+from torch import nn, Tensor\r\n+import pandas as pd\r\n+\r\n+from data_loading.types.interval_types import TimeInterval\r\n+from .base_resolution_transformer import BaseResolutionTransformer\r\n+from models.components.layers import EncoderLayer, DecoderLayer\r\n+\r\n+class SubhourlyTransformer(BaseResolutionTransformer):\r\n+    \"\"\"Transformer optimized for sub-hourly predictions (â‰¤60 minutes).\"\"\"\r\n+    \r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        \"\"\"\r\n+        Initialize SubhourlyTransformer with proper layer creation.\r\n+        \r\n+        Args:\r\n+            config: Configuration dictionary containing model parameters\r\n+        \"\"\"\r\n+        # Ensure output_features is in config\r\n+        if 'output_features' not in config:\r\n+            config['output_features'] = 1  # Default to 1 if not specified\r\n+        \r\n+        # Call parent class initialization first\r\n+        super().__init__(config)\r\n+        \r\n+        # Store configuration parameters\r\n+        self.n_encoder_layers = config.get('n_encoder_layers', 6)\r\n+        self.n_decoder_layers = config.get('n_decoder_layers', 6)\r\n+        self.d_model = config.get('d_model', 512)\r\n+        self.n_heads = config.get('n_heads', 8)\r\n+        self.d_ff = config.get('d_ff', 2048)\r\n+        self.dropout = config.get('dropout', 0.1)\r\n+        \r\n+        # Calculate steps per hour based on resolution\r\n+        self.minutes_per_step = config.get('forecast_resolution_minutes', 15)\r\n+        self.steps_per_hour = 60 // self.minutes_per_step\r\n+        \r\n+        # Create encoder layers\r\n+        self.encoder_layers = self._create_encoder_layers()\r\n+        \r\n+        # Create decoder layers\r\n+        self.decoder_layers = self._create_decoder_layers()\r\n+        \r\n+        # Initialize short-term pattern recognition\r\n+        self.short_term_conv = nn.Conv1d(\r\n+            in_channels=self.d_model,\r\n+            out_channels=self.d_model,\r\n+            kernel_size=3,\r\n+            padding=1,\r\n+            groups=self.d_model  # Depthwise convolution for efficiency\r\n+        )\r\n+        \r\n+        print(\"DEBUG: SubhourlyTransformer initialization complete\")\r\n+        print(f\"DEBUG: Configuration loaded - n_encoder_layers: {self.n_encoder_layers}, d_model: {self.d_model}\")\r\n+\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers optimized for sub-hourly patterns.\"\"\"\r\n+        return nn.ModuleList([\r\n+            EncoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",  # Use standard attention for short sequences\r\n+                activation=\"gelu\"  # GELU for better gradient flow\r\n+            ) for _ in range(self.n_encoder_layers)\r\n+        ])\r\n+\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers optimized for sub-hourly patterns.\"\"\"\r\n+        return nn.ModuleList([\r\n+            DecoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",\r\n+                activation=\"gelu\"\r\n+            ) for _ in range(self.n_decoder_layers)\r\n+        ])\r\n+\r\n+    def encode(\r\n+        self,\r\n+        src: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Enhanced encoding with short-term pattern recognition.\"\"\"\r\n+        # Standard embedding\r\n+        src = self.encoder_embedding(src)\r\n+        \r\n+        # Apply short-term pattern recognition\r\n+        src_conv = src.transpose(1, 2)  # [batch, d_model, seq_len]\r\n+        src_conv = self.short_term_conv(src_conv)\r\n+        src_conv = src_conv.transpose(1, 2)  # [batch, seq_len, d_model]\r\n+        \r\n+        # Combine with original embeddings\r\n+        src = src + src_conv\r\n+        \r\n+        # Pass through encoder layers\r\n+        for layer in self.encoder_layers:\r\n+            src = layer(\r\n+                src,\r\n+                src_mask=src_mask,\r\n+                src_key_padding_mask=src_key_padding_mask\r\n+            )\r\n+        return src\r\n+\r\n+    def _adjust_attention_for_resolution(\r\n+        self,\r\n+        attention_weights: Tensor,\r\n+        resolution_minutes: int\r\n+    ) -> Tensor:\r\n+        \"\"\"Adjust attention weights for sub-hourly patterns.\"\"\"\r\n+        # Enhance attention to recent time steps\r\n+        sequence_length = attention_weights.size(-1)\r\n+        recent_steps = min(self.steps_per_hour * 2, sequence_length)  # Last 2 hours\r\n+        \r\n+        # Create decaying weights for recent time steps\r\n+        decay = torch.exp(torch.arange(recent_steps, 0, -1, device=attention_weights.device) * -0.1)\r\n+        decay = torch.cat([torch.ones(sequence_length - recent_steps, device=attention_weights.device), decay])\r\n+        \r\n+        # Apply decay to attention weights\r\n+        return attention_weights * decay.unsqueeze(0).unsqueeze(0)\r\n+\r\n+    @classmethod\r\n+    def get_resolution_type(cls) -> TimeInterval:\r\n+        \"\"\"Get the time interval type for sub-hourly transformer.\"\"\"\r\n+        return TimeInterval.FIFTEEN_MIN\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        tgt: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Forward pass with enhanced sub-hourly pattern recognition.\"\"\"\r\n+        output = super().forward(\r\n+            src, tgt,\r\n+            src_mask=src_mask,\r\n+            tgt_mask=tgt_mask,\r\n+            src_key_padding_mask=src_key_padding_mask,\r\n+            tgt_key_padding_mask= tgt_key_padding_mask\r\n+        )\r\n+        \r\n+        # Additional processing for sub-hourly predictions if needed\r\n+        if hasattr(self, 'output_processing'):\r\n+            output = self.output_processing(output)\r\n+            \r\n+        return output\r\n+\r\n+    def _handle_temporal_features(\r\n+        self,\r\n+        x: Tensor,\r\n+        timestamps: torch.Tensor\r\n+    ) -> Tensor:\r\n+        \"\"\"Handle temporal features for sub-hourly data.\"\"\"\r\n+        # Convert tensor timestamps to pandas datetime\r\n+        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n+        \r\n+        # Extract temporal features\r\n+        minutes = torch.tensor(dates.minute.values, device=timestamps.device)\r\n+        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n+        \r\n+        # Process and return features\r\n+        return x  # This should be implemented with actual feature processing\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733265470569,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,8 +52,10 @@\n         )\r\n         \r\n         print(\"DEBUG: SubhourlyTransformer initialization complete\")\r\n         print(f\"DEBUG: Configuration loaded - n_encoder_layers: {self.n_encoder_layers}, d_model: {self.d_model}\")\r\n+        \r\n+        self.use_reentrant = False\r\n \r\n     def _create_encoder_layers(self) -> nn.ModuleList:\r\n         \"\"\"Create encoder layers optimized for sub-hourly patterns.\"\"\"\r\n         return nn.ModuleList([\r\n@@ -85,221 +87,84 @@\n         src: Tensor,\r\n         src_mask: Optional[Tensor] = None,\r\n         src_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n-        \"\"\"Enhanced encoding with short-term pattern recognition.\"\"\"\r\n-        # Standard embedding\r\n+        \"\"\"Enhanced encoding with gradient-aware short-term pattern recognition.\"\"\"\r\n+        # Ensure input tensors have gradients enabled\r\n+        if not src.requires_grad:\r\n+            src = src.detach().requires_grad_(True)\r\n+            \r\n+        # Standard embedding with gradient tracking\r\n         src = self.encoder_embedding(src)\r\n+        if not src.requires_grad:\r\n+            src = src.detach().requires_grad_(True)\r\n         \r\n         # Apply short-term pattern recognition\r\n         src_conv = src.transpose(1, 2)  # [batch, d_model, seq_len]\r\n         src_conv = self.short_term_conv(src_conv)\r\n         src_conv = src_conv.transpose(1, 2)  # [batch, seq_len, d_model]\r\n         \r\n-        # Combine with original embeddings\r\n+        # Combine with original embeddings, maintaining gradients\r\n         src = src + src_conv\r\n         \r\n-        # Pass through encoder layers\r\n+        # Pass through encoder layers with proper gradient handling\r\n         for layer in self.encoder_layers:\r\n-            src = layer(\r\n-                src,\r\n-                src_mask=src_mask,\r\n-                src_key_padding_mask=src_key_padding_mask\r\n-            )\r\n-        return src\r\n+            if self.use_checkpointing and self.training:\r\n+                def create_custom_forward(mod):\r\n+                    def custom_forward(*inputs):\r\n+                        x = inputs[0]\r\n+                        if not x.requires_grad:\r\n+                            x = x.detach().requires_grad_(True)\r\n+                        mask = inputs[1] if len(inputs) > 1 else None\r\n+                        key_padding_mask = inputs[2] if len(inputs) > 2 else None\r\n+                        return mod(x, src_mask=mask, src_key_padding_mask=key_padding_mask)\r\n+                    return custom_forward\r\n \r\n-    def _adjust_attention_for_resolution(\r\n-        self,\r\n-        attention_weights: Tensor,\r\n-        resolution_minutes: int\r\n-    ) -> Tensor:\r\n-        \"\"\"Adjust attention weights for sub-hourly patterns.\"\"\"\r\n-        # Enhance attention to recent time steps\r\n-        sequence_length = attention_weights.size(-1)\r\n-        recent_steps = min(self.steps_per_hour * 2, sequence_length)  # Last 2 hours\r\n+                src = checkpoint(\r\n+                    create_custom_forward(layer),\r\n+                    src, src_mask, src_key_padding_mask,\r\n+                    use_reentrant=False,  # Explicitly set checkpointing behavior\r\n+                    preserve_rng_state=True\r\n+                )\r\n+            else:\r\n+                src = layer(\r\n+                    src,\r\n+                    src_mask=src_mask,\r\n+                    src_key_padding_mask=src_key_padding_mask\r\n+                )\r\n         \r\n-        # Create decaying weights for recent time steps\r\n-        decay = torch.exp(torch.arange(recent_steps, 0, -1, device=attention_weights.device) * -0.1)\r\n-        decay = torch.cat([torch.ones(sequence_length - recent_steps, device=attention_weights.device), decay])\r\n-        \r\n-        # Apply decay to attention weights\r\n-        return attention_weights * decay.unsqueeze(0).unsqueeze(0)\r\n-\r\n-    @classmethod\r\n-    def get_resolution_type(cls) -> TimeInterval:\r\n-        \"\"\"Get the time interval type for sub-hourly transformer.\"\"\"\r\n-        return TimeInterval.FIFTEEN_MIN\r\n-\r\n-    def forward(\r\n-        self,\r\n-        src: Tensor,\r\n-        tgt: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Forward pass with enhanced sub-hourly pattern recognition.\"\"\"\r\n-        output = super().forward(\r\n-            src, tgt,\r\n-            src_mask=src_mask,\r\n-            tgt_mask=tgt_mask,\r\n-            src_key_padding_mask=src_key_padding_mask,\r\n-            tgt_key_padding_mask= tgt_key_padding_mask\r\n-        )\r\n-        \r\n-        # Additional processing for sub-hourly predictions if needed\r\n-        if hasattr(self, 'output_processing'):\r\n-            output = self.output_processing(output)\r\n+        # Add debug logging for gradient tracking\r\n+        if torch.is_grad_enabled():\r\n+            print(f\"Debug - Encode output requires_grad: {src.requires_grad}\")\r\n+            print(f\"Debug - Encode output is_leaf: {src.is_leaf}\")\r\n             \r\n-        return output\r\n-\r\n-    def _handle_temporal_features(\r\n-        self,\r\n-        x: Tensor,\r\n-        timestamps: torch.Tensor\r\n-    ) -> Tensor:\r\n-        \"\"\"Handle temporal features for sub-hourly data.\"\"\"\r\n-        # Convert tensor timestamps to pandas datetime\r\n-        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n-        \r\n-        # Extract temporal features\r\n-        minutes = torch.tensor(dates.minute.values, device=timestamps.device)\r\n-        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n-        \r\n-        # Process and return features\r\n-        return x  # This should be implemented with actual feature processing\n-from typing import Dict, Any, Optional\r\n-import torch\r\n-from torch import nn, Tensor\r\n-import pandas as pd\r\n-\r\n-from data_loading.types.interval_types import TimeInterval\r\n-from .base_resolution_transformer import BaseResolutionTransformer\r\n-from models.components.layers import EncoderLayer, DecoderLayer\r\n-\r\n-class SubhourlyTransformer(BaseResolutionTransformer):\r\n-    \"\"\"Transformer optimized for sub-hourly predictions (â‰¤60 minutes).\"\"\"\r\n-    \r\n-    def __init__(self, config: Dict[str, Any]):\r\n-        \"\"\"\r\n-        Initialize SubhourlyTransformer with proper layer creation.\r\n-        \r\n-        Args:\r\n-            config: Configuration dictionary containing model parameters\r\n-        \"\"\"\r\n-        # Ensure output_features is in config\r\n-        if 'output_features' not in config:\r\n-            config['output_features'] = 1  # Default to 1 if not specified\r\n-        \r\n-        # Call parent class initialization first\r\n-        super().__init__(config)\r\n-        \r\n-        # Store configuration parameters\r\n-        self.n_encoder_layers = config.get('n_encoder_layers', 6)\r\n-        self.n_decoder_layers = config.get('n_decoder_layers', 6)\r\n-        self.d_model = config.get('d_model', 512)\r\n-        self.n_heads = config.get('n_heads', 8)\r\n-        self.d_ff = config.get('d_ff', 2048)\r\n-        self.dropout = config.get('dropout', 0.1)\r\n-        \r\n-        # Calculate steps per hour based on resolution\r\n-        self.minutes_per_step = config.get('forecast_resolution_minutes', 15)\r\n-        self.steps_per_hour = 60 // self.minutes_per_step\r\n-        \r\n-        # Create encoder layers\r\n-        self.encoder_layers = self._create_encoder_layers()\r\n-        \r\n-        # Create decoder layers\r\n-        self.decoder_layers = self._create_decoder_layers()\r\n-        \r\n-        # Initialize short-term pattern recognition\r\n-        self.short_term_conv = nn.Conv1d(\r\n-            in_channels=self.d_model,\r\n-            out_channels=self.d_model,\r\n-            kernel_size=3,\r\n-            padding=1,\r\n-            groups=self.d_model  # Depthwise convolution for efficiency\r\n-        )\r\n-        \r\n-        print(\"DEBUG: SubhourlyTransformer initialization complete\")\r\n-        print(f\"DEBUG: Configuration loaded - n_encoder_layers: {self.n_encoder_layers}, d_model: {self.d_model}\")\r\n-\r\n-    def get_input_dims(self) -> int:\r\n-        \"\"\"Get input dimensions for subhourly transformer.\"\"\"\r\n-        return self.input_features\r\n-\r\n-    def get_output_dims(self) -> int:\r\n-        \"\"\"Get output dimensions for subhourly transformer.\"\"\"\r\n-        return self.output_features\r\n-\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create encoder layers optimized for sub-hourly patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            EncoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",  # Use standard attention for short sequences\r\n-                activation=\"gelu\"  # GELU for better gradient flow\r\n-            ) for _ in range(self.n_encoder_layers)\r\n-        ])\r\n-\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create decoder layers optimized for sub-hourly patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            DecoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"gelu\"\r\n-            ) for _ in range(self.n_decoder_layers)\r\n-        ])\r\n-\r\n-    def encode(\r\n-        self,\r\n-        src: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Enhanced encoding with short-term pattern recognition.\"\"\"\r\n-        # Standard embedding\r\n-        src = self.encoder_embedding(src)\r\n-        \r\n-        # Apply short-term pattern recognition\r\n-        src_conv = src.transpose(1, 2)  # [batch, d_model, seq_len]\r\n-        src_conv = self.short_term_conv(src_conv)\r\n-        src_conv = src_conv.transpose(1, 2)  # [batch, seq_len, d_model]\r\n-        \r\n-        # Combine with original embeddings\r\n-        src = src + src_conv\r\n-        \r\n-        # Pass through encoder layers\r\n-        for layer in self.encoder_layers:\r\n-            src = layer(\r\n-                src,\r\n-                src_mask=src_mask,\r\n-                src_key_padding_mask=src_key_padding_mask\r\n-            )\r\n         return src\r\n \r\n     def _adjust_attention_for_resolution(\r\n         self,\r\n         attention_weights: Tensor,\r\n         resolution_minutes: int\r\n     ) -> Tensor:\r\n-        \"\"\"Adjust attention weights for sub-hourly patterns.\"\"\"\r\n-        # Enhance attention to recent time steps\r\n+        \"\"\"Adjust attention weights for sub-hourly patterns with gradient tracking.\"\"\"\r\n         sequence_length = attention_weights.size(-1)\r\n-        recent_steps = min(self.steps_per_hour * 2, sequence_length)  # Last 2 hours\r\n+        recent_steps = min(self.steps_per_hour * 2, sequence_length)\r\n         \r\n-        # Create decaying weights for recent time steps\r\n-        decay = torch.exp(torch.arange(recent_steps, 0, -1, device=attention_weights.device) * -0.1)\r\n-        decay = torch.cat([torch.ones(sequence_length - recent_steps, device=attention_weights.device), decay])\r\n+        # Create decaying weights with gradients\r\n+        decay = torch.exp(torch.arange(\r\n+            recent_steps, 0, -1, \r\n+            device=attention_weights.device, \r\n+            dtype=attention_weights.dtype\r\n+        ) * -0.1).requires_grad_(True)\r\n         \r\n+        # Concatenate with ones tensor\r\n+        ones = torch.ones(\r\n+            sequence_length - recent_steps,\r\n+            device=attention_weights.device,\r\n+            dtype=attention_weights.dtype,\r\n+            requires_grad=True\r\n+        )\r\n+        decay = torch.cat([ones, decay])\r\n+        \r\n         # Apply decay to attention weights\r\n         return attention_weights * decay.unsqueeze(0).unsqueeze(0)\r\n \r\n     @classmethod\r\n@@ -315,20 +180,31 @@\n         tgt_mask: Optional[Tensor] = None,\r\n         src_key_padding_mask: Optional[Tensor] = None,\r\n         tgt_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n-        \"\"\"Forward pass with enhanced sub-hourly pattern recognition.\"\"\"\r\n+        \"\"\"Forward pass with gradient tracking verification.\"\"\"\r\n+        # Ensure input tensors have gradients enabled\r\n+        if not src.requires_grad:\r\n+            src = src.detach().requires_grad_(True)\r\n+        if not tgt.requires_grad:\r\n+            tgt = tgt.detach().requires_grad_(True)\r\n+            \r\n+        # Log gradient status for debugging\r\n+        if torch.is_grad_enabled():\r\n+            print(f\"Debug - Forward input src requires_grad: {src.requires_grad}\")\r\n+            print(f\"Debug - Forward input tgt requires_grad: {tgt.requires_grad}\")\r\n+            \r\n         output = super().forward(\r\n             src, tgt,\r\n             src_mask=src_mask,\r\n             tgt_mask=tgt_mask,\r\n             src_key_padding_mask=src_key_padding_mask,\r\n-            tgt_key_padding_mask= tgt_key_padding_mask\r\n+            tgt_key_padding_mask=tgt_key_padding_mask\r\n         )\r\n         \r\n-        # Additional processing for sub-hourly predictions if needed\r\n-        if hasattr(self, 'output_processing'):\r\n-            output = self.output_processing(output)\r\n+        # Verify gradient tracking is maintained\r\n+        if torch.is_grad_enabled():\r\n+            print(f\"Debug - Forward output requires_grad: {output.requires_grad}\")\r\n             \r\n         return output\r\n \r\n     def _handle_temporal_features(\r\n"
                },
                {
                    "date": 1733265543301,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,222 @@\n+from typing import Dict, Any, Optional\r\n+import torch\r\n+from torch import nn, Tensor\r\n+import pandas as pd\r\n+\r\n+from data_loading.types.interval_types import TimeInterval\r\n+from .base_resolution_transformer import BaseResolutionTransformer\r\n+from models.components.layers import EncoderLayer, DecoderLayer\r\n+from torch.utils.checkpoint import checkpoint\r\n+\r\n+class SubhourlyTransformer(BaseResolutionTransformer):\r\n+    \"\"\"Transformer optimized for sub-hourly predictions (â‰¤60 minutes).\"\"\"\r\n+    \r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        \"\"\"\r\n+        Initialize SubhourlyTransformer with proper layer creation.\r\n+        \r\n+        Args:\r\n+            config: Configuration dictionary containing model parameters\r\n+        \"\"\"\r\n+        # Ensure output_features is in config\r\n+        if 'output_features' not in config:\r\n+            config['output_features'] = 1  # Default to 1 if not specified\r\n+        \r\n+        # Call parent class initialization first\r\n+        super().__init__(config)\r\n+        \r\n+        # Store configuration parameters\r\n+        self.n_encoder_layers = config.get('n_encoder_layers', 6)\r\n+        self.n_decoder_layers = config.get('n_decoder_layers', 6)\r\n+        self.d_model = config.get('d_model', 512)\r\n+        self.n_heads = config.get('n_heads', 8)\r\n+        self.d_ff = config.get('d_ff', 2048)\r\n+        self.dropout = config.get('dropout', 0.1)\r\n+        self.use_checkpointing = config.get('use_checkpointing', True)  # Add explicit flag\r\n+        \r\n+        # Calculate steps per hour based on resolution\r\n+        self.minutes_per_step = config.get('forecast_resolution_minutes', 15)\r\n+        self.steps_per_hour = 60 // self.minutes_per_step\r\n+        \r\n+        # Create encoder and decoder layers\r\n+        self.encoder_layers = self._create_encoder_layers()\r\n+        self.decoder_layers = self._create_decoder_layers()\r\n+        \r\n+        # Initialize short-term pattern recognition\r\n+        self.short_term_conv = nn.Conv1d(\r\n+            in_channels=self.d_model,\r\n+            out_channels=self.d_model,\r\n+            kernel_size=3,\r\n+            padding=1,\r\n+            groups=self.d_model  # Depthwise convolution for efficiency\r\n+        )\r\n+        \r\n+        print(\"DEBUG: SubhourlyTransformer initialization complete\")\r\n+        print(f\"DEBUG: Configuration loaded - n_encoder_layers: {self.n_encoder_layers}, d_model: {self.d_model}\")\r\n+\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers optimized for sub-hourly patterns.\"\"\"\r\n+        return nn.ModuleList([\r\n+            EncoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",  # Use standard attention for short sequences\r\n+                activation=\"gelu\"  # GELU for better gradient flow\r\n+            ) for _ in range(self.n_encoder_layers)\r\n+        ])\r\n+\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers optimized for sub-hourly patterns.\"\"\"\r\n+        return nn.ModuleList([\r\n+            DecoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",\r\n+                activation=\"gelu\"\r\n+            ) for _ in range(self.n_decoder_layers)\r\n+        ])\r\n+\r\n+    def encode(\r\n+        self,\r\n+        src: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Enhanced encoding with gradient-aware short-term pattern recognition.\"\"\"\r\n+        # Ensure input tensors have gradients enabled\r\n+        if not src.requires_grad:\r\n+            src = src.detach().requires_grad_(True)\r\n+            \r\n+        # Standard embedding with gradient tracking\r\n+        src = self.encoder_embedding(src)\r\n+        if not src.requires_grad:\r\n+            src = src.detach().requires_grad_(True)\r\n+        \r\n+        # Apply short-term pattern recognition\r\n+        src_conv = src.transpose(1, 2)  # [batch, d_model, seq_len]\r\n+        src_conv = self.short_term_conv(src_conv)\r\n+        src_conv = src_conv.transpose(1, 2)  # [batch, seq_len, d_model]\r\n+        \r\n+        # Combine with original embeddings, maintaining gradients\r\n+        src = src + src_conv\r\n+        \r\n+        # Pass through encoder layers with proper gradient handling\r\n+        for layer in self.encoder_layers:\r\n+            if self.use_checkpointing and self.training:\r\n+                def create_custom_forward(mod):\r\n+                    def custom_forward(*inputs):\r\n+                        x = inputs[0]\r\n+                        if not x.requires_grad:\r\n+                            x = x.detach().requires_grad_(True)\r\n+                        mask = inputs[1] if len(inputs) > 1 else None\r\n+                        key_padding_mask = inputs[2] if len(inputs) > 2 else None\r\n+                        return mod(x, src_mask=mask, src_key_padding_mask=key_padding_mask)\r\n+                    return custom_forward\r\n+\r\n+                src = checkpoint(\r\n+                    create_custom_forward(layer),\r\n+                    src, src_mask, src_key_padding_mask,\r\n+                    use_reentrant=False,  # Explicitly set checkpointing behavior\r\n+                    preserve_rng_state=True\r\n+                )\r\n+            else:\r\n+                src = layer(\r\n+                    src,\r\n+                    src_mask=src_mask,\r\n+                    src_key_padding_mask=src_key_padding_mask\r\n+                )\r\n+        \r\n+        # Add debug logging for gradient tracking\r\n+        if torch.is_grad_enabled():\r\n+            print(f\"Debug - Encode output requires_grad: {src.requires_grad}\")\r\n+            print(f\"Debug - Encode output is_leaf: {src.is_leaf}\")\r\n+            \r\n+        return src\r\n+\r\n+    def _adjust_attention_for_resolution(\r\n+        self,\r\n+        attention_weights: Tensor,\r\n+        resolution_minutes: int\r\n+    ) -> Tensor:\r\n+        \"\"\"Adjust attention weights for sub-hourly patterns with gradient tracking.\"\"\"\r\n+        sequence_length = attention_weights.size(-1)\r\n+        recent_steps = min(self.steps_per_hour * 2, sequence_length)\r\n+        \r\n+        # Create decaying weights with gradients\r\n+        decay = torch.exp(torch.arange(\r\n+            recent_steps, 0, -1, \r\n+            device=attention_weights.device, \r\n+            dtype=attention_weights.dtype\r\n+        ) * -0.1).requires_grad_(True)\r\n+        \r\n+        # Concatenate with ones tensor\r\n+        ones = torch.ones(\r\n+            sequence_length - recent_steps,\r\n+            device=attention_weights.device,\r\n+            dtype=attention_weights.dtype,\r\n+            requires_grad=True\r\n+        )\r\n+        decay = torch.cat([ones, decay])\r\n+        \r\n+        # Apply decay to attention weights\r\n+        return attention_weights * decay.unsqueeze(0).unsqueeze(0)\r\n+\r\n+    @classmethod\r\n+    def get_resolution_type(cls) -> TimeInterval:\r\n+        \"\"\"Get the time interval type for sub-hourly transformer.\"\"\"\r\n+        return TimeInterval.FIFTEEN_MIN\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        tgt: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Forward pass with gradient tracking verification.\"\"\"\r\n+        # Ensure input tensors have gradients enabled\r\n+        if not src.requires_grad:\r\n+            src = src.detach().requires_grad_(True)\r\n+        if not tgt.requires_grad:\r\n+            tgt = tgt.detach().requires_grad_(True)\r\n+            \r\n+        # Log gradient status for debugging\r\n+        if torch.is_grad_enabled():\r\n+            print(f\"Debug - Forward input src requires_grad: {src.requires_grad}\")\r\n+            print(f\"Debug - Forward input tgt requires_grad: {tgt.requires_grad}\")\r\n+            \r\n+        output = super().forward(\r\n+            src, tgt,\r\n+            src_mask=src_mask,\r\n+            tgt_mask=tgt_mask,\r\n+            src_key_padding_mask=src_key_padding_mask,\r\n+            tgt_key_padding_mask=tgt_key_padding_mask\r\n+        )\r\n+        \r\n+        # Verify gradient tracking is maintained\r\n+        if torch.is_grad_enabled():\r\n+            print(f\"Debug - Forward output requires_grad: {output.requires_grad}\")\r\n+            \r\n+        return output\r\n+\r\n+    def _handle_temporal_features(\r\n+        self,\r\n+        x: Tensor,\r\n+        timestamps: torch.Tensor\r\n+    ) -> Tensor:\r\n+        \"\"\"Handle temporal features for sub-hourly data.\"\"\"\r\n+        # Convert tensor timestamps to pandas datetime\r\n+        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n+        \r\n+        # Extract temporal features\r\n+        minutes = torch.tensor(dates.minute.values, device=timestamps.device)\r\n+        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n+        \r\n+        # Process and return features\r\n+        return x  # This should be implemented with actual feature processing\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733265602359,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,244 +79,63 @@\n                 activation=\"gelu\"\r\n             ) for _ in range(self.n_decoder_layers)\r\n         ])\r\n \r\n-    def encode(\r\n-        self,\r\n-        src: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Enhanced encoding with gradient-aware short-term pattern recognition.\"\"\"\r\n-        # Ensure input tensors have gradients enabled\r\n-        if not src.requires_grad:\r\n-            src = src.detach().requires_grad_(True)\r\n-            \r\n-        # Standard embedding with gradient tracking\r\n-        src = self.encoder_embedding(src)\r\n-        if not src.requires_grad:\r\n-            src = src.detach().requires_grad_(True)\r\n+    def _create_custom_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n+        \"\"\"Creates a type-safe forward function for checkpointing.\r\n         \r\n-        # Apply short-term pattern recognition\r\n-        src_conv = src.transpose(1, 2)  # [batch, d_model, seq_len]\r\n-        src_conv = self.short_term_conv(src_conv)\r\n-        src_conv = src_conv.transpose(1, 2)  # [batch, seq_len, d_model]\r\n+        This helper method ensures proper type handling for the checkpointed function.\r\n+        The returned function is guaranteed to return a Tensor, maintaining type safety.\r\n         \r\n-        # Combine with original embeddings, maintaining gradients\r\n-        src = src + src_conv\r\n-        \r\n-        # Pass through encoder layers with proper gradient handling\r\n-        for layer in self.encoder_layers:\r\n-            if self.use_checkpointing and self.training:\r\n-                def create_custom_forward(mod):\r\n-                    def custom_forward(*inputs):\r\n-                        x = inputs[0]\r\n-                        if not x.requires_grad:\r\n-                            x = x.detach().requires_grad_(True)\r\n-                        mask = inputs[1] if len(inputs) > 1 else None\r\n-                        key_padding_mask = inputs[2] if len(inputs) > 2 else None\r\n-                        return mod(x, src_mask=mask, src_key_padding_mask=key_padding_mask)\r\n-                    return custom_forward\r\n-\r\n-                src = checkpoint(\r\n-                    create_custom_forward(layer),\r\n-                    src, src_mask, src_key_padding_mask,\r\n-                    use_reentrant=False,  # Explicitly set checkpointing behavior\r\n-                    preserve_rng_state=True\r\n-                )\r\n-            else:\r\n-                src = layer(\r\n-                    src,\r\n-                    src_mask=src_mask,\r\n-                    src_key_padding_mask=src_key_padding_mask\r\n-                )\r\n-        \r\n-        # Add debug logging for gradient tracking\r\n-        if torch.is_grad_enabled():\r\n-            print(f\"Debug - Encode output requires_grad: {src.requires_grad}\")\r\n-            print(f\"Debug - Encode output is_leaf: {src.is_leaf}\")\r\n+        Args:\r\n+            layer: The encoder layer to be wrapped\r\n             \r\n-        return src\r\n-\r\n-    def _adjust_attention_for_resolution(\r\n-        self,\r\n-        attention_weights: Tensor,\r\n-        resolution_minutes: int\r\n-    ) -> Tensor:\r\n-        \"\"\"Adjust attention weights for sub-hourly patterns with gradient tracking.\"\"\"\r\n-        sequence_length = attention_weights.size(-1)\r\n-        recent_steps = min(self.steps_per_hour * 2, sequence_length)\r\n-        \r\n-        # Create decaying weights with gradients\r\n-        decay = torch.exp(torch.arange(\r\n-            recent_steps, 0, -1, \r\n-            device=attention_weights.device, \r\n-            dtype=attention_weights.dtype\r\n-        ) * -0.1).requires_grad_(True)\r\n-        \r\n-        # Concatenate with ones tensor\r\n-        ones = torch.ones(\r\n-            sequence_length - recent_steps,\r\n-            device=attention_weights.device,\r\n-            dtype=attention_weights.dtype,\r\n-            requires_grad=True\r\n-        )\r\n-        decay = torch.cat([ones, decay])\r\n-        \r\n-        # Apply decay to attention weights\r\n-        return attention_weights * decay.unsqueeze(0).unsqueeze(0)\r\n-\r\n-    @classmethod\r\n-    def get_resolution_type(cls) -> TimeInterval:\r\n-        \"\"\"Get the time interval type for sub-hourly transformer.\"\"\"\r\n-        return TimeInterval.FIFTEEN_MIN\r\n-\r\n-    def forward(\r\n-        self,\r\n-        src: Tensor,\r\n-        tgt: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Forward pass with gradient tracking verification.\"\"\"\r\n-        # Ensure input tensors have gradients enabled\r\n-        if not src.requires_grad:\r\n-            src = src.detach().requires_grad_(True)\r\n-        if not tgt.requires_grad:\r\n-            tgt = tgt.detach().requires_grad_(True)\r\n+        Returns:\r\n+            A callable that properly handles types for checkpointing\r\n+        \"\"\"\r\n+        def custom_forward(*inputs: Any) -> Tensor:\r\n+            # Validate inputs\r\n+            if not inputs or not isinstance(inputs[0], torch.Tensor):\r\n+                raise ValueError(\"First input must be a tensor\")\r\n             \r\n-        # Log gradient status for debugging\r\n-        if torch.is_grad_enabled():\r\n-            print(f\"Debug - Forward input src requires_grad: {src.requires_grad}\")\r\n-            print(f\"Debug - Forward input tgt requires_grad: {tgt.requires_grad}\")\r\n+            x = inputs[0]\r\n+            if not x.requires_grad:\r\n+                x = x.detach().requires_grad_(True)\r\n+                \r\n+            # Handle optional mask inputs\r\n+            mask = inputs[1] if len(inputs) > 1 else None\r\n+            key_padding_mask = inputs[2] if len(inputs) > 2 else None\r\n             \r\n-        output = super().forward(\r\n-            src, tgt,\r\n-            src_mask=src_mask,\r\n-            tgt_mask=tgt_mask,\r\n-            src_key_padding_mask=src_key_padding_mask,\r\n-            tgt_key_padding_mask=tgt_key_padding_mask\r\n-        )\r\n-        \r\n-        # Verify gradient tracking is maintained\r\n-        if torch.is_grad_enabled():\r\n-            print(f\"Debug - Forward output requires_grad: {output.requires_grad}\")\r\n+            # Process through layer and ensure tensor output\r\n+            output = layer(x, src_mask=mask, src_key_padding_mask=key_padding_mask)\r\n+            if not isinstance(output, torch.Tensor):\r\n+                raise ValueError(\"Layer output must be a tensor\")\r\n+                \r\n+            return output\r\n             \r\n-        return output\r\n+        return custom_forward\r\n \r\n-    def _handle_temporal_features(\r\n-        self,\r\n-        x: Tensor,\r\n-        timestamps: torch.Tensor\r\n-    ) -> Tensor:\r\n-        \"\"\"Handle temporal features for sub-hourly data.\"\"\"\r\n-        # Convert tensor timestamps to pandas datetime\r\n-        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n-        \r\n-        # Extract temporal features\r\n-        minutes = torch.tensor(dates.minute.values, device=timestamps.device)\r\n-        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n-        \r\n-        # Process and return features\r\n-        return x  # This should be implemented with actual feature processing\n-from typing import Dict, Any, Optional\r\n-import torch\r\n-from torch import nn, Tensor\r\n-import pandas as pd\r\n-\r\n-from data_loading.types.interval_types import TimeInterval\r\n-from .base_resolution_transformer import BaseResolutionTransformer\r\n-from models.components.layers import EncoderLayer, DecoderLayer\r\n-\r\n-class SubhourlyTransformer(BaseResolutionTransformer):\r\n-    \"\"\"Transformer optimized for sub-hourly predictions (â‰¤60 minutes).\"\"\"\r\n-    \r\n-    def __init__(self, config: Dict[str, Any]):\r\n-        \"\"\"\r\n-        Initialize SubhourlyTransformer with proper layer creation.\r\n-        \r\n-        Args:\r\n-            config: Configuration dictionary containing model parameters\r\n-        \"\"\"\r\n-        # Ensure output_features is in config\r\n-        if 'output_features' not in config:\r\n-            config['output_features'] = 1  # Default to 1 if not specified\r\n-        \r\n-        # Call parent class initialization first\r\n-        super().__init__(config)\r\n-        \r\n-        # Store configuration parameters\r\n-        self.n_encoder_layers = config.get('n_encoder_layers', 6)\r\n-        self.n_decoder_layers = config.get('n_decoder_layers', 6)\r\n-        self.d_model = config.get('d_model', 512)\r\n-        self.n_heads = config.get('n_heads', 8)\r\n-        self.d_ff = config.get('d_ff', 2048)\r\n-        self.dropout = config.get('dropout', 0.1)\r\n-        \r\n-        # Calculate steps per hour based on resolution\r\n-        self.minutes_per_step = config.get('forecast_resolution_minutes', 15)\r\n-        self.steps_per_hour = 60 // self.minutes_per_step\r\n-        \r\n-        # Create encoder layers\r\n-        self.encoder_layers = self._create_encoder_layers()\r\n-        \r\n-        # Create decoder layers\r\n-        self.decoder_layers = self._create_decoder_layers()\r\n-        \r\n-        # Initialize short-term pattern recognition\r\n-        self.short_term_conv = nn.Conv1d(\r\n-            in_channels=self.d_model,\r\n-            out_channels=self.d_model,\r\n-            kernel_size=3,\r\n-            padding=1,\r\n-            groups=self.d_model  # Depthwise convolution for efficiency\r\n-        )\r\n-        \r\n-        print(\"DEBUG: SubhourlyTransformer initialization complete\")\r\n-        print(f\"DEBUG: Configuration loaded - n_encoder_layers: {self.n_encoder_layers}, d_model: {self.d_model}\")\r\n-        \r\n-        self.use_reentrant = False\r\n-\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create encoder layers optimized for sub-hourly patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            EncoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",  # Use standard attention for short sequences\r\n-                activation=\"gelu\"  # GELU for better gradient flow\r\n-            ) for _ in range(self.n_encoder_layers)\r\n-        ])\r\n-\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create decoder layers optimized for sub-hourly patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            DecoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"gelu\"\r\n-            ) for _ in range(self.n_decoder_layers)\r\n-        ])\r\n-\r\n     def encode(\r\n         self,\r\n         src: Tensor,\r\n         src_mask: Optional[Tensor] = None,\r\n         src_key_padding_mask: Optional[Tensor] = None,\r\n     ) -> Tensor:\r\n-        \"\"\"Enhanced encoding with gradient-aware short-term pattern recognition.\"\"\"\r\n+        \"\"\"Enhanced encoding with type-safe gradient tracking.\r\n+        \r\n+        Args:\r\n+            src: Source sequence tensor\r\n+            src_mask: Optional attention mask\r\n+            src_key_padding_mask: Optional padding mask\r\n+            \r\n+        Returns:\r\n+            Encoded tensor with proper gradient tracking\r\n+        \"\"\"\r\n         # Ensure input tensors have gradients enabled\r\n         if not src.requires_grad:\r\n             src = src.detach().requires_grad_(True)\r\n             \r\n-        # Standard embedding with gradient tracking\r\n+        # Apply embedding with type safety\r\n         src = self.encoder_embedding(src)\r\n         if not src.requires_grad:\r\n             src = src.detach().requires_grad_(True)\r\n         \r\n@@ -324,41 +143,39 @@\n         src_conv = src.transpose(1, 2)  # [batch, d_model, seq_len]\r\n         src_conv = self.short_term_conv(src_conv)\r\n         src_conv = src_conv.transpose(1, 2)  # [batch, seq_len, d_model]\r\n         \r\n-        # Combine with original embeddings, maintaining gradients\r\n+        # Combine with original embeddings\r\n         src = src + src_conv\r\n         \r\n-        # Pass through encoder layers with proper gradient handling\r\n+        # Process through encoder layers with type-safe checkpointing\r\n         for layer in self.encoder_layers:\r\n             if self.use_checkpointing and self.training:\r\n-                def create_custom_forward(mod):\r\n-                    def custom_forward(*inputs):\r\n-                        x = inputs[0]\r\n-                        if not x.requires_grad:\r\n-                            x = x.detach().requires_grad_(True)\r\n-                        mask = inputs[1] if len(inputs) > 1 else None\r\n-                        key_padding_mask = inputs[2] if len(inputs) > 2 else None\r\n-                        return mod(x, src_mask=mask, src_key_padding_mask=key_padding_mask)\r\n-                    return custom_forward\r\n-\r\n-                src = checkpoint(\r\n-                    create_custom_forward(layer),\r\n+                # Create type-safe forward function\r\n+                custom_forward = self._create_custom_forward(layer)\r\n+                \r\n+                # Apply checkpointing with explicit type casting\r\n+                checkpoint_output = checkpoint(\r\n+                    custom_forward,\r\n                     src, src_mask, src_key_padding_mask,\r\n-                    use_reentrant=False,  # Explicitly set checkpointing behavior\r\n+                    use_reentrant=False,\r\n                     preserve_rng_state=True\r\n                 )\r\n+                \r\n+                # Ensure output maintains type safety\r\n+                src = cast(Tensor, checkpoint_output)\r\n+                if not src.requires_grad:\r\n+                    src = src.detach().requires_grad_(True)\r\n             else:\r\n                 src = layer(\r\n                     src,\r\n                     src_mask=src_mask,\r\n                     src_key_padding_mask=src_key_padding_mask\r\n                 )\r\n         \r\n-        # Add debug logging for gradient tracking\r\n-        if torch.is_grad_enabled():\r\n-            print(f\"Debug - Encode output requires_grad: {src.requires_grad}\")\r\n-            print(f\"Debug - Encode output is_leaf: {src.is_leaf}\")\r\n+        # Validate final output\r\n+        if not isinstance(src, torch.Tensor):\r\n+            raise ValueError(\"Encoder output must be a tensor\")\r\n             \r\n         return src\r\n \r\n     def _adjust_attention_for_resolution(\r\n"
                },
                {
                    "date": 1733265616520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,5 @@\n-from typing import Dict, Any, Optional\r\n+from typing import Callable, Dict, Any, Optional, cast\r\n import torch\r\n from torch import nn, Tensor\r\n import pandas as pd\r\n \r\n"
                },
                {
                    "date": 1733265692710,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,279 @@\n+from typing import Callable, Dict, Any, Optional, cast\r\n+import torch\r\n+from torch import nn, Tensor\r\n+import pandas as pd\r\n+\r\n+from data_loading.types.interval_types import TimeInterval\r\n+from .base_resolution_transformer import BaseResolutionTransformer\r\n+from models.components.layers import EncoderLayer, DecoderLayer\r\n+from torch.utils.checkpoint import checkpoint\r\n+\r\n+class SubhourlyTransformer(BaseResolutionTransformer):\r\n+    \"\"\"Transformer optimized for sub-hourly predictions (â‰¤60 minutes).\"\"\"\r\n+    \r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        \"\"\"\r\n+        Initialize SubhourlyTransformer with proper layer creation.\r\n+        \r\n+        Args:\r\n+            config: Configuration dictionary containing model parameters\r\n+        \"\"\"\r\n+        # Ensure output_features is in config\r\n+        if 'output_features' not in config:\r\n+            config['output_features'] = 1  # Default to 1 if not specified\r\n+        \r\n+        # Call parent class initialization first\r\n+        super().__init__(config)\r\n+        \r\n+        # Store configuration parameters\r\n+        self.n_encoder_layers = config.get('n_encoder_layers', 6)\r\n+        self.n_decoder_layers = config.get('n_decoder_layers', 6)\r\n+        self.d_model = config.get('d_model', 512)\r\n+        self.n_heads = config.get('n_heads', 8)\r\n+        self.d_ff = config.get('d_ff', 2048)\r\n+        self.dropout = config.get('dropout', 0.1)\r\n+        self.use_checkpointing = config.get('use_checkpointing', True)  # Add explicit flag\r\n+        \r\n+        # Calculate steps per hour based on resolution\r\n+        self.minutes_per_step = config.get('forecast_resolution_minutes', 15)\r\n+        self.steps_per_hour = 60 // self.minutes_per_step\r\n+        \r\n+        # Create encoder and decoder layers\r\n+        self.encoder_layers = self._create_encoder_layers()\r\n+        self.decoder_layers = self._create_decoder_layers()\r\n+        \r\n+        # Initialize short-term pattern recognition\r\n+        self.short_term_conv = nn.Conv1d(\r\n+            in_channels=self.d_model,\r\n+            out_channels=self.d_model,\r\n+            kernel_size=3,\r\n+            padding=1,\r\n+            groups=self.d_model  # Depthwise convolution for efficiency\r\n+        )\r\n+        \r\n+        print(\"DEBUG: SubhourlyTransformer initialization complete\")\r\n+        print(f\"DEBUG: Configuration loaded - n_encoder_layers: {self.n_encoder_layers}, d_model: {self.d_model}\")\r\n+\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create encoder layers optimized for sub-hourly patterns.\"\"\"\r\n+        return nn.ModuleList([\r\n+            EncoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",  # Use standard attention for short sequences\r\n+                activation=\"gelu\"  # GELU for better gradient flow\r\n+            ) for _ in range(self.n_encoder_layers)\r\n+        ])\r\n+\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        \"\"\"Create decoder layers optimized for sub-hourly patterns.\"\"\"\r\n+        return nn.ModuleList([\r\n+            DecoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",\r\n+                activation=\"gelu\"\r\n+            ) for _ in range(self.n_decoder_layers)\r\n+        ])\r\n+\r\n+    def _create_custom_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n+        \"\"\"Creates a type-safe forward function for checkpointing.\r\n+        \r\n+        This helper method ensures proper type handling for the checkpointed function.\r\n+        The returned function is guaranteed to return a Tensor, maintaining type safety.\r\n+        \r\n+        Args:\r\n+            layer: The encoder layer to be wrapped\r\n+            \r\n+        Returns:\r\n+            A callable that properly handles types for checkpointing\r\n+        \"\"\"\r\n+        def custom_forward(*inputs: Any) -> Tensor:\r\n+            # Validate inputs\r\n+            if not inputs or not isinstance(inputs[0], torch.Tensor):\r\n+                raise ValueError(\"First input must be a tensor\")\r\n+            \r\n+            x = inputs[0]\r\n+            if not x.requires_grad:\r\n+                x = x.detach().requires_grad_(True)\r\n+                \r\n+            # Handle optional mask inputs\r\n+            mask = inputs[1] if len(inputs) > 1 else None\r\n+            key_padding_mask = inputs[2] if len(inputs) > 2 else None\r\n+            \r\n+            # Process through layer and ensure tensor output\r\n+            output = layer(x, src_mask=mask, src_key_padding_mask=key_padding_mask)\r\n+            if not isinstance(output, torch.Tensor):\r\n+                raise ValueError(\"Layer output must be a tensor\")\r\n+                \r\n+            return output\r\n+            \r\n+        return custom_forward\r\n+\r\n+    def encode(\r\n+            self,\r\n+            src: Tensor,\r\n+            src_mask: Optional[Tensor] = None,\r\n+            src_key_padding_mask: Optional[Tensor] = None,\r\n+        ) -> Tensor:\r\n+            \"\"\"Enhanced encoding with improved gradient handling for checkpointing.\"\"\"\r\n+            # Initial gradient setup\r\n+            if not src.requires_grad:\r\n+                src = src.detach().requires_grad_(True)\r\n+                \r\n+            # Apply embedding\r\n+            src = self.encoder_embedding(src)\r\n+            if not src.requires_grad:\r\n+                src = src.detach().requires_grad_(True)\r\n+            \r\n+            # Apply short-term pattern recognition\r\n+            src_conv = src.transpose(1, 2)\r\n+            src_conv = self.short_term_conv(src_conv)\r\n+            src_conv = src_conv.transpose(1, 2)\r\n+            src = src + src_conv\r\n+            \r\n+            # Process through encoder layers\r\n+            for i, layer in enumerate(self.encoder_layers):\r\n+                if self.use_checkpointing and self.training:\r\n+                    def custom_forward(x_inner, mask=None, key_padding_mask=None):\r\n+                        # Ensure inputs maintain gradients within checkpoint\r\n+                        x_inner = x_inner.detach().requires_grad_(True)\r\n+                        if mask is not None:\r\n+                            mask = mask.detach()\r\n+                        if key_padding_mask is not None:\r\n+                            key_padding_mask = key_padding_mask.detach()\r\n+                        \r\n+                        # Process through layer\r\n+                        out = layer(x_inner, \r\n+                                src_mask=mask, \r\n+                                src_key_padding_mask=key_padding_mask)\r\n+                        \r\n+                        # Ensure output maintains gradients\r\n+                        if not out.requires_grad:\r\n+                            out = out.detach().requires_grad_(True)\r\n+                        return out\r\n+\r\n+                    # Prepare inputs for checkpointing\r\n+                    inputs = [src]\r\n+                    if src_mask is not None:\r\n+                        inputs.append(src_mask)\r\n+                    else:\r\n+                        inputs.append(None)\r\n+                    if src_key_padding_mask is not None:\r\n+                        inputs.append(src_key_padding_mask)\r\n+                    else:\r\n+                        inputs.append(None)\r\n+                    \r\n+                    # Apply checkpointing with proper gradient handling\r\n+                    src = checkpoint(\r\n+                        custom_forward,\r\n+                        *inputs,\r\n+                        use_reentrant=False,\r\n+                        preserve_rng_state=True\r\n+                    )\r\n+                    \r\n+                    # Double-check gradient state after checkpoint\r\n+                    if not src.requires_grad:\r\n+                        src = src.detach().requires_grad_(True)\r\n+                else:\r\n+                    # Standard forward pass\r\n+                    src = layer(\r\n+                        src,\r\n+                        src_mask=src_mask,\r\n+                        src_key_padding_mask=src_key_padding_mask\r\n+                    )\r\n+                    \r\n+                # Log gradient state for debugging\r\n+                print(f\"Debug - Layer {i} output requires_grad: {src.requires_grad}\")\r\n+                if hasattr(src, 'grad_fn'):\r\n+                    print(f\"Debug - Layer {i} grad_fn: {type(src.grad_fn).__name__}\")\r\n+            \r\n+            return src\r\n+\r\n+    def _adjust_attention_for_resolution(\r\n+        self,\r\n+        attention_weights: Tensor,\r\n+        resolution_minutes: int\r\n+    ) -> Tensor:\r\n+        \"\"\"Adjust attention weights for sub-hourly patterns with gradient tracking.\"\"\"\r\n+        sequence_length = attention_weights.size(-1)\r\n+        recent_steps = min(self.steps_per_hour * 2, sequence_length)\r\n+        \r\n+        # Create decaying weights with gradients\r\n+        decay = torch.exp(torch.arange(\r\n+            recent_steps, 0, -1, \r\n+            device=attention_weights.device, \r\n+            dtype=attention_weights.dtype\r\n+        ) * -0.1).requires_grad_(True)\r\n+        \r\n+        # Concatenate with ones tensor\r\n+        ones = torch.ones(\r\n+            sequence_length - recent_steps,\r\n+            device=attention_weights.device,\r\n+            dtype=attention_weights.dtype,\r\n+            requires_grad=True\r\n+        )\r\n+        decay = torch.cat([ones, decay])\r\n+        \r\n+        # Apply decay to attention weights\r\n+        return attention_weights * decay.unsqueeze(0).unsqueeze(0)\r\n+\r\n+    @classmethod\r\n+    def get_resolution_type(cls) -> TimeInterval:\r\n+        \"\"\"Get the time interval type for sub-hourly transformer.\"\"\"\r\n+        return TimeInterval.FIFTEEN_MIN\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        tgt: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+    ) -> Tensor:\r\n+        \"\"\"Forward pass with gradient tracking verification.\"\"\"\r\n+        # Ensure input tensors have gradients enabled\r\n+        if not src.requires_grad:\r\n+            src = src.detach().requires_grad_(True)\r\n+        if not tgt.requires_grad:\r\n+            tgt = tgt.detach().requires_grad_(True)\r\n+            \r\n+        # Log gradient status for debugging\r\n+        if torch.is_grad_enabled():\r\n+            print(f\"Debug - Forward input src requires_grad: {src.requires_grad}\")\r\n+            print(f\"Debug - Forward input tgt requires_grad: {tgt.requires_grad}\")\r\n+            \r\n+        output = super().forward(\r\n+            src, tgt,\r\n+            src_mask=src_mask,\r\n+            tgt_mask=tgt_mask,\r\n+            src_key_padding_mask=src_key_padding_mask,\r\n+            tgt_key_padding_mask=tgt_key_padding_mask\r\n+        )\r\n+        \r\n+        # Verify gradient tracking is maintained\r\n+        if torch.is_grad_enabled():\r\n+            print(f\"Debug - Forward output requires_grad: {output.requires_grad}\")\r\n+            \r\n+        return output\r\n+\r\n+    def _handle_temporal_features(\r\n+        self,\r\n+        x: Tensor,\r\n+        timestamps: torch.Tensor\r\n+    ) -> Tensor:\r\n+        \"\"\"Handle temporal features for sub-hourly data.\"\"\"\r\n+        # Convert tensor timestamps to pandas datetime\r\n+        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n+        \r\n+        # Extract temporal features\r\n+        minutes = torch.tensor(dates.minute.values, device=timestamps.device)\r\n+        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n+        \r\n+        # Process and return features\r\n+        return x  # This should be implemented with actual feature processing\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733265968031,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,345 +119,76 @@\n             src: Tensor,\r\n             src_mask: Optional[Tensor] = None,\r\n             src_key_padding_mask: Optional[Tensor] = None,\r\n         ) -> Tensor:\r\n-            \"\"\"Enhanced encoding with improved gradient handling for checkpointing.\"\"\"\r\n-            # Initial gradient setup\r\n+            \"\"\"Enhanced encoding with proper gradient handling.\"\"\"\r\n+            \r\n+            # Initial gradient setup with function preservation\r\n             if not src.requires_grad:\r\n-                src = src.detach().requires_grad_(True)\r\n+                src = src.clone().detach().requires_grad_(True)\r\n                 \r\n-            # Apply embedding\r\n+            # Apply embedding while preserving gradients\r\n             src = self.encoder_embedding(src)\r\n-            if not src.requires_grad:\r\n-                src = src.detach().requires_grad_(True)\r\n             \r\n-            # Apply short-term pattern recognition\r\n+            # Apply short-term pattern recognition with gradient preservation\r\n             src_conv = src.transpose(1, 2)\r\n             src_conv = self.short_term_conv(src_conv)\r\n             src_conv = src_conv.transpose(1, 2)\r\n             src = src + src_conv\r\n             \r\n-            # Process through encoder layers\r\n+            # Process through encoder layers with gradient-aware checkpointing\r\n             for i, layer in enumerate(self.encoder_layers):\r\n                 if self.use_checkpointing and self.training:\r\n-                    def custom_forward(x_inner, mask=None, key_padding_mask=None):\r\n-                        # Ensure inputs maintain gradients within checkpoint\r\n-                        x_inner = x_inner.detach().requires_grad_(True)\r\n-                        if mask is not None:\r\n-                            mask = mask.detach()\r\n-                        if key_padding_mask is not None:\r\n-                            key_padding_mask = key_padding_mask.detach()\r\n-                        \r\n-                        # Process through layer\r\n-                        out = layer(x_inner, \r\n-                                src_mask=mask, \r\n-                                src_key_padding_mask=key_padding_mask)\r\n-                        \r\n-                        # Ensure output maintains gradients\r\n-                        if not out.requires_grad:\r\n-                            out = out.detach().requires_grad_(True)\r\n-                        return out\r\n+                    def create_custom_forward():\r\n+                        def custom_forward(*inputs):\r\n+                            x = inputs[0]\r\n+                            if not x.requires_grad:\r\n+                                x = x.clone().detach().requires_grad_(True)\r\n+                                \r\n+                            mask = inputs[1] if len(inputs) > 1 else None\r\n+                            key_padding_mask = inputs[2] if len(inputs) > 2 else None\r\n+                            \r\n+                            # Run the layer computation\r\n+                            result = layer(x, src_mask=mask, src_key_padding_mask=key_padding_mask)\r\n+                            \r\n+                            # Ensure we preserve the gradient function\r\n+                            if not result.grad_fn:\r\n+                                result = result.clone()\r\n+                                \r\n+                            return result\r\n+                        return custom_forward\r\n \r\n-                    # Prepare inputs for checkpointing\r\n-                    inputs = [src]\r\n-                    if src_mask is not None:\r\n-                        inputs.append(src_mask)\r\n-                    else:\r\n-                        inputs.append(None)\r\n-                    if src_key_padding_mask is not None:\r\n-                        inputs.append(src_key_padding_mask)\r\n-                    else:\r\n-                        inputs.append(None)\r\n+                    # Create fresh tensor copies to ensure proper gradient tracking\r\n+                    checkpoint_inputs = [\r\n+                        src.clone() if isinstance(src, Tensor) else src,\r\n+                        src_mask.clone() if isinstance(src_mask, Tensor) else src_mask,\r\n+                        src_key_padding_mask.clone() if isinstance(src_key_padding_mask, Tensor) else src_key_padding_mask\r\n+                    ]\r\n                     \r\n-                    # Apply checkpointing with proper gradient handling\r\n                     src = checkpoint(\r\n-                        custom_forward,\r\n-                        *inputs,\r\n+                        create_custom_forward(),\r\n+                        *checkpoint_inputs,\r\n                         use_reentrant=False,\r\n                         preserve_rng_state=True\r\n-                    )\r\n+                    ) # type: ignore\r\n                     \r\n-                    # Double-check gradient state after checkpoint\r\n-                    if not src.requires_grad:\r\n-                        src = src.detach().requires_grad_(True)\r\n+                    # Ensure gradient tracking is maintained\r\n+                    if not src.grad_fn:\r\n+                        src = src.clone()\r\n                 else:\r\n-                    # Standard forward pass\r\n                     src = layer(\r\n                         src,\r\n                         src_mask=src_mask,\r\n                         src_key_padding_mask=src_key_padding_mask\r\n                     )\r\n-                    \r\n-                # Log gradient state for debugging\r\n-                print(f\"Debug - Layer {i} output requires_grad: {src.requires_grad}\")\r\n-                if hasattr(src, 'grad_fn'):\r\n-                    print(f\"Debug - Layer {i} grad_fn: {type(src.grad_fn).__name__}\")\r\n-            \r\n-            return src\r\n-\r\n-    def _adjust_attention_for_resolution(\r\n-        self,\r\n-        attention_weights: Tensor,\r\n-        resolution_minutes: int\r\n-    ) -> Tensor:\r\n-        \"\"\"Adjust attention weights for sub-hourly patterns with gradient tracking.\"\"\"\r\n-        sequence_length = attention_weights.size(-1)\r\n-        recent_steps = min(self.steps_per_hour * 2, sequence_length)\r\n-        \r\n-        # Create decaying weights with gradients\r\n-        decay = torch.exp(torch.arange(\r\n-            recent_steps, 0, -1, \r\n-            device=attention_weights.device, \r\n-            dtype=attention_weights.dtype\r\n-        ) * -0.1).requires_grad_(True)\r\n-        \r\n-        # Concatenate with ones tensor\r\n-        ones = torch.ones(\r\n-            sequence_length - recent_steps,\r\n-            device=attention_weights.device,\r\n-            dtype=attention_weights.dtype,\r\n-            requires_grad=True\r\n-        )\r\n-        decay = torch.cat([ones, decay])\r\n-        \r\n-        # Apply decay to attention weights\r\n-        return attention_weights * decay.unsqueeze(0).unsqueeze(0)\r\n-\r\n-    @classmethod\r\n-    def get_resolution_type(cls) -> TimeInterval:\r\n-        \"\"\"Get the time interval type for sub-hourly transformer.\"\"\"\r\n-        return TimeInterval.FIFTEEN_MIN\r\n-\r\n-    def forward(\r\n-        self,\r\n-        src: Tensor,\r\n-        tgt: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        tgt_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-        tgt_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Forward pass with gradient tracking verification.\"\"\"\r\n-        # Ensure input tensors have gradients enabled\r\n-        if not src.requires_grad:\r\n-            src = src.detach().requires_grad_(True)\r\n-        if not tgt.requires_grad:\r\n-            tgt = tgt.detach().requires_grad_(True)\r\n-            \r\n-        # Log gradient status for debugging\r\n-        if torch.is_grad_enabled():\r\n-            print(f\"Debug - Forward input src requires_grad: {src.requires_grad}\")\r\n-            print(f\"Debug - Forward input tgt requires_grad: {tgt.requires_grad}\")\r\n-            \r\n-        output = super().forward(\r\n-            src, tgt,\r\n-            src_mask=src_mask,\r\n-            tgt_mask=tgt_mask,\r\n-            src_key_padding_mask=src_key_padding_mask,\r\n-            tgt_key_padding_mask=tgt_key_padding_mask\r\n-        )\r\n-        \r\n-        # Verify gradient tracking is maintained\r\n-        if torch.is_grad_enabled():\r\n-            print(f\"Debug - Forward output requires_grad: {output.requires_grad}\")\r\n-            \r\n-        return output\r\n-\r\n-    def _handle_temporal_features(\r\n-        self,\r\n-        x: Tensor,\r\n-        timestamps: torch.Tensor\r\n-    ) -> Tensor:\r\n-        \"\"\"Handle temporal features for sub-hourly data.\"\"\"\r\n-        # Convert tensor timestamps to pandas datetime\r\n-        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n-        \r\n-        # Extract temporal features\r\n-        minutes = torch.tensor(dates.minute.values, device=timestamps.device)\r\n-        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n-        \r\n-        # Process and return features\r\n-        return x  # This should be implemented with actual feature processing\n-from typing import Callable, Dict, Any, Optional, cast\r\n-import torch\r\n-from torch import nn, Tensor\r\n-import pandas as pd\r\n-\r\n-from data_loading.types.interval_types import TimeInterval\r\n-from .base_resolution_transformer import BaseResolutionTransformer\r\n-from models.components.layers import EncoderLayer, DecoderLayer\r\n-from torch.utils.checkpoint import checkpoint\r\n-\r\n-class SubhourlyTransformer(BaseResolutionTransformer):\r\n-    \"\"\"Transformer optimized for sub-hourly predictions (â‰¤60 minutes).\"\"\"\r\n-    \r\n-    def __init__(self, config: Dict[str, Any]):\r\n-        \"\"\"\r\n-        Initialize SubhourlyTransformer with proper layer creation.\r\n-        \r\n-        Args:\r\n-            config: Configuration dictionary containing model parameters\r\n-        \"\"\"\r\n-        # Ensure output_features is in config\r\n-        if 'output_features' not in config:\r\n-            config['output_features'] = 1  # Default to 1 if not specified\r\n-        \r\n-        # Call parent class initialization first\r\n-        super().__init__(config)\r\n-        \r\n-        # Store configuration parameters\r\n-        self.n_encoder_layers = config.get('n_encoder_layers', 6)\r\n-        self.n_decoder_layers = config.get('n_decoder_layers', 6)\r\n-        self.d_model = config.get('d_model', 512)\r\n-        self.n_heads = config.get('n_heads', 8)\r\n-        self.d_ff = config.get('d_ff', 2048)\r\n-        self.dropout = config.get('dropout', 0.1)\r\n-        self.use_checkpointing = config.get('use_checkpointing', True)  # Add explicit flag\r\n-        \r\n-        # Calculate steps per hour based on resolution\r\n-        self.minutes_per_step = config.get('forecast_resolution_minutes', 15)\r\n-        self.steps_per_hour = 60 // self.minutes_per_step\r\n-        \r\n-        # Create encoder and decoder layers\r\n-        self.encoder_layers = self._create_encoder_layers()\r\n-        self.decoder_layers = self._create_decoder_layers()\r\n-        \r\n-        # Initialize short-term pattern recognition\r\n-        self.short_term_conv = nn.Conv1d(\r\n-            in_channels=self.d_model,\r\n-            out_channels=self.d_model,\r\n-            kernel_size=3,\r\n-            padding=1,\r\n-            groups=self.d_model  # Depthwise convolution for efficiency\r\n-        )\r\n-        \r\n-        print(\"DEBUG: SubhourlyTransformer initialization complete\")\r\n-        print(f\"DEBUG: Configuration loaded - n_encoder_layers: {self.n_encoder_layers}, d_model: {self.d_model}\")\r\n-\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create encoder layers optimized for sub-hourly patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            EncoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",  # Use standard attention for short sequences\r\n-                activation=\"gelu\"  # GELU for better gradient flow\r\n-            ) for _ in range(self.n_encoder_layers)\r\n-        ])\r\n-\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        \"\"\"Create decoder layers optimized for sub-hourly patterns.\"\"\"\r\n-        return nn.ModuleList([\r\n-            DecoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"gelu\"\r\n-            ) for _ in range(self.n_decoder_layers)\r\n-        ])\r\n-\r\n-    def _create_custom_forward(self, layer: nn.Module) -> Callable[..., Tensor]:\r\n-        \"\"\"Creates a type-safe forward function for checkpointing.\r\n-        \r\n-        This helper method ensures proper type handling for the checkpointed function.\r\n-        The returned function is guaranteed to return a Tensor, maintaining type safety.\r\n-        \r\n-        Args:\r\n-            layer: The encoder layer to be wrapped\r\n-            \r\n-        Returns:\r\n-            A callable that properly handles types for checkpointing\r\n-        \"\"\"\r\n-        def custom_forward(*inputs: Any) -> Tensor:\r\n-            # Validate inputs\r\n-            if not inputs or not isinstance(inputs[0], torch.Tensor):\r\n-                raise ValueError(\"First input must be a tensor\")\r\n-            \r\n-            x = inputs[0]\r\n-            if not x.requires_grad:\r\n-                x = x.detach().requires_grad_(True)\r\n                 \r\n-            # Handle optional mask inputs\r\n-            mask = inputs[1] if len(inputs) > 1 else None\r\n-            key_padding_mask = inputs[2] if len(inputs) > 2 else None\r\n-            \r\n-            # Process through layer and ensure tensor output\r\n-            output = layer(x, src_mask=mask, src_key_padding_mask=key_padding_mask)\r\n-            if not isinstance(output, torch.Tensor):\r\n-                raise ValueError(\"Layer output must be a tensor\")\r\n+                # Debug logging for gradient tracking\r\n+                # print(f\"Debug - Layer {i} output requires_grad: {src.requires_grad}\")\r\n+                # print(f\"Debug - Layer {i} grad_fn: {type(src.grad_fn).__name__ if src.grad_fn else 'None'}\")\r\n+                # print(f\"Debug - Layer {i} is_leaf: {src.is_leaf}\")\r\n                 \r\n-            return output\r\n-            \r\n-        return custom_forward\r\n+            return src\r\n \r\n-    def encode(\r\n-        self,\r\n-        src: Tensor,\r\n-        src_mask: Optional[Tensor] = None,\r\n-        src_key_padding_mask: Optional[Tensor] = None,\r\n-    ) -> Tensor:\r\n-        \"\"\"Enhanced encoding with type-safe gradient tracking.\r\n-        \r\n-        Args:\r\n-            src: Source sequence tensor\r\n-            src_mask: Optional attention mask\r\n-            src_key_padding_mask: Optional padding mask\r\n-            \r\n-        Returns:\r\n-            Encoded tensor with proper gradient tracking\r\n-        \"\"\"\r\n-        # Ensure input tensors have gradients enabled\r\n-        if not src.requires_grad:\r\n-            src = src.detach().requires_grad_(True)\r\n-            \r\n-        # Apply embedding with type safety\r\n-        src = self.encoder_embedding(src)\r\n-        if not src.requires_grad:\r\n-            src = src.detach().requires_grad_(True)\r\n-        \r\n-        # Apply short-term pattern recognition\r\n-        src_conv = src.transpose(1, 2)  # [batch, d_model, seq_len]\r\n-        src_conv = self.short_term_conv(src_conv)\r\n-        src_conv = src_conv.transpose(1, 2)  # [batch, seq_len, d_model]\r\n-        \r\n-        # Combine with original embeddings\r\n-        src = src + src_conv\r\n-        \r\n-        # Process through encoder layers with type-safe checkpointing\r\n-        for layer in self.encoder_layers:\r\n-            if self.use_checkpointing and self.training:\r\n-                # Create type-safe forward function\r\n-                custom_forward = self._create_custom_forward(layer)\r\n-                \r\n-                # Apply checkpointing with explicit type casting\r\n-                checkpoint_output = checkpoint(\r\n-                    custom_forward,\r\n-                    src, src_mask, src_key_padding_mask,\r\n-                    use_reentrant=False,\r\n-                    preserve_rng_state=True\r\n-                )\r\n-                \r\n-                # Ensure output maintains type safety\r\n-                src = cast(Tensor, checkpoint_output)\r\n-                if not src.requires_grad:\r\n-                    src = src.detach().requires_grad_(True)\r\n-            else:\r\n-                src = layer(\r\n-                    src,\r\n-                    src_mask=src_mask,\r\n-                    src_key_padding_mask=src_key_padding_mask\r\n-                )\r\n-        \r\n-        # Validate final output\r\n-        if not isinstance(src, torch.Tensor):\r\n-            raise ValueError(\"Encoder output must be a tensor\")\r\n-            \r\n-        return src\r\n-\r\n     def _adjust_attention_for_resolution(\r\n         self,\r\n         attention_weights: Tensor,\r\n         resolution_minutes: int\r\n@@ -506,11 +237,11 @@\n         if not tgt.requires_grad:\r\n             tgt = tgt.detach().requires_grad_(True)\r\n             \r\n         # Log gradient status for debugging\r\n-        if torch.is_grad_enabled():\r\n-            print(f\"Debug - Forward input src requires_grad: {src.requires_grad}\")\r\n-            print(f\"Debug - Forward input tgt requires_grad: {tgt.requires_grad}\")\r\n+        # if torch.is_grad_enabled():\r\n+        #     print(f\"Debug - Forward input src requires_grad: {src.requires_grad}\")\r\n+        #     print(f\"Debug - Forward input tgt requires_grad: {tgt.requires_grad}\")\r\n             \r\n         output = super().forward(\r\n             src, tgt,\r\n             src_mask=src_mask,\r\n@@ -519,10 +250,10 @@\n             tgt_key_padding_mask=tgt_key_padding_mask\r\n         )\r\n         \r\n         # Verify gradient tracking is maintained\r\n-        if torch.is_grad_enabled():\r\n-            print(f\"Debug - Forward output requires_grad: {output.requires_grad}\")\r\n+        # if torch.is_grad_enabled():\r\n+        #     print(f\"Debug - Forward output requires_grad: {output.requires_grad}\")\r\n             \r\n         return output\r\n \r\n     def _handle_temporal_features(\r\n"
                }
            ],
            "date": 1733264330899,
            "name": "Commit-0",
            "content": "from typing import Dict, Any, Optional\r\nimport torch\r\nfrom torch import nn, Tensor\r\nimport pandas as pd\r\n\r\nfrom data_loading.types.interval_types import TimeInterval\r\nfrom .base_resolution_transformer import BaseResolutionTransformer\r\nfrom models.components.layers import EncoderLayer, DecoderLayer\r\n\r\nclass SubhourlyTransformer(BaseResolutionTransformer):\r\n    \"\"\"Transformer optimized for sub-hourly predictions (â‰¤60 minutes).\"\"\"\r\n    \r\n    def __init__(self, config: Dict[str, Any]):\r\n        \"\"\"\r\n        Initialize SubhourlyTransformer with proper layer creation.\r\n        \r\n        Args:\r\n            config: Configuration dictionary containing model parameters\r\n        \"\"\"\r\n        # Ensure output_features is in config\r\n        if 'output_features' not in config:\r\n            config['output_features'] = 1  # Default to 1 if not specified\r\n        \r\n        # Call parent class initialization first\r\n        super().__init__(config)\r\n        \r\n        # Store configuration parameters\r\n        self.n_encoder_layers = config.get('n_encoder_layers', 6)\r\n        self.n_decoder_layers = config.get('n_decoder_layers', 6)\r\n        self.d_model = config.get('d_model', 512)\r\n        self.n_heads = config.get('n_heads', 8)\r\n        self.d_ff = config.get('d_ff', 2048)\r\n        self.dropout = config.get('dropout', 0.1)\r\n        \r\n        # Calculate steps per hour based on resolution\r\n        self.minutes_per_step = config.get('forecast_resolution_minutes', 15)\r\n        self.steps_per_hour = 60 // self.minutes_per_step\r\n        \r\n        # Create encoder layers\r\n        self.encoder_layers = self._create_encoder_layers()\r\n        \r\n        # Create decoder layers\r\n        self.decoder_layers = self._create_decoder_layers()\r\n        \r\n        # Initialize short-term pattern recognition\r\n        self.short_term_conv = nn.Conv1d(\r\n            in_channels=self.d_model,\r\n            out_channels=self.d_model,\r\n            kernel_size=3,\r\n            padding=1,\r\n            groups=self.d_model  # Depthwise convolution for efficiency\r\n        )\r\n        \r\n        print(\"DEBUG: SubhourlyTransformer initialization complete\")\r\n        print(f\"DEBUG: Configuration loaded - n_encoder_layers: {self.n_encoder_layers}, d_model: {self.d_model}\")\r\n\r\n    def get_input_dims(self) -> int:\r\n        \"\"\"Get input dimensions for subhourly transformer.\"\"\"\r\n        return self.input_features\r\n\r\n    def get_output_dims(self) -> int:\r\n        \"\"\"Get output dimensions for subhourly transformer.\"\"\"\r\n        return self.output_features\r\n\r\n    def _create_encoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create encoder layers optimized for sub-hourly patterns.\"\"\"\r\n        return nn.ModuleList([\r\n            EncoderLayer(\r\n                d_model=self.d_model,\r\n                n_heads=self.n_heads,\r\n                d_ff=self.d_ff,\r\n                dropout=self.dropout,\r\n                attention_type=\"standard\",  # Use standard attention for short sequences\r\n                activation=\"gelu\"  # GELU for better gradient flow\r\n            ) for _ in range(self.n_encoder_layers)\r\n        ])\r\n\r\n    def _create_decoder_layers(self) -> nn.ModuleList:\r\n        \"\"\"Create decoder layers optimized for sub-hourly patterns.\"\"\"\r\n        return nn.ModuleList([\r\n            DecoderLayer(\r\n                d_model=self.d_model,\r\n                n_heads=self.n_heads,\r\n                d_ff=self.d_ff,\r\n                dropout=self.dropout,\r\n                attention_type=\"standard\",\r\n                activation=\"gelu\"\r\n            ) for _ in range(self.n_decoder_layers)\r\n        ])\r\n\r\n    def encode(\r\n        self,\r\n        src: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Enhanced encoding with short-term pattern recognition.\"\"\"\r\n        # Standard embedding\r\n        src = self.encoder_embedding(src)\r\n        \r\n        # Apply short-term pattern recognition\r\n        src_conv = src.transpose(1, 2)  # [batch, d_model, seq_len]\r\n        src_conv = self.short_term_conv(src_conv)\r\n        src_conv = src_conv.transpose(1, 2)  # [batch, seq_len, d_model]\r\n        \r\n        # Combine with original embeddings\r\n        src = src + src_conv\r\n        \r\n        # Pass through encoder layers\r\n        for layer in self.encoder_layers:\r\n            src = layer(\r\n                src,\r\n                src_mask=src_mask,\r\n                src_key_padding_mask=src_key_padding_mask\r\n            )\r\n        return src\r\n\r\n    def _adjust_attention_for_resolution(\r\n        self,\r\n        attention_weights: Tensor,\r\n        resolution_minutes: int\r\n    ) -> Tensor:\r\n        \"\"\"Adjust attention weights for sub-hourly patterns.\"\"\"\r\n        # Enhance attention to recent time steps\r\n        sequence_length = attention_weights.size(-1)\r\n        recent_steps = min(self.steps_per_hour * 2, sequence_length)  # Last 2 hours\r\n        \r\n        # Create decaying weights for recent time steps\r\n        decay = torch.exp(torch.arange(recent_steps, 0, -1, device=attention_weights.device) * -0.1)\r\n        decay = torch.cat([torch.ones(sequence_length - recent_steps, device=attention_weights.device), decay])\r\n        \r\n        # Apply decay to attention weights\r\n        return attention_weights * decay.unsqueeze(0).unsqueeze(0)\r\n\r\n    @classmethod\r\n    def get_resolution_type(cls) -> TimeInterval:\r\n        \"\"\"Get the time interval type for sub-hourly transformer.\"\"\"\r\n        return TimeInterval.FIFTEEN_MIN\r\n\r\n    def forward(\r\n        self,\r\n        src: Tensor,\r\n        tgt: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None,\r\n        src_key_padding_mask: Optional[Tensor] = None,\r\n        tgt_key_padding_mask: Optional[Tensor] = None,\r\n    ) -> Tensor:\r\n        \"\"\"Forward pass with enhanced sub-hourly pattern recognition.\"\"\"\r\n        output = super().forward(\r\n            src, tgt,\r\n            src_mask=src_mask,\r\n            tgt_mask=tgt_mask,\r\n            src_key_padding_mask=src_key_padding_mask,\r\n            tgt_key_padding_mask= tgt_key_padding_mask\r\n        )\r\n        \r\n        # Additional processing for sub-hourly predictions if needed\r\n        if hasattr(self, 'output_processing'):\r\n            output = self.output_processing(output)\r\n            \r\n        return output\r\n\r\n    def _handle_temporal_features(\r\n        self,\r\n        x: Tensor,\r\n        timestamps: torch.Tensor\r\n    ) -> Tensor:\r\n        \"\"\"Handle temporal features for sub-hourly data.\"\"\"\r\n        # Convert tensor timestamps to pandas datetime\r\n        dates = pd.to_datetime(timestamps.cpu().numpy(), unit='s')\r\n        \r\n        # Extract temporal features\r\n        minutes = torch.tensor(dates.minute.values, device=timestamps.device)\r\n        hours = torch.tensor(dates.hour.values, device=timestamps.device)\r\n        \r\n        # Process and return features\r\n        return x  # This should be implemented with actual feature processing"
        }
    ]
}