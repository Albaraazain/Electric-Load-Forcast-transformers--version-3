{
    "sourceFile": "models/architectures/transformers/timeseries_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1733090227746,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733090465161,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,173 @@\n+from typing import Dict, Any, List, Optional, Tuple, Union\r\n+import torch\r\n+from torch import nn, Tensor\r\n+\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.components.embeddings import TotalEmbedding\r\n+from models.base.base_model import BaseModel  # Add this import\r\n+\r\n+\r\n+def average_attention_scores(attention_scores: List[torch.Tensor]) -> torch.Tensor:\r\n+    \"\"\"Calculate average attention scores across layers.\"\"\"\r\n+    return torch.mean(torch.stack(attention_scores), dim=0)\r\n+\r\n+\r\n+@ModelFactory.register(ModelType.TIME_SERIES_TRANSFORMER)\r\n+class TimeSeriesTransformer(BaseModel):  # Change parent class to BaseModel\r\n+    \"\"\"Transformer model specifically designed for time series forecasting.\"\"\"\r\n+\r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        super().__init__(config)  # Call BaseModel's __init__\r\n+        \r\n+        # Extract configuration\r\n+        d_model = config['d_model']\r\n+        n_heads = config['n_heads']\r\n+        n_encoder_layers = config['n_encoder_layers']\r\n+        n_decoder_layers = config['n_decoder_layers']\r\n+        d_ff = config['d_ff']\r\n+        dropout = config['dropout']\r\n+        input_features = config['input_features']\r\n+\r\n+        # Initialize transformer\r\n+        self.transformer = nn.Transformer(\r\n+            d_model=d_model,\r\n+            nhead=n_heads,\r\n+            num_encoder_layers=n_encoder_layers,\r\n+            num_decoder_layers=n_decoder_layers,\r\n+            dim_feedforward=d_ff,\r\n+            dropout=dropout,\r\n+            batch_first=True  # Important for time series data\r\n+        )\r\n+\r\n+        # Initialize embeddings\r\n+        self.encoder_embedding = TotalEmbedding(\r\n+            d_model=d_model,\r\n+            value_features=1,  # Main time series value\r\n+            time_features=input_features - 1,  # Additional features\r\n+            dropout=dropout\r\n+        )\r\n+        self.decoder_embedding = TotalEmbedding(\r\n+            d_model=d_model,\r\n+            value_features=1,\r\n+            time_features=input_features - 1,\r\n+            dropout=dropout\r\n+        )\r\n+\r\n+        # Output projection\r\n+        self.projection = nn.Linear(d_model, 1, bias=True)\r\n+        self.relu = nn.ReLU()\r\n+\r\n+    def forward(\r\n+        self,\r\n+        x_enc: Tensor,\r\n+        x_dec: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        tgt_mask: Optional[Tensor] = None\r\n+    ) -> Tensor:\r\n+        \"\"\"\r\n+        Execute the model for the given input.\r\n+\r\n+        Args:\r\n+            x_enc: Raw input for encoder [batch_size, seq_enc_length, features]\r\n+            x_dec: Raw input for decoder [batch_size, seq_dec_length, features]\r\n+            src_mask: Optional mask for encoder\r\n+            tgt_mask: Optional mask for decoder (usually needed for autoregressive prediction)\r\n+\r\n+        Returns:\r\n+            Predictions tensor of shape [batch_size, seq_dec_length, 1]\r\n+        \"\"\"\r\n+        # Apply embeddings\r\n+        enc_embedding = self.encoder_embedding(x_enc)\r\n+        dec_embedding = self.decoder_embedding(x_dec)\r\n+\r\n+        # Apply transformer\r\n+        out = self.transformer(\r\n+            enc_embedding,\r\n+            dec_embedding,\r\n+            src_mask=src_mask,\r\n+            tgt_mask=tgt_mask\r\n+        )\r\n+\r\n+        # Project to output dimension\r\n+        out = self.projection(self.relu(out))\r\n+        return out\r\n+\r\n+    def get_cross_attention_scores(self) -> torch.Tensor:\r\n+        \"\"\"Get average cross-attention scores across all decoder layers.\"\"\"\r\n+        return average_attention_scores([\r\n+            layer.multihead_attn.attention_weights\r\n+            for layer in self.transformer.decoder.layers\r\n+        ])\r\n+\r\n+    def get_self_attention_scores(self) -> torch.Tensor:\r\n+        \"\"\"Get average self-attention scores across all decoder layers.\"\"\"\r\n+        return average_attention_scores([\r\n+            layer.self_attn.attention_weights\r\n+            for layer in self.transformer.decoder.layers\r\n+        ])\r\n+\r\n+    def create_subsequent_mask(self, size: int) -> Tensor:\r\n+        \"\"\"Create causal mask to prevent attending to future tokens.\"\"\"\r\n+        return self.transformer.generate_square_subsequent_mask(size)\r\n+\r\n+    def predict_sequence(\r\n+        self,\r\n+        src: Tensor,\r\n+        forecast_length: int,\r\n+        return_attention: bool = False\r\n+    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:\r\n+        \"\"\"\r\n+        Generate multi-step predictions.\r\n+\r\n+        Args:\r\n+            src: Source sequence [batch_size, seq_length, features]\r\n+            forecast_length: Number of steps to predict\r\n+            return_attention: Whether to return attention weights\r\n+\r\n+        Returns:\r\n+            predictions: Predicted sequence\r\n+            attention_weights: Optional attention weights if return_attention=True\r\n+        \"\"\"\r\n+        self.eval()\r\n+        device = src.device\r\n+\r\n+        with torch.no_grad():\r\n+            # Initial embedding\r\n+            enc_out = self.encoder_embedding(src)\r\n+\r\n+            # Start with the last value of source sequence\r\n+            dec_input = src[:, -1:, :]\r\n+            predictions = []\r\n+            attention_weights = []\r\n+\r\n+            # Generate predictions autoregressively\r\n+            for i in range(forecast_length):\r\n+                # Create appropriate masking\r\n+                tgt_mask = self.create_subsequent_mask(dec_input.size(1)).to(device)\r\n+\r\n+                # Get decoder embedding\r\n+                dec_emb = self.decoder_embedding(dec_input)\r\n+\r\n+                # Get transformer output\r\n+                out = self.transformer(\r\n+                    enc_out,\r\n+                    dec_emb,\r\n+                    tgt_mask=tgt_mask\r\n+                )\r\n+\r\n+                # Project and store prediction\r\n+                pred = self.projection(self.relu(out[:, -1:, :]))\r\n+                predictions.append(pred)\r\n+\r\n+                if return_attention:\r\n+                    attention_weights.append(self.get_cross_attention_scores())\r\n+\r\n+                # Update decoder input for next step\r\n+                dec_input = torch.cat([dec_input, pred], dim=1)\r\n+\r\n+            predictions = torch.cat(predictions, dim=1)\r\n+\r\n+            if return_attention:\r\n+                return predictions, torch.stack(attention_weights)\r\n+            return predictions\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733090227746,
            "name": "Commit-0",
            "content": "from typing import Dict, Any, List, Optional, Tuple\r\nimport torch\r\nfrom torch import nn, Tensor\r\n\r\nfrom models.registry.factory import ModelFactory\r\nfrom models.registry.model_types import ModelType\r\nfrom models.components.embeddings import TotalEmbedding\r\n\r\n\r\ndef average_attention_scores(attention_scores: List[torch.Tensor]) -> torch.Tensor:\r\n    \"\"\"Calculate average attention scores across layers.\"\"\"\r\n    return torch.mean(torch.stack(attention_scores), dim=0)\r\n\r\n\r\n@ModelFactory.register(ModelType.TIME_SERIES_TRANSFORMER)\r\nclass TimeSeriesTransformer(nn.Module):\r\n    \"\"\"Transformer model specifically designed for time series forecasting.\"\"\"\r\n\r\n    def __init__(self, config: Dict[str, Any]):\r\n        super().__init__()\r\n        \r\n        # Extract configuration\r\n        d_model = config['d_model']\r\n        n_heads = config['n_heads']\r\n        n_encoder_layers = config['n_encoder_layers']\r\n        n_decoder_layers = config['n_decoder_layers']\r\n        d_ff = config['d_ff']\r\n        dropout = config['dropout']\r\n        input_features = config['input_features']\r\n\r\n        # Initialize transformer\r\n        self.transformer = nn.Transformer(\r\n            d_model=d_model,\r\n            nhead=n_heads,\r\n            num_encoder_layers=n_encoder_layers,\r\n            num_decoder_layers=n_decoder_layers,\r\n            dim_feedforward=d_ff,\r\n            dropout=dropout,\r\n            batch_first=True  # Important for time series data\r\n        )\r\n\r\n        # Initialize embeddings\r\n        self.encoder_embedding = TotalEmbedding(\r\n            d_model=d_model,\r\n            value_features=1,  # Main time series value\r\n            time_features=input_features - 1,  # Additional features\r\n            dropout=dropout\r\n        )\r\n        self.decoder_embedding = TotalEmbedding(\r\n            d_model=d_model,\r\n            value_features=1,\r\n            time_features=input_features - 1,\r\n            dropout=dropout\r\n        )\r\n\r\n        # Output projection\r\n        self.projection = nn.Linear(d_model, 1, bias=True)\r\n        self.relu = nn.ReLU()\r\n\r\n    def forward(\r\n        self,\r\n        x_enc: Tensor,\r\n        x_dec: Tensor,\r\n        src_mask: Optional[Tensor] = None,\r\n        tgt_mask: Optional[Tensor] = None\r\n    ) -> Tensor:\r\n        \"\"\"\r\n        Execute the model for the given input.\r\n\r\n        Args:\r\n            x_enc: Raw input for encoder [batch_size, seq_enc_length, features]\r\n            x_dec: Raw input for decoder [batch_size, seq_dec_length, features]\r\n            src_mask: Optional mask for encoder\r\n            tgt_mask: Optional mask for decoder (usually needed for autoregressive prediction)\r\n\r\n        Returns:\r\n            Predictions tensor of shape [batch_size, seq_dec_length, 1]\r\n        \"\"\"\r\n        # Apply embeddings\r\n        enc_embedding = self.encoder_embedding(x_enc)\r\n        dec_embedding = self.decoder_embedding(x_dec)\r\n\r\n        # Apply transformer\r\n        out = self.transformer(\r\n            enc_embedding,\r\n            dec_embedding,\r\n            src_mask=src_mask,\r\n            tgt_mask=tgt_mask\r\n        )\r\n\r\n        # Project to output dimension\r\n        out = self.projection(self.relu(out))\r\n        return out\r\n\r\n    def get_cross_attention_scores(self) -> torch.Tensor:\r\n        \"\"\"Get average cross-attention scores across all decoder layers.\"\"\"\r\n        return average_attention_scores([\r\n            layer.multihead_attn.attention_weights\r\n            for layer in self.transformer.decoder.layers\r\n        ])\r\n\r\n    def get_self_attention_scores(self) -> torch.Tensor:\r\n        \"\"\"Get average self-attention scores across all decoder layers.\"\"\"\r\n        return average_attention_scores([\r\n            layer.self_attn.attention_weights\r\n            for layer in self.transformer.decoder.layers\r\n        ])\r\n\r\n    def create_subsequent_mask(self, size: int) -> Tensor:\r\n        \"\"\"Create causal mask to prevent attending to future tokens.\"\"\"\r\n        return self.transformer.generate_square_subsequent_mask(size)\r\n\r\n    def predict_sequence(\r\n        self,\r\n        src: Tensor,\r\n        forecast_length: int,\r\n        return_attention: bool = False\r\n    ) -> Union[Tensor, Tuple[Tensor, Tensor]]:\r\n        \"\"\"\r\n        Generate multi-step predictions.\r\n\r\n        Args:\r\n            src: Source sequence [batch_size, seq_length, features]\r\n            forecast_length: Number of steps to predict\r\n            return_attention: Whether to return attention weights\r\n\r\n        Returns:\r\n            predictions: Predicted sequence\r\n            attention_weights: Optional attention weights if return_attention=True\r\n        \"\"\"\r\n        self.eval()\r\n        device = src.device\r\n\r\n        with torch.no_grad():\r\n            # Initial embedding\r\n            enc_out = self.encoder_embedding(src)\r\n\r\n            # Start with the last value of source sequence\r\n            dec_input = src[:, -1:, :]\r\n            predictions = []\r\n            attention_weights = []\r\n\r\n            # Generate predictions autoregressively\r\n            for i in range(forecast_length):\r\n                # Create appropriate masking\r\n                tgt_mask = self.create_subsequent_mask(dec_input.size(1)).to(device)\r\n\r\n                # Get decoder embedding\r\n                dec_emb = self.decoder_embedding(dec_input)\r\n\r\n                # Get transformer output\r\n                out = self.transformer(\r\n                    enc_out,\r\n                    dec_emb,\r\n                    tgt_mask=tgt_mask\r\n                )\r\n\r\n                # Project and store prediction\r\n                pred = self.projection(self.relu(out[:, -1:, :]))\r\n                predictions.append(pred)\r\n\r\n                if return_attention:\r\n                    attention_weights.append(self.get_cross_attention_scores())\r\n\r\n                # Update decoder input for next step\r\n                dec_input = torch.cat([dec_input, pred], dim=1)\r\n\r\n            predictions = torch.cat(predictions, dim=1)\r\n\r\n            if return_attention:\r\n                return predictions, torch.stack(attention_weights)\r\n            return predictions"
        }
    ]
}