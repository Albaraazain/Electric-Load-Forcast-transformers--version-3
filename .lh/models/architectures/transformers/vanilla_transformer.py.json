{
    "sourceFile": "models/architectures/transformers/vanilla_transformer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1733005282427,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733005946617,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,67 @@\n+import torch\r\n+from torch import nn\r\n+from typing import Dict, Any\r\n+\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.architectures.transformers.base_transformer import BaseTransformer\r\n+from models.components.layers import EncoderLayer, DecoderLayer\r\n+\r\n+@ModelFactory.register(ModelType.VANILLA_TRANSFORMER)\r\n+class VanillaTransformer(BaseTransformer):\r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        super().__init__(config)\r\n+        \r\n+        self.input_dim = config['input_features']\r\n+        self.output_dim = config['input_features']  # Match input features for reconstruction\r\n+        self.seq_length = config.get('transformer_labels_count', 12)\r\n+        \r\n+        self.input_embedding = nn.Linear(self.input_dim, self.d_model)\r\n+        self.positional_encoding = nn.Parameter(\r\n+            torch.randn(1, config.get('max_seq_length', 1000), self.d_model)\r\n+        )\r\n+        \r\n+        # Simplified output projection\r\n+        self.output_projection = nn.Linear(self.d_model, self.output_dim)\r\n+\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        return nn.ModuleList([\r\n+            EncoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",\r\n+                activation=\"relu\"\r\n+            ) for _ in range(self.n_encoder_layers)\r\n+        ])\r\n+\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        return nn.ModuleList([\r\n+            DecoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",\r\n+                activation=\"relu\"\r\n+            ) for _ in range(self.n_decoder_layers)\r\n+        ])\r\n+\r\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n+        batch_size, seq_length, _ = x.shape\r\n+\r\n+        # Embed input\r\n+        x = self.input_embedding(x)\r\n+        \r\n+        # Add positional encoding\r\n+        x = x + self.positional_encoding[:, :seq_length, :]\r\n+        \r\n+        # Pass through encoder layers\r\n+        for encoder_layer in self.encoder_layers:\r\n+            x = encoder_layer(x)\r\n+        \r\n+        # Direct projection to output dimension\r\n+        x = self.output_projection(x)\r\n+        \r\n+        return x\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733005954737,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,97 +63,5 @@\n         \r\n         # Direct projection to output dimension\r\n         x = self.output_projection(x)\r\n         \r\n-        return x\n-import torch\r\n-from torch import nn\r\n-from typing import Dict, Any\r\n-\r\n-from models.registry.factory import ModelFactory\r\n-from models.registry.model_types import ModelType\r\n-from models.architectures.transformers.base_transformer import BaseTransformer\r\n-from models.components.layers import EncoderLayer, DecoderLayer\r\n-\r\n-# Ensure this model is imported and registered early in your application startup\r\n-@ModelFactory.register(ModelType.VANILLA_TRANSFORMER)\r\n-class VanillaTransformer(BaseTransformer):\r\n-    \"\"\"Standard transformer implementation.\"\"\"\r\n-\r\n-    def __init__(self, config: Dict[str, Any]):\r\n-        super().__init__(config)\r\n-        \r\n-        # Use the parameters from ModelConfig\r\n-        self.input_dim = config['input_features']\r\n-        self.output_dim = config.get('output_features', 1)\r\n-        self.seq_length = config.get('transformer_labels_count', 12)  # Add this line\r\n-        \r\n-        # Linear layers for input and output transformations\r\n-        self.input_embedding = nn.Linear(self.input_dim, self.d_model)\r\n-        self.positional_encoding = nn.Parameter(\r\n-            torch.randn(1, config.get('max_seq_length', 1000), self.d_model)\r\n-        )\r\n-        \r\n-        # Encoder and decoder layers are created by parent class\r\n-        \r\n-        # Output projection with sequence length consideration\r\n-        self.output_projection = nn.Linear(self.d_model * 24, self.seq_length * self.output_dim)\r\n-\r\n-    def _create_encoder_layers(self) -> nn.ModuleList:\r\n-        return nn.ModuleList([\r\n-            EncoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"relu\"\r\n-            ) for _ in range(self.n_encoder_layers)\r\n-        ])\r\n-\r\n-    def _create_decoder_layers(self) -> nn.ModuleList:\r\n-        return nn.ModuleList([\r\n-            DecoderLayer(\r\n-                d_model=self.d_model,\r\n-                n_heads=self.n_heads,\r\n-                d_ff=self.d_ff,\r\n-                dropout=self.dropout,\r\n-                attention_type=\"standard\",\r\n-                activation=\"relu\"\r\n-            ) for _ in range(self.n_decoder_layers)\r\n-        ])\r\n-\r\n-    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n-        print(f\"\\nDebug - Transformer forward:\")\r\n-        print(f\"Input shape: {x.shape}\")\r\n-        print(f\"Input value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n-\r\n-        batch_size, seq_length, _ = x.shape\r\n-\r\n-        # Embed input\r\n-        x = self.input_embedding(x)\r\n-        print(f\"After embedding shape: {x.shape}\")\r\n-        print(f\"After embedding value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n-\r\n-        # Add positional encoding\r\n-        x = x + self.positional_encoding[:, :seq_length, :]\r\n-        print(f\"After positional encoding shape: {x.shape}\")\r\n-\r\n-        # Pass through encoder layers\r\n-        for i, encoder_layer in enumerate(self.encoder_layers):\r\n-            x = encoder_layer(x)\r\n-            print(f\"After encoder layer {i} shape: {x.shape}\")\r\n-            print(f\"After encoder layer {i} value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n-\r\n-        # Reshape and project\r\n-        x = x.reshape(batch_size, -1)\r\n-        print(f\"After reshape shape: {x.shape}\")\r\n-        \r\n-        x = self.output_projection(x)\r\n-        print(f\"After output projection shape: {x.shape}\")\r\n-        print(f\"After output projection value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n-        \r\n-        x = x.reshape(batch_size, self.seq_length, self.output_dim)\r\n-        print(f\"Final output shape: {x.shape}\")\r\n-        print(f\"Final output value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n-        \r\n         return x\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733006131067,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,70 @@\n+import torch\r\n+from torch import nn\r\n+from typing import Dict, Any\r\n+\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.architectures.transformers.base_transformer import BaseTransformer\r\n+from models.components.layers import EncoderLayer, DecoderLayer\r\n+\r\n+@ModelFactory.register(ModelType.VANILLA_TRANSFORMER)\r\n+class VanillaTransformer(BaseTransformer):\r\n+    def __init__(self, config: Dict[str, Any]):\r\n+        super().__init__(config)\r\n+        \r\n+        self.input_dim = config['input_features']\r\n+        self.output_dim = config['input_features']  # Match input features for reconstruction\r\n+        self.seq_length = config.get('transformer_labels_count', 12)\r\n+        \r\n+        self.input_embedding = nn.Linear(self.input_dim, self.d_model)\r\n+        self.positional_encoding = nn.Parameter(\r\n+            torch.randn(1, config.get('max_seq_length', 1000), self.d_model)\r\n+        )\r\n+        \r\n+        # Simplified output projection (per timestep)\r\n+        self.output_projection = nn.Linear(self.d_model, self.output_dim)\r\n+\r\n+    def _create_encoder_layers(self) -> nn.ModuleList:\r\n+        return nn.ModuleList([\r\n+            EncoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",\r\n+                activation=\"relu\"\r\n+            ) for _ in range(self.n_encoder_layers)\r\n+        ])\r\n+\r\n+    def _create_decoder_layers(self) -> nn.ModuleList:\r\n+        return nn.ModuleList([\r\n+            DecoderLayer(\r\n+                d_model=self.d_model,\r\n+                n_heads=self.n_heads,\r\n+                d_ff=self.d_ff,\r\n+                dropout=self.dropout,\r\n+                attention_type=\"standard\",\r\n+                activation=\"relu\"\r\n+            ) for _ in range(self.n_decoder_layers)\r\n+        ])\r\n+\r\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n+        batch_size, seq_length, _ = x.shape\r\n+\r\n+        # Embed input\r\n+        x = self.input_embedding(x)\r\n+        \r\n+        # Add positional encoding\r\n+        x = x + self.positional_encoding[:, :seq_length, :]\r\n+        \r\n+        # Pass through encoder layers\r\n+        for encoder_layer in self.encoder_layers:\r\n+            x = encoder_layer(x)\r\n+        \r\n+        # Take only the last seq_length time steps\r\n+        x = x[:, -self.seq_length:, :]\r\n+        \r\n+        # Project to output dimension\r\n+        x = self.output_projection(x)\r\n+        \r\n+        return x\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733005282427,
            "name": "Commit-0",
            "content": "import torch\r\nfrom torch import nn\r\nfrom typing import Dict, Any\r\n\r\nfrom models.registry.factory import ModelFactory\r\nfrom models.registry.model_types import ModelType\r\nfrom models.architectures.transformers.base_transformer import BaseTransformer\r\nfrom models.components.layers import EncoderLayer, DecoderLayer\r\n\r\n# Ensure this model is imported and registered early in your application startup\r\n@ModelFactory.register(ModelType.VANILLA_TRANSFORMER)\r\nclass VanillaTransformer(BaseTransformer):\r\n    \"\"\"Standard transformer implementation.\"\"\"\r\n\r\n    def __init__(self, config: Dict[str, Any]):\r\n        super().__init__(config)\r\n        \r\n        # Use the parameters from ModelConfig\r\n        self.input_dim = config['input_features']\r\n        self.output_dim = config.get('output_features', 1)\r\n        self.seq_length = config.get('transformer_labels_count', 12)  # Add this line\r\n        \r\n        # Linear layers for input and output transformations\r\n        self.input_embedding = nn.Linear(self.input_dim, self.d_model)\r\n        self.positional_encoding = nn.Parameter(\r\n            torch.randn(1, config.get('max_seq_length', 1000), self.d_model)\r\n        )\r\n        \r\n        # Encoder and decoder layers are created by parent class\r\n        \r\n        # Output projection with sequence length consideration\r\n        self.output_projection = nn.Linear(self.d_model * 24, self.seq_length * self.output_dim)\r\n\r\n    def _create_encoder_layers(self) -> nn.ModuleList:\r\n        return nn.ModuleList([\r\n            EncoderLayer(\r\n                d_model=self.d_model,\r\n                n_heads=self.n_heads,\r\n                d_ff=self.d_ff,\r\n                dropout=self.dropout,\r\n                attention_type=\"standard\",\r\n                activation=\"relu\"\r\n            ) for _ in range(self.n_encoder_layers)\r\n        ])\r\n\r\n    def _create_decoder_layers(self) -> nn.ModuleList:\r\n        return nn.ModuleList([\r\n            DecoderLayer(\r\n                d_model=self.d_model,\r\n                n_heads=self.n_heads,\r\n                d_ff=self.d_ff,\r\n                dropout=self.dropout,\r\n                attention_type=\"standard\",\r\n                activation=\"relu\"\r\n            ) for _ in range(self.n_decoder_layers)\r\n        ])\r\n\r\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n        print(f\"\\nDebug - Transformer forward:\")\r\n        print(f\"Input shape: {x.shape}\")\r\n        print(f\"Input value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n\r\n        batch_size, seq_length, _ = x.shape\r\n\r\n        # Embed input\r\n        x = self.input_embedding(x)\r\n        print(f\"After embedding shape: {x.shape}\")\r\n        print(f\"After embedding value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n\r\n        # Add positional encoding\r\n        x = x + self.positional_encoding[:, :seq_length, :]\r\n        print(f\"After positional encoding shape: {x.shape}\")\r\n\r\n        # Pass through encoder layers\r\n        for i, encoder_layer in enumerate(self.encoder_layers):\r\n            x = encoder_layer(x)\r\n            print(f\"After encoder layer {i} shape: {x.shape}\")\r\n            print(f\"After encoder layer {i} value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n\r\n        # Reshape and project\r\n        x = x.reshape(batch_size, -1)\r\n        print(f\"After reshape shape: {x.shape}\")\r\n        \r\n        x = self.output_projection(x)\r\n        print(f\"After output projection shape: {x.shape}\")\r\n        print(f\"After output projection value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n        \r\n        x = x.reshape(batch_size, self.seq_length, self.output_dim)\r\n        print(f\"Final output shape: {x.shape}\")\r\n        print(f\"Final output value range: [{x.min():.2f}, {x.max():.2f}]\")\r\n        \r\n        return x"
        }
    ]
}