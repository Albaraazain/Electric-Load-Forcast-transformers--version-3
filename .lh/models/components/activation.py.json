{
    "sourceFile": "models/components/activation.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1733089050304,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733089085631,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,8 @@\n import torch\r\n from torch import nn, Tensor\r\n import torch.nn.functional as F\r\n-from typing import Optional\r\n+from typing import Optional, Tuple\r\n import warnings\r\n \r\n class GELU(nn.Module):\r\n     r\"\"\"Applies the Gaussian Error Linear Units function:\r\n"
                }
            ],
            "date": 1733089050304,
            "name": "Commit-0",
            "content": "import torch\r\nfrom torch import nn, Tensor\r\nimport torch.nn.functional as F\r\nfrom typing import Optional\r\nimport warnings\r\n\r\nclass GELU(nn.Module):\r\n    r\"\"\"Applies the Gaussian Error Linear Units function:\r\n    \r\n    .. math:: \\text{GELU}(x) = x * \\Phi(x)\r\n    \r\n    where :math:`\\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.\r\n    \"\"\"\r\n    def forward(self, input: Tensor) -> Tensor:\r\n        return F.gelu(input)\r\n\r\nclass ReLU(nn.Module):\r\n    r\"\"\"Applies the rectified linear unit function element-wise:\r\n    \r\n    .. math:: \\text{ReLU}(x) = \\max(0, x)\r\n    \"\"\"\r\n    def __init__(self, inplace: bool = False):\r\n        super(ReLU, self).__init__()\r\n        self.inplace = inplace\r\n\r\n    def forward(self, input: Tensor) -> Tensor:\r\n        return F.relu(input, inplace=self.inplace)\r\n\r\n    def extra_repr(self) -> str:\r\n        inplace_str = 'inplace=True' if self.inplace else ''\r\n        return inplace_str\r\n\r\nclass MultiheadAttention(nn.Module):\r\n    r\"\"\"Allows the model to jointly attend to information from different representation subspaces.\r\n    See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\r\n\r\n    .. math::\r\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\r\n\r\n    where :math:`head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)`.\r\n\r\n    Args:\r\n        embed_dim: Total dimension of the model.\r\n        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\r\n            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\r\n        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\r\n        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\r\n        batch_first: If ``True``, then the input and output tensors are provided\r\n            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\r\n    \"\"\"\r\n    __constants__ = ['batch_first']\r\n    bias_k: Optional[torch.Tensor]\r\n    bias_v: Optional[torch.Tensor]\r\n\r\n    def __init__(self, embed_dim, num_heads, dropout=0., bias=True,\r\n                 add_bias_kv=False, add_zero_attn=False,\r\n                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\r\n        factory_kwargs = {'device': device, 'dtype': dtype}\r\n        super(MultiheadAttention, self).__init__()\r\n        self.embed_dim = embed_dim\r\n        self.kdim = kdim if kdim is not None else embed_dim\r\n        self.vdim = vdim if vdim is not None else embed_dim\r\n        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\r\n\r\n        self.num_heads = num_heads\r\n        self.dropout = dropout\r\n        self.batch_first = batch_first\r\n        self.head_dim = embed_dim // num_heads\r\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\r\n\r\n        if self._qkv_same_embed_dim is False:\r\n            self.q_proj_weight = nn.Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\r\n            self.k_proj_weight = nn.Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\r\n            self.v_proj_weight = nn.Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\r\n            self.register_parameter('in_proj_weight', None)\r\n        else:\r\n            self.in_proj_weight = nn.Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\r\n            self.register_parameter('q_proj_weight', None)\r\n            self.register_parameter('k_proj_weight', None)\r\n            self.register_parameter('v_proj_weight', None)\r\n\r\n        if bias:\r\n            self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\r\n        else:\r\n            self.register_parameter('in_proj_bias', None)\r\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\r\n\r\n        self._reset_parameters()\r\n\r\n    def _reset_parameters(self):\r\n        if self._qkv_same_embed_dim:\r\n            nn.init.xavier_uniform_(self.in_proj_weight)\r\n        else:\r\n            nn.init.xavier_uniform_(self.q_proj_weight)\r\n            nn.init.xavier_uniform_(self.k_proj_weight)\r\n            nn.init.xavier_uniform_(self.v_proj_weight)\r\n\r\n        if self.in_proj_bias is not None:\r\n            nn.init.constant_(self.in_proj_bias, 0.)\r\n            nn.init.constant_(self.out_proj.bias, 0.)\r\n\r\n    def forward(self, query: Tensor, key: Tensor, value: Tensor, \r\n                key_padding_mask: Optional[Tensor] = None,\r\n                need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\r\n        \r\n        if self.batch_first:\r\n            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\r\n            \r\n        if not self._qkv_same_embed_dim:\r\n            attn_output, attn_output_weights = F.multi_head_attention_forward(\r\n                query, key, value, self.embed_dim, self.num_heads,\r\n                self.in_proj_weight, self.in_proj_bias,\r\n                None, None, False,\r\n                self.dropout, self.out_proj.weight, self.out_proj.bias,\r\n                training=self.training,\r\n                key_padding_mask=key_padding_mask, need_weights=need_weights,\r\n                attn_mask=attn_mask, use_separate_proj_weight=True,\r\n                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\r\n                v_proj_weight=self.v_proj_weight)\r\n        else:\r\n            attn_output, attn_output_weights = F.multi_head_attention_forward(\r\n                query, key, value, self.embed_dim, self.num_heads,\r\n                self.in_proj_weight, self.in_proj_bias,\r\n                None, None, False,\r\n                self.dropout, self.out_proj.weight, self.out_proj.bias,\r\n                training=self.training,\r\n                key_padding_mask=key_padding_mask, need_weights=need_weights,\r\n                attn_mask=attn_mask)\r\n\r\n        if self.batch_first:\r\n            return attn_output.transpose(1, 0), attn_output_weights\r\n        return attn_output, attn_output_weights"
        }
    ]
}