{
    "sourceFile": "models/components/attention.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1733089218465,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733089238178,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -110,10 +110,9 @@\n             n_heads: int,\r\n             kernel_size: int = 3,\r\n             dropout: float = 0.1\r\n     ):\r\n-        # Important: Call parent's init first!\r\n-        nn.Module.__init__(self)\r\n+        super().__init__()\r\n \r\n         self.attention = MultiHeadAttention(d_model, n_heads, dropout)\r\n         padding = (kernel_size - 1) // 2\r\n \r\n@@ -130,21 +129,23 @@\n             kernel_size,\r\n             padding=padding,\r\n             padding_mode='replicate'\r\n         )\r\n+\r\n     def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=False):\r\n         # Apply convolution\r\n         query = self.query_conv(query.transpose(-1, -2)).transpose(-1, -2)\r\n         key = self.key_conv(key.transpose(-1, -2)).transpose(-1, -2)\r\n \r\n-        # Use attention with same signature\r\n-        return self.attention(query, key, value,\r\n-                            attn_mask=attn_mask,\r\n-                            key_padding_mask=key_padding_mask,\r\n-                            need_weights=need_weights)\r\n+        # Use attention\r\n+        return self.attention(\r\n+            query, key, value,\r\n+            need_weights=need_weights,\r\n+            attn_mask=attn_mask,\r\n+            key_padding_mask=key_padding_mask\r\n+        )\r\n \r\n \r\n-\r\n class ProbSparseAttention(nn.Module):\r\n     \"\"\"Probabilistic sparse attention mechanism.\"\"\"\r\n \r\n     def __init__(\r\n"
                },
                {
                    "date": 1733089255018,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -154,15 +154,15 @@\n             n_heads: int,\r\n             factor: int = 5,\r\n             dropout: float = 0.1\r\n     ):\r\n-        # Important: Call parent's init first!\r\n-        nn.Module.__init__(self)\r\n+        super().__init__()\r\n \r\n         self.attention = MultiHeadAttention(d_model, n_heads, dropout)\r\n         self.factor = factor\r\n \r\n     def _prob_QK(self, Q: Tensor, K: Tensor, sample_k: int, n_top: int) -> Tuple[Tensor, Tensor]:\r\n+        \"\"\"Calculate Top-u query sampling attention.\"\"\"\r\n         B, H, L_Q, D = Q.shape\r\n         _, _, L_K, _ = K.shape\r\n \r\n         # Calculate Q_K\r\n@@ -177,9 +177,11 @@\n \r\n         return Q_reduce, M_top\r\n \r\n     def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=False):\r\n-        return self.attention(query, key, value,\r\n-                            attn_mask=attn_mask,\r\n-                            key_padding_mask=key_padding_mask,\r\n-                            need_weights=need_weights)\r\n+        return self.attention(\r\n+            query, key, value,\r\n+            need_weights=need_weights,\r\n+            attn_mask=attn_mask,\r\n+            key_padding_mask=key_padding_mask\r\n+        )\r\n \r\n"
                },
                {
                    "date": 1733089462928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,103 +5,119 @@\n import torch.nn.functional as F\r\n from typing import Optional, Tuple\r\n \r\n class MultiHeadAttention(nn.Module):\r\n-    \"\"\"Multi-head attention mechanism with improved implementations.\"\"\"\r\n+    r\"\"\"Allows the model to jointly attend to information from different representation subspaces.\r\n+    See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\r\n \r\n-    def __init__(\r\n-            self,\r\n-            d_model: int,\r\n-            n_heads: int,\r\n-            dropout: float = 0.1,\r\n-            bias: bool = True,\r\n-            batch_first: bool = True\r\n-    ):\r\n+    .. math::\r\n+        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\r\n+\r\n+    Args:\r\n+        embed_dim: Total dimension of the model.\r\n+        num_heads: Number of parallel attention heads. Note that ``embed_dim`` will be split\r\n+            across ``num_heads`` (i.e. each head will have dimension ``embed_dim // num_heads``).\r\n+        dropout: Dropout probability on ``attn_output_weights``. Default: ``0.0`` (no dropout).\r\n+        bias: If specified, adds bias to input / output projection layers. Default: ``True``.\r\n+        add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: ``False``.\r\n+        add_zero_attn: If specified, adds a new batch of zeros to the key and value sequences at dim=1.\r\n+            Default: ``False``.\r\n+        kdim: Total number of features for keys. Default: ``None`` (uses ``kdim=embed_dim``).\r\n+        vdim: Total number of features for values. Default: ``None`` (uses ``vdim=embed_dim``).\r\n+        batch_first: If ``True``, then the input and output tensors are provided\r\n+            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\r\n+    \"\"\"\r\n+    __constants__ = ['batch_first']\r\n+    bias_k: Optional[torch.Tensor]\r\n+    bias_v: Optional[torch.Tensor]\r\n+\r\n+    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\r\n+                 kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\r\n+        factory_kwargs = {'device': device, 'dtype': dtype}\r\n         super().__init__()\r\n+        self.embed_dim = embed_dim\r\n+        self.kdim = kdim if kdim is not None else embed_dim\r\n+        self.vdim = vdim if vdim is not None else embed_dim\r\n+        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\r\n \r\n-        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\r\n-\r\n-        self.d_model = d_model\r\n-        self.n_heads = n_heads\r\n-        self.d_k = d_model // n_heads\r\n-        self.scale = math.sqrt(self.d_k)\r\n+        self.num_heads = num_heads\r\n+        self.dropout = dropout\r\n         self.batch_first = batch_first\r\n+        self.head_dim = embed_dim // num_heads\r\n+        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\r\n \r\n-        # Use single projection layer for Q, K, V\r\n-        self.in_proj_weight = nn.Parameter(torch.empty((3 * d_model, d_model)))\r\n+        if self._qkv_same_embed_dim is False:\r\n+            self.q_proj_weight = nn.Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))\r\n+            self.k_proj_weight = nn.Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))\r\n+            self.v_proj_weight = nn.Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))\r\n+            self.register_parameter('in_proj_weight', None)\r\n+        else:\r\n+            self.in_proj_weight = nn.Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))\r\n+            self.register_parameter('q_proj_weight', None)\r\n+            self.register_parameter('k_proj_weight', None)\r\n+            self.register_parameter('v_proj_weight', None)\r\n+\r\n         if bias:\r\n-            self.in_proj_bias = nn.Parameter(torch.empty(3 * d_model))\r\n+            self.in_proj_bias = nn.Parameter(torch.empty(3 * embed_dim, **factory_kwargs))\r\n         else:\r\n             self.register_parameter('in_proj_bias', None)\r\n+        self.out_proj = NonDynamicallyQuantizableLinear(embed_dim, embed_dim, bias=bias, **factory_kwargs)\r\n \r\n-        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\r\n-        self.dropout = nn.Dropout(dropout)\r\n-        \r\n+        if add_bias_kv:\r\n+            self.bias_k = nn.Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\r\n+            self.bias_v = nn.Parameter(torch.empty((1, 1, embed_dim), **factory_kwargs))\r\n+        else:\r\n+            self.bias_k = self.bias_v = None\r\n+\r\n+        self.add_zero_attn = add_zero_attn\r\n         self._reset_parameters()\r\n \r\n     def _reset_parameters(self):\r\n-        nn.init.xavier_uniform_(self.in_proj_weight)\r\n+        if self._qkv_same_embed_dim:\r\n+            xavier_uniform_(self.in_proj_weight)\r\n+        else:\r\n+            xavier_uniform_(self.q_proj_weight)\r\n+            xavier_uniform_(self.k_proj_weight)\r\n+            xavier_uniform_(self.v_proj_weight)\r\n+\r\n         if self.in_proj_bias is not None:\r\n-            nn.init.constant_(self.in_proj_bias, 0.)\r\n-            nn.init.constant_(self.out_proj.bias, 0.)\r\n+            constant_(self.in_proj_bias, 0.)\r\n+            constant_(self.out_proj.bias, 0.)\r\n+        if self.bias_k is not None:\r\n+            xavier_normal_(self.bias_k)\r\n+        if self.bias_v is not None:\r\n+            xavier_normal_(self.bias_v)\r\n \r\n-    def forward(self, query: Tensor, key: Tensor, value: Tensor, \r\n-                need_weights: bool = True,\r\n-                attn_mask: Optional[Tensor] = None,\r\n-                key_padding_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\r\n-        \"\"\"\r\n-        Args:\r\n-            query, key, value: Input tensors\r\n-            need_weights: If True, returns attention weights\r\n-            attn_mask: Optional mask to prevent attention to certain positions\r\n-            key_padding_mask: Optional mask for padded keys\r\n-        \"\"\"\r\n-        if not self.batch_first:\r\n+    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\r\n+                need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\r\n+        if self.batch_first:\r\n             query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\r\n \r\n-        batch_size, seq_len, _ = query.size()\r\n-        \r\n-        # Calculate Q, K, V projections\r\n-        chunk_size = self.d_model\r\n-        q, k, v = F.linear(query, self.in_proj_weight, self.in_proj_bias).chunk(3, dim=-1)\r\n+        if not self._qkv_same_embed_dim:\r\n+            attn_output, attn_output_weights = F.multi_head_attention_forward(\r\n+                query, key, value, self.embed_dim, self.num_heads,\r\n+                self.in_proj_weight, self.in_proj_bias,\r\n+                self.bias_k, self.bias_v, self.add_zero_attn,\r\n+                self.dropout, self.out_proj.weight, self.out_proj.bias,\r\n+                training=self.training,\r\n+                key_padding_mask=key_padding_mask, need_weights=need_weights,\r\n+                attn_mask=attn_mask, use_separate_proj_weight=True,\r\n+                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,\r\n+                v_proj_weight=self.v_proj_weight)\r\n+        else:\r\n+            attn_output, attn_output_weights = F.multi_head_attention_forward(\r\n+                query, key, value, self.embed_dim, self.num_heads,\r\n+                self.in_proj_weight, self.in_proj_bias,\r\n+                self.bias_k, self.bias_v, self.add_zero_attn,\r\n+                self.dropout, self.out_proj.weight, self.out_proj.bias,\r\n+                training=self.training,\r\n+                key_padding_mask=key_padding_mask, need_weights=need_weights,\r\n+                attn_mask=attn_mask)\r\n \r\n-        # Reshape and transpose for attention calculation\r\n-        q = q.contiguous().view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\r\n-        k = k.contiguous().view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\r\n-        v = v.contiguous().view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\r\n+        if self.batch_first:\r\n+            return attn_output.transpose(1, 0), attn_output_weights\r\n+        return attn_output, attn_output_weights\r\n \r\n-        # Calculate attention scores\r\n-        attn_output_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale\r\n-\r\n-        if attn_mask is not None:\r\n-            if attn_mask.dim() == 2:\r\n-                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\r\n-            attn_output_weights = attn_output_weights.masked_fill(attn_mask == 0, float('-inf'))\r\n-\r\n-        if key_padding_mask is not None:\r\n-            attn_output_weights = attn_output_weights.masked_fill(\r\n-                key_padding_mask.unsqueeze(1).unsqueeze(2),\r\n-                float('-inf'),\r\n-            )\r\n-\r\n-        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\r\n-        attn_output_weights = self.dropout(attn_output_weights)\r\n-\r\n-        # Apply attention to values\r\n-        attn_output = torch.matmul(attn_output_weights, v)\r\n-        \r\n-        # Reshape and apply output projection\r\n-        attn_output = attn_output.transpose(1, 2).contiguous().view(\r\n-            batch_size, seq_len, self.d_model\r\n-        )\r\n-        attn_output = self.out_proj(attn_output)\r\n-\r\n-        if not self.batch_first:\r\n-            attn_output = attn_output.transpose(1, 0)\r\n-\r\n-        return attn_output, attn_output_weights if need_weights else None\r\n-\r\n-\r\n class ConvolutionalAttention(nn.Module):\r\n     \"\"\"Attention mechanism with convolutional processing.\"\"\"\r\n \r\n     def __init__(\r\n@@ -111,9 +127,8 @@\n             kernel_size: int = 3,\r\n             dropout: float = 0.1\r\n     ):\r\n         super().__init__()\r\n-\r\n         self.attention = MultiHeadAttention(d_model, n_heads, dropout)\r\n         padding = (kernel_size - 1) // 2\r\n \r\n         self.query_conv = nn.Conv1d(\r\n@@ -136,16 +151,13 @@\n         query = self.query_conv(query.transpose(-1, -2)).transpose(-1, -2)\r\n         key = self.key_conv(key.transpose(-1, -2)).transpose(-1, -2)\r\n \r\n         # Use attention\r\n-        return self.attention(\r\n-            query, key, value,\r\n-            need_weights=need_weights,\r\n-            attn_mask=attn_mask,\r\n-            key_padding_mask=key_padding_mask\r\n-        )\r\n+        return self.attention(query, key, value,\r\n+                            key_padding_mask=key_padding_mask,\r\n+                            need_weights=need_weights,\r\n+                            attn_mask=attn_mask)\r\n \r\n-\r\n class ProbSparseAttention(nn.Module):\r\n     \"\"\"Probabilistic sparse attention mechanism.\"\"\"\r\n \r\n     def __init__(\r\n@@ -155,14 +167,12 @@\n             factor: int = 5,\r\n             dropout: float = 0.1\r\n     ):\r\n         super().__init__()\r\n-\r\n         self.attention = MultiHeadAttention(d_model, n_heads, dropout)\r\n         self.factor = factor\r\n \r\n     def _prob_QK(self, Q: Tensor, K: Tensor, sample_k: int, n_top: int) -> Tuple[Tensor, Tensor]:\r\n-        \"\"\"Calculate Top-u query sampling attention.\"\"\"\r\n         B, H, L_Q, D = Q.shape\r\n         _, _, L_K, _ = K.shape\r\n \r\n         # Calculate Q_K\r\n@@ -177,11 +187,8 @@\n \r\n         return Q_reduce, M_top\r\n \r\n     def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=False):\r\n-        return self.attention(\r\n-            query, key, value,\r\n-            need_weights=need_weights,\r\n-            attn_mask=attn_mask,\r\n-            key_padding_mask=key_padding_mask\r\n-        )\r\n-\r\n+        return self.attention(query, key, value,\r\n+                            key_padding_mask=key_padding_mask,\r\n+                            need_weights=need_weights,\r\n+                            attn_mask=attn_mask)\r\n"
                },
                {
                    "date": 1733089475917,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,12 @@\n import torch\r\n from torch import nn, Tensor\r\n import torch.nn.functional as F\r\n from typing import Optional, Tuple\r\n+from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\r\n+from torch.nn.init import constant_, xavier_normal_, xavier_uniform_\r\n \r\n+\r\n class MultiHeadAttention(nn.Module):\r\n     r\"\"\"Allows the model to jointly attend to information from different representation subspaces.\r\n     See `Attention Is All You Need <https://arxiv.org/abs/1706.03762>`_.\r\n \r\n"
                },
                {
                    "date": 1733239027013,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,13 +89,27 @@\n             xavier_normal_(self.bias_k)\r\n         if self.bias_v is not None:\r\n             xavier_normal_(self.bias_v)\r\n \r\n-    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = None,\r\n+    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask=None,\r\n                 need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\r\n         if self.batch_first:\r\n             query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\r\n \r\n+        # Add debug prints to track tensors and their devices\r\n+        print(f\"\\nDEBUG - MultiHeadAttention devices:\")\r\n+        print(f\"Query device: {query.device}\")\r\n+        print(f\"Key device: {key.device}\")\r\n+        print(f\"Value device: {value.device}\")\r\n+        print(f\"Attention mask device: {attn_mask.device if attn_mask is not None else 'None'}\")\r\n+        print(f\"Key padding mask device: {key_padding_mask.device if key_padding_mask is not None else 'None'}\")\r\n+\r\n+\r\n+                # Ensure attn_mask is on the correct device\r\n+        if attn_mask is not None:\r\n+            attn_mask = attn_mask.to(query.device)\r\n+\r\n+\r\n         if not self._qkv_same_embed_dim:\r\n             attn_output, attn_output_weights = F.multi_head_attention_forward(\r\n                 query, key, value, self.embed_dim, self.num_heads,\r\n                 self.in_proj_weight, self.in_proj_bias,\r\n"
                },
                {
                    "date": 1733239917940,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,8 +72,10 @@\n             self.bias_k = self.bias_v = None\r\n \r\n         self.add_zero_attn = add_zero_attn\r\n         self._reset_parameters()\r\n+        self.debug_counter = 0\r\n+        self.max_debug_prints = 3  # Only print first 3 times\r\n \r\n     def _reset_parameters(self):\r\n         if self._qkv_same_embed_dim:\r\n             xavier_uniform_(self.in_proj_weight)\r\n@@ -94,18 +96,19 @@\n                 need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\r\n         if self.batch_first:\r\n             query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\r\n \r\n-        # Add debug prints to track tensors and their devices\r\n-        print(f\"\\nDEBUG - MultiHeadAttention devices:\")\r\n-        print(f\"Query device: {query.device}\")\r\n-        print(f\"Key device: {key.device}\")\r\n-        print(f\"Value device: {value.device}\")\r\n-        print(f\"Attention mask device: {attn_mask.device if attn_mask is not None else 'None'}\")\r\n-        print(f\"Key padding mask device: {key_padding_mask.device if key_padding_mask is not None else 'None'}\")\r\n+        # Only print debug info for first few calls\r\n+        if self.debug_counter < self.max_debug_prints:\r\n+            print(f\"\\nDEBUG - MultiHeadAttention devices (call {self.debug_counter + 1}/{self.max_debug_prints}):\")\r\n+            print(f\"Query device: {query.device}\")\r\n+            print(f\"Key device: {key.device}\")\r\n+            print(f\"Value device: {value.device}\")\r\n+            print(f\"Attention mask device: {attn_mask.device if attn_mask is not None else 'None'}\")\r\n+            print(f\"Key padding mask device: {key_padding_mask.device if key_padding_mask is not None else 'None'}\")\r\n+            self.debug_counter += 1\r\n \r\n-\r\n-                # Ensure attn_mask is on the correct device\r\n+        # Ensure attn_mask is on the correct device\r\n         if attn_mask is not None:\r\n             attn_mask = attn_mask.to(query.device)\r\n \r\n \r\n"
                },
                {
                    "date": 1733251180335,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,8 +5,10 @@\n import torch.nn.functional as F\r\n from typing import Optional, Tuple\r\n from torch.nn.modules.linear import NonDynamicallyQuantizableLinear\r\n from torch.nn.init import constant_, xavier_normal_, xavier_uniform_\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n \r\n \r\n class MultiHeadAttention(nn.Module):\r\n     r\"\"\"Allows the model to jointly attend to information from different representation subspaces.\r\n@@ -36,8 +38,17 @@\n     def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False,\r\n                  kdim=None, vdim=None, batch_first=False, device=None, dtype=None) -> None:\r\n         factory_kwargs = {'device': device, 'dtype': dtype}\r\n         super().__init__()\r\n+        \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"MultiHeadAttention\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        \r\n         self.embed_dim = embed_dim\r\n         self.kdim = kdim if kdim is not None else embed_dim\r\n         self.vdim = vdim if vdim is not None else embed_dim\r\n         self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim\r\n@@ -96,16 +107,17 @@\n                 need_weights: bool = True, attn_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\r\n         if self.batch_first:\r\n             query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\r\n \r\n-        # Only print debug info for first few calls\r\n+        # Only log debug info for first few calls\r\n         if self.debug_counter < self.max_debug_prints:\r\n-            print(f\"\\nDEBUG - MultiHeadAttention devices (call {self.debug_counter + 1}/{self.max_debug_prints}):\")\r\n-            print(f\"Query device: {query.device}\")\r\n-            print(f\"Key device: {key.device}\")\r\n-            print(f\"Value device: {value.device}\")\r\n-            print(f\"Attention mask device: {attn_mask.device if attn_mask is not None else 'None'}\")\r\n-            print(f\"Key padding mask device: {key_padding_mask.device if key_padding_mask is not None else 'None'}\")\r\n+            self.logger.debug(f\"MultiHeadAttention forward pass (call {self.debug_counter + 1}/{self.max_debug_prints})\", {\r\n+                \"query_device\": str(query.device),\r\n+                \"key_device\": str(key.device),\r\n+                \"value_device\": str(value.device),\r\n+                \"attention_mask_device\": str(attn_mask.device) if attn_mask is not None else \"None\",\r\n+                \"key_padding_mask_device\": str(key_padding_mask.device) if key_padding_mask is not None else \"None\"\r\n+            })\r\n             self.debug_counter += 1\r\n \r\n         # Ensure attn_mask is on the correct device\r\n         if attn_mask is not None:\r\n@@ -147,8 +159,16 @@\n             kernel_size: int = 3,\r\n             dropout: float = 0.1\r\n     ):\r\n         super().__init__()\r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"ConvolutionalAttention\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        \r\n         self.attention = MultiHeadAttention(d_model, n_heads, dropout)\r\n         padding = (kernel_size - 1) // 2\r\n \r\n         self.query_conv = nn.Conv1d(\r\n@@ -187,8 +207,16 @@\n             factor: int = 5,\r\n             dropout: float = 0.1\r\n     ):\r\n         super().__init__()\r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"ProbSparseAttention\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        \r\n         self.attention = MultiHeadAttention(d_model, n_heads, dropout)\r\n         self.factor = factor\r\n \r\n     def _prob_QK(self, Q: Tensor, K: Tensor, sample_k: int, n_top: int) -> Tuple[Tensor, Tensor]:\r\n"
                }
            ],
            "date": 1733089218465,
            "name": "Commit-0",
            "content": "# models/components/attention.py\r\nimport math\r\nimport torch\r\nfrom torch import nn, Tensor\r\nimport torch.nn.functional as F\r\nfrom typing import Optional, Tuple\r\n\r\nclass MultiHeadAttention(nn.Module):\r\n    \"\"\"Multi-head attention mechanism with improved implementations.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            d_model: int,\r\n            n_heads: int,\r\n            dropout: float = 0.1,\r\n            bias: bool = True,\r\n            batch_first: bool = True\r\n    ):\r\n        super().__init__()\r\n\r\n        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\r\n\r\n        self.d_model = d_model\r\n        self.n_heads = n_heads\r\n        self.d_k = d_model // n_heads\r\n        self.scale = math.sqrt(self.d_k)\r\n        self.batch_first = batch_first\r\n\r\n        # Use single projection layer for Q, K, V\r\n        self.in_proj_weight = nn.Parameter(torch.empty((3 * d_model, d_model)))\r\n        if bias:\r\n            self.in_proj_bias = nn.Parameter(torch.empty(3 * d_model))\r\n        else:\r\n            self.register_parameter('in_proj_bias', None)\r\n\r\n        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\r\n        self.dropout = nn.Dropout(dropout)\r\n        \r\n        self._reset_parameters()\r\n\r\n    def _reset_parameters(self):\r\n        nn.init.xavier_uniform_(self.in_proj_weight)\r\n        if self.in_proj_bias is not None:\r\n            nn.init.constant_(self.in_proj_bias, 0.)\r\n            nn.init.constant_(self.out_proj.bias, 0.)\r\n\r\n    def forward(self, query: Tensor, key: Tensor, value: Tensor, \r\n                need_weights: bool = True,\r\n                attn_mask: Optional[Tensor] = None,\r\n                key_padding_mask: Optional[Tensor] = None) -> Tuple[Tensor, Optional[Tensor]]:\r\n        \"\"\"\r\n        Args:\r\n            query, key, value: Input tensors\r\n            need_weights: If True, returns attention weights\r\n            attn_mask: Optional mask to prevent attention to certain positions\r\n            key_padding_mask: Optional mask for padded keys\r\n        \"\"\"\r\n        if not self.batch_first:\r\n            query, key, value = [x.transpose(1, 0) for x in (query, key, value)]\r\n\r\n        batch_size, seq_len, _ = query.size()\r\n        \r\n        # Calculate Q, K, V projections\r\n        chunk_size = self.d_model\r\n        q, k, v = F.linear(query, self.in_proj_weight, self.in_proj_bias).chunk(3, dim=-1)\r\n\r\n        # Reshape and transpose for attention calculation\r\n        q = q.contiguous().view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\r\n        k = k.contiguous().view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\r\n        v = v.contiguous().view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\r\n\r\n        # Calculate attention scores\r\n        attn_output_weights = torch.matmul(q, k.transpose(-2, -1)) / self.scale\r\n\r\n        if attn_mask is not None:\r\n            if attn_mask.dim() == 2:\r\n                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\r\n            attn_output_weights = attn_output_weights.masked_fill(attn_mask == 0, float('-inf'))\r\n\r\n        if key_padding_mask is not None:\r\n            attn_output_weights = attn_output_weights.masked_fill(\r\n                key_padding_mask.unsqueeze(1).unsqueeze(2),\r\n                float('-inf'),\r\n            )\r\n\r\n        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\r\n        attn_output_weights = self.dropout(attn_output_weights)\r\n\r\n        # Apply attention to values\r\n        attn_output = torch.matmul(attn_output_weights, v)\r\n        \r\n        # Reshape and apply output projection\r\n        attn_output = attn_output.transpose(1, 2).contiguous().view(\r\n            batch_size, seq_len, self.d_model\r\n        )\r\n        attn_output = self.out_proj(attn_output)\r\n\r\n        if not self.batch_first:\r\n            attn_output = attn_output.transpose(1, 0)\r\n\r\n        return attn_output, attn_output_weights if need_weights else None\r\n\r\n\r\nclass ConvolutionalAttention(nn.Module):\r\n    \"\"\"Attention mechanism with convolutional processing.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            d_model: int,\r\n            n_heads: int,\r\n            kernel_size: int = 3,\r\n            dropout: float = 0.1\r\n    ):\r\n        # Important: Call parent's init first!\r\n        nn.Module.__init__(self)\r\n\r\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\r\n        padding = (kernel_size - 1) // 2\r\n\r\n        self.query_conv = nn.Conv1d(\r\n            d_model,\r\n            d_model,\r\n            kernel_size,\r\n            padding=padding,\r\n            padding_mode='replicate'\r\n        )\r\n        self.key_conv = nn.Conv1d(\r\n            d_model,\r\n            d_model,\r\n            kernel_size,\r\n            padding=padding,\r\n            padding_mode='replicate'\r\n        )\r\n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=False):\r\n        # Apply convolution\r\n        query = self.query_conv(query.transpose(-1, -2)).transpose(-1, -2)\r\n        key = self.key_conv(key.transpose(-1, -2)).transpose(-1, -2)\r\n\r\n        # Use attention with same signature\r\n        return self.attention(query, key, value,\r\n                            attn_mask=attn_mask,\r\n                            key_padding_mask=key_padding_mask,\r\n                            need_weights=need_weights)\r\n\r\n\r\n\r\nclass ProbSparseAttention(nn.Module):\r\n    \"\"\"Probabilistic sparse attention mechanism.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            d_model: int,\r\n            n_heads: int,\r\n            factor: int = 5,\r\n            dropout: float = 0.1\r\n    ):\r\n        # Important: Call parent's init first!\r\n        nn.Module.__init__(self)\r\n\r\n        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\r\n        self.factor = factor\r\n\r\n    def _prob_QK(self, Q: Tensor, K: Tensor, sample_k: int, n_top: int) -> Tuple[Tensor, Tensor]:\r\n        B, H, L_Q, D = Q.shape\r\n        _, _, L_K, _ = K.shape\r\n\r\n        # Calculate Q_K\r\n        Q_K = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D)\r\n\r\n        # Find top k queries\r\n        M = Q_K.max(-1)[0] - torch.div(Q_K.sum(-1), L_K)\r\n        M_top = M.topk(n_top, sorted=False)[1]\r\n\r\n        # Reduce Q to top k queries\r\n        Q_reduce = torch.gather(Q, 2, M_top.unsqueeze(-1).expand(-1, -1, -1, D))\r\n\r\n        return Q_reduce, M_top\r\n\r\n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=False):\r\n        return self.attention(query, key, value,\r\n                            attn_mask=attn_mask,\r\n                            key_padding_mask=key_padding_mask,\r\n                            need_weights=need_weights)\r\n\r\n"
        }
    ]
}