{
    "sourceFile": "models/components/embeddings.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1733089841915,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733089961946,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,59 +1,93 @@\n-# models/components/embeddings.py\r\n import math\r\n import torch\r\n from torch import nn, Tensor\r\n from typing import Optional\r\n \r\n class ValueEmbedding(nn.Module):\r\n+    \"\"\"Projects time series values to model dimension.\"\"\"\r\n \r\n-    def __init__(self, d_model: int, time_series_features=1):\r\n-        super(ValueEmbedding, self).__init__()\r\n+    def __init__(self, d_model: int, time_series_features: int = 1):\r\n+        super().__init__()\r\n         self.linear = nn.Linear(time_series_features, d_model)\r\n \r\n     def forward(self, x: Tensor) -> Tensor:\r\n         \"\"\"\r\n         Creates from the given tensor a linear projection.\r\n \r\n-        :param x: the input tensor to project, shape: [batch_size, sequence_length, features]\r\n-        :return: the projected tensor of shape: [batch_size, sequence_length, model_dimension]\r\n+        Args:\r\n+            x: the input tensor to project, shape: [batch_size, sequence_length, features]\r\n+        Returns:\r\n+            the projected tensor of shape: [batch_size, sequence_length, model_dimension]\r\n         \"\"\"\r\n         return self.linear(x)\r\n \r\n-class PositionalEmbedding(nn.Module):\r\n-    \"\"\"Adds positional information to embeddings.\"\"\"\r\n \r\n-    def __init__(\r\n-            self,\r\n-            d_model: int,\r\n-            max_seq_len: int = 5000,\r\n-            dropout: float = 0.1\r\n-    ):\r\n+class PositionalEncoding(nn.Module):\r\n+    \"\"\"Fixed sinusoidal positional encoding.\"\"\"\r\n+\r\n+    def __init__(self, d_model: int, max_len: int = 5000):\r\n         super().__init__()\r\n-        self.dropout = nn.Dropout(p=dropout)\r\n \r\n-        position = torch.arange(max_seq_len).unsqueeze(1)\r\n-        div_term = torch.exp(\r\n-            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\r\n-        )\r\n+        position = torch.arange(max_len).unsqueeze(1)\r\n+        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\r\n \r\n-        pe = torch.zeros(1, max_seq_len, d_model)\r\n+        pe = torch.zeros(1, max_len, d_model)\r\n+        pe.require_grad = False\r\n         pe[0, :, 0::2] = torch.sin(position * div_term)\r\n         pe[0, :, 1::2] = torch.cos(position * div_term)\r\n-\r\n         self.register_buffer('pe', pe)\r\n \r\n     def forward(self, x: Tensor) -> Tensor:\r\n         \"\"\"\r\n+        Creates a positional encoding for the given tensor.\r\n+\r\n         Args:\r\n-            x: Tensor of shape [batch_size, seq_len, d_model]\r\n+            x: tensor for which the pe is created, shape: [batch_size, sequence_length, model_dimension]\r\n+        Returns:\r\n+            positional encoding of dimension [1, sequence_length, model_dimension]\r\n         \"\"\"\r\n-        x = x + self.pe[:, :x.size(1)]\r\n-        return self.dropout(x)\r\n+        return self.pe[:, :x.size(1), :]\r\n \r\n-class CombinedEmbedding(nn.Module):\r\n+\r\n+class TotalEmbedding(nn.Module):\r\n     \"\"\"Combines value and positional embeddings with learnable weights.\"\"\"\r\n \r\n+    def __init__(self, d_model: int, value_features: int, time_features: int, dropout: float):\r\n+        super().__init__()\r\n+\r\n+        self.value_embedding = ValueEmbedding(d_model, value_features + time_features)\r\n+        self.positional_encoding = PositionalEncoding(d_model)\r\n+\r\n+        # Initialize learnable weights with ones for stable training\r\n+        self.linear_embedding_weight = nn.Linear(2, 1, bias=False)\r\n+        self.linear_embedding_weight.weight.data.fill_(1)\r\n+        self.dropout = nn.Dropout(p=dropout)\r\n+\r\n+    def forward(self, x: Tensor) -> Tensor:\r\n+        \"\"\"\r\n+        Projects the given tensor x on the model_dimension (in the last dimension) and combines this with a positional\r\n+        encoding (PE). The PE is added with learned weights to the projected x tensor. Dropout is applied on the final\r\n+        result.\r\n+\r\n+        Args:\r\n+            x: tensor of dimension [Batch_Size, Sequence_Length, Features]\r\n+        Returns:\r\n+            the embedded value of shape: [Batch_Size, Sequence_Length, model_dimension]\r\n+        \"\"\"\r\n+        value_embedded = self.value_embedding(x)\r\n+        pe = self.positional_encoding(x).repeat(x.shape[0], 1, 1)\r\n+\r\n+        # Combine embeddings with learned weights\r\n+        return self.dropout(\r\n+            self.linear_embedding_weight.weight[0][0] * value_embedded + \r\n+            self.linear_embedding_weight.weight[0][1] * pe\r\n+        )\r\n+\r\n+\r\n+class CombinedEmbedding(nn.Module):\r\n+    \"\"\"Alternative combined embedding with separate value and positional components.\"\"\"\r\n+\r\n     def __init__(\r\n             self,\r\n             d_model: int,\r\n             n_features: int,\r\n@@ -61,14 +95,14 @@\n             dropout: float = 0.1\r\n     ):\r\n         super().__init__()\r\n \r\n+        # Core embeddings\r\n         self.value_embedding = ValueEmbedding(d_model, n_features)\r\n-        self.positional_embedding = PositionalEmbedding(\r\n-            d_model,\r\n-            max_seq_len,\r\n-            dropout\r\n-        )\r\n+        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\r\n+        \r\n+        # Dropout for regularization\r\n+        self.dropout = nn.Dropout(p=dropout)\r\n \r\n         # Learnable weights for combining embeddings\r\n         self.alpha = nn.Parameter(torch.ones(1))\r\n         self.beta = nn.Parameter(torch.ones(1))\r\n@@ -80,7 +114,7 @@\n         Returns:\r\n             Combined embedding tensor of shape [batch_size, seq_len, d_model]\r\n         \"\"\"\r\n         value_embedded = self.value_embedding(x)\r\n-        pos_embedding = self.positional_embedding(value_embedded)\r\n+        pos_embedded = self.positional_encoding(x)\r\n \r\n\\ No newline at end of file\n-        return self.alpha * value_embedded + self.beta * pos_embedding\n+        return self.dropout(self.alpha * value_embedded + self.beta * pos_embedded)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733089989502,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -31,9 +31,9 @@\n         position = torch.arange(max_len).unsqueeze(1)\r\n         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\r\n \r\n         pe = torch.zeros(1, max_len, d_model)\r\n-        pe.require_grad = False\r\n+        pe.requires_grad_(False)\r\n         pe[0, :, 0::2] = torch.sin(position * div_term)\r\n         pe[0, :, 1::2] = torch.cos(position * div_term)\r\n         self.register_buffer('pe', pe)\r\n \r\n"
                }
            ],
            "date": 1733089841915,
            "name": "Commit-0",
            "content": "# models/components/embeddings.py\r\nimport math\r\nimport torch\r\nfrom torch import nn, Tensor\r\nfrom typing import Optional\r\n\r\nclass ValueEmbedding(nn.Module):\r\n\r\n    def __init__(self, d_model: int, time_series_features=1):\r\n        super(ValueEmbedding, self).__init__()\r\n        self.linear = nn.Linear(time_series_features, d_model)\r\n\r\n    def forward(self, x: Tensor) -> Tensor:\r\n        \"\"\"\r\n        Creates from the given tensor a linear projection.\r\n\r\n        :param x: the input tensor to project, shape: [batch_size, sequence_length, features]\r\n        :return: the projected tensor of shape: [batch_size, sequence_length, model_dimension]\r\n        \"\"\"\r\n        return self.linear(x)\r\n\r\nclass PositionalEmbedding(nn.Module):\r\n    \"\"\"Adds positional information to embeddings.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            d_model: int,\r\n            max_seq_len: int = 5000,\r\n            dropout: float = 0.1\r\n    ):\r\n        super().__init__()\r\n        self.dropout = nn.Dropout(p=dropout)\r\n\r\n        position = torch.arange(max_seq_len).unsqueeze(1)\r\n        div_term = torch.exp(\r\n            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\r\n        )\r\n\r\n        pe = torch.zeros(1, max_seq_len, d_model)\r\n        pe[0, :, 0::2] = torch.sin(position * div_term)\r\n        pe[0, :, 1::2] = torch.cos(position * div_term)\r\n\r\n        self.register_buffer('pe', pe)\r\n\r\n    def forward(self, x: Tensor) -> Tensor:\r\n        \"\"\"\r\n        Args:\r\n            x: Tensor of shape [batch_size, seq_len, d_model]\r\n        \"\"\"\r\n        x = x + self.pe[:, :x.size(1)]\r\n        return self.dropout(x)\r\n\r\nclass CombinedEmbedding(nn.Module):\r\n    \"\"\"Combines value and positional embeddings with learnable weights.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            d_model: int,\r\n            n_features: int,\r\n            max_seq_len: int = 5000,\r\n            dropout: float = 0.1\r\n    ):\r\n        super().__init__()\r\n\r\n        self.value_embedding = ValueEmbedding(d_model, n_features)\r\n        self.positional_embedding = PositionalEmbedding(\r\n            d_model,\r\n            max_seq_len,\r\n            dropout\r\n        )\r\n\r\n        # Learnable weights for combining embeddings\r\n        self.alpha = nn.Parameter(torch.ones(1))\r\n        self.beta = nn.Parameter(torch.ones(1))\r\n\r\n    def forward(self, x: Tensor) -> Tensor:\r\n        \"\"\"\r\n        Args:\r\n            x: Input tensor of shape [batch_size, seq_len, features]\r\n        Returns:\r\n            Combined embedding tensor of shape [batch_size, seq_len, d_model]\r\n        \"\"\"\r\n        value_embedded = self.value_embedding(x)\r\n        pos_embedding = self.positional_embedding(value_embedded)\r\n\r\n        return self.alpha * value_embedded + self.beta * pos_embedding"
        }
    ]
}