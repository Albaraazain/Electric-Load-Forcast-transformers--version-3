{
    "sourceFile": "models/components/embeddings.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1733089841915,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733089961946,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,59 +1,93 @@\n-# models/components/embeddings.py\r\n import math\r\n import torch\r\n from torch import nn, Tensor\r\n from typing import Optional\r\n \r\n class ValueEmbedding(nn.Module):\r\n+    \"\"\"Projects time series values to model dimension.\"\"\"\r\n \r\n-    def __init__(self, d_model: int, time_series_features=1):\r\n-        super(ValueEmbedding, self).__init__()\r\n+    def __init__(self, d_model: int, time_series_features: int = 1):\r\n+        super().__init__()\r\n         self.linear = nn.Linear(time_series_features, d_model)\r\n \r\n     def forward(self, x: Tensor) -> Tensor:\r\n         \"\"\"\r\n         Creates from the given tensor a linear projection.\r\n \r\n-        :param x: the input tensor to project, shape: [batch_size, sequence_length, features]\r\n-        :return: the projected tensor of shape: [batch_size, sequence_length, model_dimension]\r\n+        Args:\r\n+            x: the input tensor to project, shape: [batch_size, sequence_length, features]\r\n+        Returns:\r\n+            the projected tensor of shape: [batch_size, sequence_length, model_dimension]\r\n         \"\"\"\r\n         return self.linear(x)\r\n \r\n-class PositionalEmbedding(nn.Module):\r\n-    \"\"\"Adds positional information to embeddings.\"\"\"\r\n \r\n-    def __init__(\r\n-            self,\r\n-            d_model: int,\r\n-            max_seq_len: int = 5000,\r\n-            dropout: float = 0.1\r\n-    ):\r\n+class PositionalEncoding(nn.Module):\r\n+    \"\"\"Fixed sinusoidal positional encoding.\"\"\"\r\n+\r\n+    def __init__(self, d_model: int, max_len: int = 5000):\r\n         super().__init__()\r\n-        self.dropout = nn.Dropout(p=dropout)\r\n \r\n-        position = torch.arange(max_seq_len).unsqueeze(1)\r\n-        div_term = torch.exp(\r\n-            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\r\n-        )\r\n+        position = torch.arange(max_len).unsqueeze(1)\r\n+        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\r\n \r\n-        pe = torch.zeros(1, max_seq_len, d_model)\r\n+        pe = torch.zeros(1, max_len, d_model)\r\n+        pe.require_grad = False\r\n         pe[0, :, 0::2] = torch.sin(position * div_term)\r\n         pe[0, :, 1::2] = torch.cos(position * div_term)\r\n-\r\n         self.register_buffer('pe', pe)\r\n \r\n     def forward(self, x: Tensor) -> Tensor:\r\n         \"\"\"\r\n+        Creates a positional encoding for the given tensor.\r\n+\r\n         Args:\r\n-            x: Tensor of shape [batch_size, seq_len, d_model]\r\n+            x: tensor for which the pe is created, shape: [batch_size, sequence_length, model_dimension]\r\n+        Returns:\r\n+            positional encoding of dimension [1, sequence_length, model_dimension]\r\n         \"\"\"\r\n-        x = x + self.pe[:, :x.size(1)]\r\n-        return self.dropout(x)\r\n+        return self.pe[:, :x.size(1), :]\r\n \r\n-class CombinedEmbedding(nn.Module):\r\n+\r\n+class TotalEmbedding(nn.Module):\r\n     \"\"\"Combines value and positional embeddings with learnable weights.\"\"\"\r\n \r\n+    def __init__(self, d_model: int, value_features: int, time_features: int, dropout: float):\r\n+        super().__init__()\r\n+\r\n+        self.value_embedding = ValueEmbedding(d_model, value_features + time_features)\r\n+        self.positional_encoding = PositionalEncoding(d_model)\r\n+\r\n+        # Initialize learnable weights with ones for stable training\r\n+        self.linear_embedding_weight = nn.Linear(2, 1, bias=False)\r\n+        self.linear_embedding_weight.weight.data.fill_(1)\r\n+        self.dropout = nn.Dropout(p=dropout)\r\n+\r\n+    def forward(self, x: Tensor) -> Tensor:\r\n+        \"\"\"\r\n+        Projects the given tensor x on the model_dimension (in the last dimension) and combines this with a positional\r\n+        encoding (PE). The PE is added with learned weights to the projected x tensor. Dropout is applied on the final\r\n+        result.\r\n+\r\n+        Args:\r\n+            x: tensor of dimension [Batch_Size, Sequence_Length, Features]\r\n+        Returns:\r\n+            the embedded value of shape: [Batch_Size, Sequence_Length, model_dimension]\r\n+        \"\"\"\r\n+        value_embedded = self.value_embedding(x)\r\n+        pe = self.positional_encoding(x).repeat(x.shape[0], 1, 1)\r\n+\r\n+        # Combine embeddings with learned weights\r\n+        return self.dropout(\r\n+            self.linear_embedding_weight.weight[0][0] * value_embedded + \r\n+            self.linear_embedding_weight.weight[0][1] * pe\r\n+        )\r\n+\r\n+\r\n+class CombinedEmbedding(nn.Module):\r\n+    \"\"\"Alternative combined embedding with separate value and positional components.\"\"\"\r\n+\r\n     def __init__(\r\n             self,\r\n             d_model: int,\r\n             n_features: int,\r\n@@ -61,14 +95,14 @@\n             dropout: float = 0.1\r\n     ):\r\n         super().__init__()\r\n \r\n+        # Core embeddings\r\n         self.value_embedding = ValueEmbedding(d_model, n_features)\r\n-        self.positional_embedding = PositionalEmbedding(\r\n-            d_model,\r\n-            max_seq_len,\r\n-            dropout\r\n-        )\r\n+        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\r\n+        \r\n+        # Dropout for regularization\r\n+        self.dropout = nn.Dropout(p=dropout)\r\n \r\n         # Learnable weights for combining embeddings\r\n         self.alpha = nn.Parameter(torch.ones(1))\r\n         self.beta = nn.Parameter(torch.ones(1))\r\n@@ -80,7 +114,7 @@\n         Returns:\r\n             Combined embedding tensor of shape [batch_size, seq_len, d_model]\r\n         \"\"\"\r\n         value_embedded = self.value_embedding(x)\r\n-        pos_embedding = self.positional_embedding(value_embedded)\r\n+        pos_embedded = self.positional_encoding(x)\r\n \r\n\\ No newline at end of file\n-        return self.alpha * value_embedded + self.beta * pos_embedding\n+        return self.dropout(self.alpha * value_embedded + self.beta * pos_embedded)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733089989502,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -31,9 +31,9 @@\n         position = torch.arange(max_len).unsqueeze(1)\r\n         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\r\n \r\n         pe = torch.zeros(1, max_len, d_model)\r\n-        pe.require_grad = False\r\n+        pe.requires_grad_(False)\r\n         pe[0, :, 0::2] = torch.sin(position * div_term)\r\n         pe[0, :, 1::2] = torch.cos(position * div_term)\r\n         self.register_buffer('pe', pe)\r\n \r\n"
                },
                {
                    "date": 1733183062140,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,27 +2,122 @@\n import torch\r\n from torch import nn, Tensor\r\n from typing import Optional\r\n \r\n+import math\r\n+import torch\r\n+from torch import nn, Tensor\r\n+\r\n class ValueEmbedding(nn.Module):\r\n-    \"\"\"Projects time series values to model dimension.\"\"\"\r\n+    \"\"\"Projects time series input features to model dimension.\"\"\"\r\n \r\n-    def __init__(self, d_model: int, time_series_features: int = 1):\r\n+    def __init__(self, d_model: int, input_features: int = 7):\r\n+        \"\"\"\r\n+        Initialize the value embedding layer.\r\n+        \r\n+        Args:\r\n+            d_model: Target embedding dimension (e.g., 512)\r\n+            input_features: Number of input features (7 in our case: 1 value + 6 time features)\r\n+        \"\"\"\r\n         super().__init__()\r\n-        self.linear = nn.Linear(time_series_features, d_model)\r\n+        self.d_model = d_model\r\n+        self.input_features = input_features\r\n+        \r\n+        # Linear projection from input_features to d_model\r\n+        self.linear = nn.Linear(input_features, d_model)\r\n+        \r\n+        # Initialize weights for stable training\r\n+        with torch.no_grad():\r\n+            # Xavier initialization for weights\r\n+            nn.init.xavier_uniform_(self.linear.weight)\r\n+            if self.linear.bias is not None:\r\n+                nn.init.zeros_(self.linear.bias)\r\n \r\n     def forward(self, x: Tensor) -> Tensor:\r\n         \"\"\"\r\n-        Creates from the given tensor a linear projection.\r\n-\r\n+        Project input features to model dimension.\r\n+        \r\n         Args:\r\n-            x: the input tensor to project, shape: [batch_size, sequence_length, features]\r\n+            x: Input tensor of shape [batch_size, seq_len, input_features]\r\n         Returns:\r\n-            the projected tensor of shape: [batch_size, sequence_length, model_dimension]\r\n+            Projected tensor of shape [batch_size, seq_len, d_model]\r\n         \"\"\"\r\n+        # Ensure input has correct shape\r\n+        if len(x.shape) != 3:\r\n+            x = x.unsqueeze(0)  # Add batch dimension if missing\r\n+            \r\n         return self.linear(x)\r\n \r\n+class CombinedEmbedding(nn.Module):\r\n+    \"\"\"Combines value and positional embeddings for transformer input.\"\"\"\r\n \r\n+    def __init__(\r\n+            self,\r\n+            d_model: int,\r\n+            input_features: int,\r\n+            max_seq_len: int = 5000,\r\n+            dropout: float = 0.1\r\n+    ):\r\n+        \"\"\"\r\n+        Initialize combined embedding module.\r\n+        \r\n+        Args:\r\n+            d_model: Model dimension (e.g., 512)\r\n+            input_features: Number of input features (7)\r\n+            max_seq_len: Maximum sequence length for positional encoding\r\n+            dropout: Dropout rate\r\n+        \"\"\"\r\n+        super().__init__()\r\n+        self.d_model = d_model\r\n+        \r\n+        # Project values to model dimension\r\n+        self.value_embedding = ValueEmbedding(d_model, input_features)\r\n+        \r\n+        # Create fixed positional encoding\r\n+        position = torch.arange(max_seq_len).unsqueeze(1)\r\n+        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\r\n+        \r\n+        pe = torch.zeros(1, max_seq_len, d_model)\r\n+        pe[0, :, 0::2] = torch.sin(position * div_term)\r\n+        pe[0, :, 1::2] = torch.cos(position * div_term)\r\n+        self.register_buffer('pe', pe)\r\n+        \r\n+        # Learnable combination weights\r\n+        self.alpha = nn.Parameter(torch.ones(1))\r\n+        self.beta = nn.Parameter(torch.ones(1))\r\n+        \r\n+        # Dropout for regularization\r\n+        self.dropout = nn.Dropout(dropout)\r\n+\r\n+    def forward(self, x: Tensor) -> Tensor:\r\n+        \"\"\"\r\n+        Create combined embedding.\r\n+        \r\n+        Args:\r\n+            x: Input tensor [batch_size, seq_len, input_features]\r\n+        Returns:\r\n+            Combined embedding [batch_size, seq_len, d_model]\r\n+        \"\"\"\r\n+        # Ensure input has batch dimension\r\n+        if len(x.shape) == 2:\r\n+            x = x.unsqueeze(0)\r\n+            \r\n+        # Get sequence length\r\n+        seq_len = x.size(1)\r\n+        \r\n+        # Create embeddings\r\n+        value_emb = self.value_embedding(x)\r\n+        pos_emb = self.pe[:, :seq_len]\r\n+        \r\n+        # Scale by sqrt(d_model) for stable gradients\r\n+        scale = math.sqrt(self.d_model)\r\n+        \r\n+        # Combine embeddings with learnable weights\r\n+        combined = (self.alpha * value_emb + self.beta * pos_emb) / scale\r\n+        \r\n+        return self.dropout(combined)\r\n+\r\n+\r\n class PositionalEncoding(nn.Module):\r\n     \"\"\"Fixed sinusoidal positional encoding.\"\"\"\r\n \r\n     def __init__(self, d_model: int, max_len: int = 5000):\r\n@@ -80,41 +175,5 @@\n         # Combine embeddings with learned weights\r\n         return self.dropout(\r\n             self.linear_embedding_weight.weight[0][0] * value_embedded + \r\n             self.linear_embedding_weight.weight[0][1] * pe\r\n-        )\r\n-\r\n-\r\n-class CombinedEmbedding(nn.Module):\r\n-    \"\"\"Alternative combined embedding with separate value and positional components.\"\"\"\r\n-\r\n-    def __init__(\r\n-            self,\r\n-            d_model: int,\r\n-            n_features: int,\r\n-            max_seq_len: int = 5000,\r\n-            dropout: float = 0.1\r\n-    ):\r\n-        super().__init__()\r\n-\r\n-        # Core embeddings\r\n-        self.value_embedding = ValueEmbedding(d_model, n_features)\r\n-        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\r\n-        \r\n-        # Dropout for regularization\r\n-        self.dropout = nn.Dropout(p=dropout)\r\n-\r\n-        # Learnable weights for combining embeddings\r\n-        self.alpha = nn.Parameter(torch.ones(1))\r\n-        self.beta = nn.Parameter(torch.ones(1))\r\n-\r\n-    def forward(self, x: Tensor) -> Tensor:\r\n-        \"\"\"\r\n-        Args:\r\n-            x: Input tensor of shape [batch_size, seq_len, features]\r\n-        Returns:\r\n-            Combined embedding tensor of shape [batch_size, seq_len, d_model]\r\n-        \"\"\"\r\n-        value_embedded = self.value_embedding(x)\r\n-        pos_embedded = self.positional_encoding(x)\r\n-\r\n-        return self.dropout(self.alpha * value_embedded + self.beta * pos_embedded)\n\\ No newline at end of file\n+        )\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733237263454,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,26 +10,19 @@\n class ValueEmbedding(nn.Module):\r\n     \"\"\"Projects time series input features to model dimension.\"\"\"\r\n \r\n     def __init__(self, d_model: int, input_features: int = 7):\r\n-        \"\"\"\r\n-        Initialize the value embedding layer.\r\n-        \r\n-        Args:\r\n-            d_model: Target embedding dimension (e.g., 512)\r\n-            input_features: Number of input features (7 in our case: 1 value + 6 time features)\r\n-        \"\"\"\r\n         super().__init__()\r\n         self.d_model = d_model\r\n         self.input_features = input_features\r\n         \r\n-        # Linear projection from input_features to d_model\r\n+        # Fix: Correct initialization to match input and output dimensions\r\n         self.linear = nn.Linear(input_features, d_model)\r\n         \r\n         # Initialize weights for stable training\r\n         with torch.no_grad():\r\n-            # Xavier initialization for weights\r\n-            nn.init.xavier_uniform_(self.linear.weight)\r\n+            # Xavier initialization with correct gain for linear layer\r\n+            nn.init.xavier_uniform_(self.linear.weight, gain=1/math.sqrt(input_features))\r\n             if self.linear.bias is not None:\r\n                 nn.init.zeros_(self.linear.bias)\r\n \r\n     def forward(self, x: Tensor) -> Tensor:\r\n@@ -40,11 +33,14 @@\n             x: Input tensor of shape [batch_size, seq_len, input_features]\r\n         Returns:\r\n             Projected tensor of shape [batch_size, seq_len, d_model]\r\n         \"\"\"\r\n-        # Ensure input has correct shape\r\n-        if len(x.shape) != 3:\r\n-            x = x.unsqueeze(0)  # Add batch dimension if missing\r\n+        # Add assertion to catch dimension mismatches\r\n+        if x.size(-1) != self.input_features:\r\n+            raise ValueError(\r\n+                f\"Expected input features dimension to be {self.input_features}, \"\r\n+                f\"but got {x.size(-1)}\"\r\n+            )\r\n             \r\n         return self.linear(x)\r\n \r\n class CombinedEmbedding(nn.Module):\r\n@@ -176,5 +172,4 @@\n         return self.dropout(\r\n             self.linear_embedding_weight.weight[0][0] * value_embedded + \r\n             self.linear_embedding_weight.weight[0][1] * pe\r\n         )\r\n-        \n\\ No newline at end of file\n"
                }
            ],
            "date": 1733089841915,
            "name": "Commit-0",
            "content": "# models/components/embeddings.py\r\nimport math\r\nimport torch\r\nfrom torch import nn, Tensor\r\nfrom typing import Optional\r\n\r\nclass ValueEmbedding(nn.Module):\r\n\r\n    def __init__(self, d_model: int, time_series_features=1):\r\n        super(ValueEmbedding, self).__init__()\r\n        self.linear = nn.Linear(time_series_features, d_model)\r\n\r\n    def forward(self, x: Tensor) -> Tensor:\r\n        \"\"\"\r\n        Creates from the given tensor a linear projection.\r\n\r\n        :param x: the input tensor to project, shape: [batch_size, sequence_length, features]\r\n        :return: the projected tensor of shape: [batch_size, sequence_length, model_dimension]\r\n        \"\"\"\r\n        return self.linear(x)\r\n\r\nclass PositionalEmbedding(nn.Module):\r\n    \"\"\"Adds positional information to embeddings.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            d_model: int,\r\n            max_seq_len: int = 5000,\r\n            dropout: float = 0.1\r\n    ):\r\n        super().__init__()\r\n        self.dropout = nn.Dropout(p=dropout)\r\n\r\n        position = torch.arange(max_seq_len).unsqueeze(1)\r\n        div_term = torch.exp(\r\n            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\r\n        )\r\n\r\n        pe = torch.zeros(1, max_seq_len, d_model)\r\n        pe[0, :, 0::2] = torch.sin(position * div_term)\r\n        pe[0, :, 1::2] = torch.cos(position * div_term)\r\n\r\n        self.register_buffer('pe', pe)\r\n\r\n    def forward(self, x: Tensor) -> Tensor:\r\n        \"\"\"\r\n        Args:\r\n            x: Tensor of shape [batch_size, seq_len, d_model]\r\n        \"\"\"\r\n        x = x + self.pe[:, :x.size(1)]\r\n        return self.dropout(x)\r\n\r\nclass CombinedEmbedding(nn.Module):\r\n    \"\"\"Combines value and positional embeddings with learnable weights.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            d_model: int,\r\n            n_features: int,\r\n            max_seq_len: int = 5000,\r\n            dropout: float = 0.1\r\n    ):\r\n        super().__init__()\r\n\r\n        self.value_embedding = ValueEmbedding(d_model, n_features)\r\n        self.positional_embedding = PositionalEmbedding(\r\n            d_model,\r\n            max_seq_len,\r\n            dropout\r\n        )\r\n\r\n        # Learnable weights for combining embeddings\r\n        self.alpha = nn.Parameter(torch.ones(1))\r\n        self.beta = nn.Parameter(torch.ones(1))\r\n\r\n    def forward(self, x: Tensor) -> Tensor:\r\n        \"\"\"\r\n        Args:\r\n            x: Input tensor of shape [batch_size, seq_len, features]\r\n        Returns:\r\n            Combined embedding tensor of shape [batch_size, seq_len, d_model]\r\n        \"\"\"\r\n        value_embedded = self.value_embedding(x)\r\n        pos_embedding = self.positional_embedding(value_embedded)\r\n\r\n        return self.alpha * value_embedded + self.beta * pos_embedding"
        }
    ]
}