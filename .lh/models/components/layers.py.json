{
    "sourceFile": "models/components/layers.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 4,
            "patches": [
                {
                    "date": 1733089683995,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733089709792,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,8 +2,10 @@\n import torch\r\n from torch import nn, Tensor\r\n from typing import Optional, Callable, Union\r\n \r\n+from models.components.activation import GELU, ReLU\r\n+\r\n from .attention import (\r\n     MultiHeadAttention,\r\n     ConvolutionalAttention,\r\n     ProbSparseAttention\r\n@@ -42,18 +44,39 @@\n \r\n class EncoderLayer(nn.Module):\r\n     \"\"\"Transformer encoder layer.\"\"\"\r\n     \r\n-    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float,\r\n-                 attention_type: str = \"standard\", activation: str = \"relu\"):\r\n+    def __init__(\r\n+        self, \r\n+        d_model: int,\r\n+        n_heads: int, \r\n+        d_ff: int,\r\n+        dropout: float,\r\n+        attention_type: str = \"standard\",\r\n+        kernel_size: int = 3,\r\n+        activation: str = \"relu\",\r\n+        batch_first: bool = True\r\n+    ):\r\n         super().__init__()\r\n-        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\r\n-        self.feed_forward = nn.Sequential(\r\n-            nn.Linear(d_model, d_ff),\r\n-            nn.ReLU() if activation == \"relu\" else nn.GELU(),\r\n-            nn.Dropout(dropout),\r\n-            nn.Linear(d_ff, d_model)\r\n-        )\r\n+        \r\n+        # Select attention mechanism\r\n+        if attention_type == \"standard\":\r\n+            self.self_attn = MultiHeadAttention(\r\n+                d_model, n_heads, dropout=dropout, batch_first=batch_first\r\n+            )\r\n+        elif attention_type == \"convolutional\":\r\n+            self.self_attn = ConvolutionalAttention(\r\n+                d_model, n_heads, kernel_size=kernel_size, dropout=dropout\r\n+            )\r\n+        elif attention_type == \"prob_sparse\":\r\n+            self.self_attn = ProbSparseAttention(\r\n+                d_model, n_heads, dropout=dropout\r\n+            )\r\n+        else:\r\n+            raise ValueError(f\"Unknown attention type: {attention_type}\")\r\n+\r\n+        # Feed forward and normalization\r\n+        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout, activation)\r\n         self.norm1 = nn.LayerNorm(d_model)\r\n         self.norm2 = nn.LayerNorm(d_model)\r\n         self.dropout = nn.Dropout(dropout)\r\n \r\n"
                },
                {
                    "date": 1733089729558,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -79,13 +79,28 @@\n         self.norm1 = nn.LayerNorm(d_model)\r\n         self.norm2 = nn.LayerNorm(d_model)\r\n         self.dropout = nn.Dropout(dropout)\r\n \r\n-    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n-        attn_output, _ = self.self_attn(x, x, x)\r\n-        x = self.norm1(x + self.dropout(attn_output))\r\n-        ff_output = self.feed_forward(x)\r\n-        return self.norm2(x + self.dropout(ff_output))\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None\r\n+    ) -> Tensor:\r\n+        # Self attention block\r\n+        attn_output, _ = self.self_attn(\r\n+            src, src, src,\r\n+            attn_mask=src_mask,\r\n+            key_padding_mask=src_key_padding_mask,\r\n+            need_weights=False\r\n+        )\r\n+        src = self.norm1(src + self.dropout(attn_output))\r\n+        \r\n+        # Feed forward block\r\n+        ff_output = self.feed_forward(src)\r\n+        src = self.norm2(src + self.dropout(ff_output))\r\n+        \r\n+        return src\r\n \r\n \r\n class DecoderLayer(nn.Module):\r\n     \"\"\"Transformer decoder layer.\"\"\"\r\n"
                },
                {
                    "date": 1733089739413,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,21 +102,50 @@\n         return src\r\n \r\n \r\n class DecoderLayer(nn.Module):\r\n-    \"\"\"Transformer decoder layer.\"\"\"\r\n+    \"\"\"Transformer decoder layer with improved implementation.\"\"\"\r\n     \r\n-    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float,\r\n-                 attention_type: str = \"standard\", activation: str = \"relu\"):\r\n+    def __init__(\r\n+        self,\r\n+        d_model: int,\r\n+        n_heads: int,\r\n+        d_ff: int,\r\n+        dropout: float,\r\n+        attention_type: str = \"standard\",\r\n+        kernel_size: int = 3,\r\n+        activation: str = \"relu\",\r\n+        batch_first: bool = True\r\n+    ):\r\n         super().__init__()\r\n-        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\r\n-        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\r\n-        self.feed_forward = nn.Sequential(\r\n-            nn.Linear(d_model, d_ff),\r\n-            nn.ReLU() if activation == \"relu\" else nn.GELU(),\r\n-            nn.Dropout(dropout),\r\n-            nn.Linear(d_ff, d_model)\r\n-        )\r\n+        \r\n+        # Select attention mechanisms\r\n+        if attention_type == \"standard\":\r\n+            self.self_attn = MultiHeadAttention(\r\n+                d_model, n_heads, dropout=dropout, batch_first=batch_first\r\n+            )\r\n+            self.cross_attn = MultiHeadAttention(\r\n+                d_model, n_heads, dropout=dropout, batch_first=batch_first\r\n+            )\r\n+        elif attention_type == \"convolutional\":\r\n+            self.self_attn = ConvolutionalAttention(\r\n+                d_model, n_heads, kernel_size=kernel_size, dropout=dropout\r\n+            )\r\n+            self.cross_attn = ConvolutionalAttention(\r\n+                d_model, n_heads, kernel_size=kernel_size, dropout=dropout\r\n+            )\r\n+        elif attention_type == \"prob_sparse\":\r\n+            self.self_attn = ProbSparseAttention(\r\n+                d_model, n_heads, dropout=dropout\r\n+            )\r\n+            self.cross_attn = ProbSparseAttention(\r\n+                d_model, n_heads, dropout=dropout\r\n+            )\r\n+        else:\r\n+            raise ValueError(f\"Unknown attention type: {attention_type}\")\r\n+\r\n+        # Feed forward and normalization\r\n+        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout, activation)\r\n         self.norm1 = nn.LayerNorm(d_model)\r\n         self.norm2 = nn.LayerNorm(d_model)\r\n         self.norm3 = nn.LayerNorm(d_model)\r\n         self.dropout = nn.Dropout(dropout)\r\n"
                },
                {
                    "date": 1733089747672,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,184 @@\n+# models/components/layers.py\r\n+import torch\r\n+from torch import nn, Tensor\r\n+from typing import Optional, Callable, Union\r\n+\r\n+from models.components.activation import GELU, ReLU\r\n+\r\n+from .attention import (\r\n+    MultiHeadAttention,\r\n+    ConvolutionalAttention,\r\n+    ProbSparseAttention\r\n+)\r\n+\r\n+\r\n+class FeedForwardNetwork(nn.Module):\r\n+    \"\"\"Standard feed-forward network used in transformers.\"\"\"\r\n+\r\n+    def __init__(\r\n+            self,\r\n+            d_model: int,\r\n+            d_ff: int,\r\n+            dropout: float = 0.1,\r\n+            activation: str = \"relu\"\r\n+    ):\r\n+        super().__init__()\r\n+\r\n+        self.linear1 = nn.Linear(d_model, d_ff)\r\n+        self.linear2 = nn.Linear(d_ff, d_model)\r\n+        self.dropout = nn.Dropout(dropout)\r\n+        \r\n+        # Use our new activation functions\r\n+        self.activation = GELU() if activation == \"gelu\" else ReLU()\r\n+\r\n+    def forward(self, x: Tensor) -> Tensor:\r\n+        return self.linear2(\r\n+            self.dropout(\r\n+                self.activation(\r\n+                    self.linear1(x)\r\n+                )\r\n+            )\r\n+        )\r\n+\r\n+\r\n+\r\n+class EncoderLayer(nn.Module):\r\n+    \"\"\"Transformer encoder layer.\"\"\"\r\n+    \r\n+    def __init__(\r\n+        self, \r\n+        d_model: int,\r\n+        n_heads: int, \r\n+        d_ff: int,\r\n+        dropout: float,\r\n+        attention_type: str = \"standard\",\r\n+        kernel_size: int = 3,\r\n+        activation: str = \"relu\",\r\n+        batch_first: bool = True\r\n+    ):\r\n+        super().__init__()\r\n+        \r\n+        # Select attention mechanism\r\n+        if attention_type == \"standard\":\r\n+            self.self_attn = MultiHeadAttention(\r\n+                d_model, n_heads, dropout=dropout, batch_first=batch_first\r\n+            )\r\n+        elif attention_type == \"convolutional\":\r\n+            self.self_attn = ConvolutionalAttention(\r\n+                d_model, n_heads, kernel_size=kernel_size, dropout=dropout\r\n+            )\r\n+        elif attention_type == \"prob_sparse\":\r\n+            self.self_attn = ProbSparseAttention(\r\n+                d_model, n_heads, dropout=dropout\r\n+            )\r\n+        else:\r\n+            raise ValueError(f\"Unknown attention type: {attention_type}\")\r\n+\r\n+        # Feed forward and normalization\r\n+        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout, activation)\r\n+        self.norm1 = nn.LayerNorm(d_model)\r\n+        self.norm2 = nn.LayerNorm(d_model)\r\n+        self.dropout = nn.Dropout(dropout)\r\n+\r\n+    def forward(\r\n+        self,\r\n+        src: Tensor,\r\n+        src_mask: Optional[Tensor] = None,\r\n+        src_key_padding_mask: Optional[Tensor] = None\r\n+    ) -> Tensor:\r\n+        # Self attention block\r\n+        attn_output, _ = self.self_attn(\r\n+            src, src, src,\r\n+            attn_mask=src_mask,\r\n+            key_padding_mask=src_key_padding_mask,\r\n+            need_weights=False\r\n+        )\r\n+        src = self.norm1(src + self.dropout(attn_output))\r\n+        \r\n+        # Feed forward block\r\n+        ff_output = self.feed_forward(src)\r\n+        src = self.norm2(src + self.dropout(ff_output))\r\n+        \r\n+        return src\r\n+\r\n+\r\n+class DecoderLayer(nn.Module):\r\n+    \"\"\"Transformer decoder layer with improved implementation.\"\"\"\r\n+    \r\n+    def __init__(\r\n+        self,\r\n+        d_model: int,\r\n+        n_heads: int,\r\n+        d_ff: int,\r\n+        dropout: float,\r\n+        attention_type: str = \"standard\",\r\n+        kernel_size: int = 3,\r\n+        activation: str = \"relu\",\r\n+        batch_first: bool = True\r\n+    ):\r\n+        super().__init__()\r\n+        \r\n+        # Select attention mechanisms\r\n+        if attention_type == \"standard\":\r\n+            self.self_attn = MultiHeadAttention(\r\n+                d_model, n_heads, dropout=dropout, batch_first=batch_first\r\n+            )\r\n+            self.cross_attn = MultiHeadAttention(\r\n+                d_model, n_heads, dropout=dropout, batch_first=batch_first\r\n+            )\r\n+        elif attention_type == \"convolutional\":\r\n+            self.self_attn = ConvolutionalAttention(\r\n+                d_model, n_heads, kernel_size=kernel_size, dropout=dropout\r\n+            )\r\n+            self.cross_attn = ConvolutionalAttention(\r\n+                d_model, n_heads, kernel_size=kernel_size, dropout=dropout\r\n+            )\r\n+        elif attention_type == \"prob_sparse\":\r\n+            self.self_attn = ProbSparseAttention(\r\n+                d_model, n_heads, dropout=dropout\r\n+            )\r\n+            self.cross_attn = ProbSparseAttention(\r\n+                d_model, n_heads, dropout=dropout\r\n+            )\r\n+        else:\r\n+            raise ValueError(f\"Unknown attention type: {attention_type}\")\r\n+\r\n+        # Feed forward and normalization\r\n+        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout, activation)\r\n+        self.norm1 = nn.LayerNorm(d_model)\r\n+        self.norm2 = nn.LayerNorm(d_model)\r\n+        self.norm3 = nn.LayerNorm(d_model)\r\n+        self.dropout = nn.Dropout(dropout)\r\n+\r\n+    def forward(\r\n+        self,\r\n+        tgt: Tensor,\r\n+        memory: Tensor,\r\n+        tgt_mask: Optional[Tensor] = None,\r\n+        memory_mask: Optional[Tensor] = None,\r\n+        tgt_key_padding_mask: Optional[Tensor] = None,\r\n+        memory_key_padding_mask: Optional[Tensor] = None\r\n+    ) -> Tensor:\r\n+        # Self attention block\r\n+        self_attn_output, _ = self.self_attn(\r\n+            tgt, tgt, tgt,\r\n+            attn_mask=tgt_mask,\r\n+            key_padding_mask=tgt_key_padding_mask,\r\n+            need_weights=False\r\n+        )\r\n+        tgt = self.norm1(tgt + self.dropout(self_attn_output))\r\n+        \r\n+        # Cross attention block\r\n+        cross_attn_output, _ = self.cross_attn(\r\n+            tgt, memory, memory,\r\n+            attn_mask=memory_mask,\r\n+            key_padding_mask=memory_key_padding_mask,\r\n+            need_weights=False\r\n+        )\r\n+        tgt = self.norm2(tgt + self.dropout(cross_attn_output))\r\n+        \r\n+        # Feed forward block\r\n+        ff_output = self.feed_forward(tgt)\r\n+        tgt = self.norm3(tgt + self.dropout(ff_output))\r\n+        \r\n+        return tgt\r\n"
                }
            ],
            "date": 1733089683995,
            "name": "Commit-0",
            "content": "# models/components/layers.py\r\nimport torch\r\nfrom torch import nn, Tensor\r\nfrom typing import Optional, Callable, Union\r\n\r\nfrom .attention import (\r\n    MultiHeadAttention,\r\n    ConvolutionalAttention,\r\n    ProbSparseAttention\r\n)\r\n\r\n\r\nclass FeedForwardNetwork(nn.Module):\r\n    \"\"\"Standard feed-forward network used in transformers.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            d_model: int,\r\n            d_ff: int,\r\n            dropout: float = 0.1,\r\n            activation: str = \"relu\"\r\n    ):\r\n        super().__init__()\r\n\r\n        self.linear1 = nn.Linear(d_model, d_ff)\r\n        self.linear2 = nn.Linear(d_ff, d_model)\r\n        self.dropout = nn.Dropout(dropout)\r\n        \r\n        # Use our new activation functions\r\n        self.activation = GELU() if activation == \"gelu\" else ReLU()\r\n\r\n    def forward(self, x: Tensor) -> Tensor:\r\n        return self.linear2(\r\n            self.dropout(\r\n                self.activation(\r\n                    self.linear1(x)\r\n                )\r\n            )\r\n        )\r\n\r\n\r\n\r\nclass EncoderLayer(nn.Module):\r\n    \"\"\"Transformer encoder layer.\"\"\"\r\n    \r\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float,\r\n                 attention_type: str = \"standard\", activation: str = \"relu\"):\r\n        super().__init__()\r\n        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\r\n        self.feed_forward = nn.Sequential(\r\n            nn.Linear(d_model, d_ff),\r\n            nn.ReLU() if activation == \"relu\" else nn.GELU(),\r\n            nn.Dropout(dropout),\r\n            nn.Linear(d_ff, d_model)\r\n        )\r\n        self.norm1 = nn.LayerNorm(d_model)\r\n        self.norm2 = nn.LayerNorm(d_model)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n        attn_output, _ = self.self_attn(x, x, x)\r\n        x = self.norm1(x + self.dropout(attn_output))\r\n        ff_output = self.feed_forward(x)\r\n        return self.norm2(x + self.dropout(ff_output))\r\n\r\n\r\nclass DecoderLayer(nn.Module):\r\n    \"\"\"Transformer decoder layer.\"\"\"\r\n    \r\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float,\r\n                 attention_type: str = \"standard\", activation: str = \"relu\"):\r\n        super().__init__()\r\n        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\r\n        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\r\n        self.feed_forward = nn.Sequential(\r\n            nn.Linear(d_model, d_ff),\r\n            nn.ReLU() if activation == \"relu\" else nn.GELU(),\r\n            nn.Dropout(dropout),\r\n            nn.Linear(d_ff, d_model)\r\n        )\r\n        self.norm1 = nn.LayerNorm(d_model)\r\n        self.norm2 = nn.LayerNorm(d_model)\r\n        self.norm3 = nn.LayerNorm(d_model)\r\n        self.dropout = nn.Dropout(dropout)\r\n\r\n    def forward(self, x: torch.Tensor, memory: torch.Tensor) -> torch.Tensor:\r\n        attn_output, _ = self.self_attn(x, x, x)\r\n        x = self.norm1(x + self.dropout(attn_output))\r\n        \r\n        cross_attn_output, _ = self.cross_attn(x, memory, memory)\r\n        x = self.norm2(x + self.dropout(cross_attn_output))\r\n        \r\n        ff_output = self.feed_forward(x)\r\n        return self.norm3(x + self.dropout(ff_output))\r\n"
        }
    ]
}