{
    "sourceFile": "models/wrappers/pytorch_wrapper.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 6,
            "patches": [
                {
                    "date": 1733003057468,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733065659398,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,8 +38,9 @@\n         # Training configuration\r\n         self.batch_size = config.get('batch_size', 32)\r\n         self.learning_rate = config.get('learning_rate', 1e-3)\r\n         self.max_epochs = config.get('max_epochs', 100)\r\n+        self.gradient_clip_val = config.get('gradient_clipping', 1.0)\r\n \r\n         # Setup optimizer and scheduler\r\n         self.optimizer = self._setup_optimizer()\r\n         self.scheduler = self._setup_scheduler()\r\n"
                },
                {
                    "date": 1733065681777,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,11 +46,15 @@\n         self.scheduler = self._setup_scheduler()\r\n         self.criterion = self._setup_criterion()\r\n \r\n     def _setup_optimizer(self) -> Optimizer:\r\n-        \"\"\"Initialize optimizer based on config.\"\"\"\r\n-        optimizer_name = self.config.get('optimizer', 'adam').lower()\r\n-        optimizer_config = self.config.get('optimizer_config', {})\r\n+        \"\"\"Initialize optimizer with improved defaults.\"\"\"\r\n+        optimizer_name = self.config.get('optimizer', 'adamw').lower()  # Default changed to AdamW\r\n+        optimizer_config = self.config.get('optimizer_config', {\r\n+            'weight_decay': 0.01,  # Add L2 regularization\r\n+            'betas': (0.9, 0.999),\r\n+            'eps': 1e-8\r\n+        })\r\n \r\n         optimizers: Dict[str, Optimizer] = {\r\n             'adam': Adam(\r\n                 self.model.parameters(),\r\n@@ -64,8 +68,9 @@\n             ),\r\n             'sgd': SGD(\r\n                 self.model.parameters(),\r\n                 lr=self.learning_rate,\r\n+                momentum=0.9,\r\n                 **optimizer_config\r\n             )\r\n         }\r\n \r\n"
                },
                {
                    "date": 1733065705763,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,257 @@\n+# models/wrappers/pytorch_wrapper.py\r\n+from __future__ import annotations\r\n+from pathlib import Path\r\n+from typing import Union\r\n+from typing import Union\r\n+from typing import Any, Dict, List, Optional, Tuple, Union, Type\r\n+import torch\r\n+from torch import nn\r\n+from os import PathLike\r\n+from torch.optim.adam import Adam\r\n+from torch.optim.adamw import AdamW\r\n+from torch.optim.sgd import SGD\r\n+from torch.optim import lr_scheduler\r\n+import torch.optim as optim\r\n+from torch.optim.optimizer import Optimizer\r\n+from torch.utils.data import Dataset as TorchDataset  # Rename to avoid conflict\r\n+from torch.utils.data import DataLoader\r\n+\r\n+from models.losses.custom_losses import MAPE\r\n+from training.base.base_trainer import TrainingEpoch\r\n+from training.reports.training_report import TrainingReport\r\n+\r\n+from ..interfaces import WrapperInterface\r\n+from ..base.base_model import BaseModel\r\n+from ..registry.model_types import ModelType\r\n+\r\n+\r\n+class PyTorchWrapper(WrapperInterface):\r\n+    \"\"\"Wrapper for PyTorch models providing consistent training and inference interface.\"\"\"\r\n+\r\n+    def __init__(self, model: BaseModel, model_type: ModelType, config: Dict[str, Any]):\r\n+        self.model = model\r\n+        self.model_type = model_type\r\n+        self.config = config\r\n+        self.device = config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\r\n+        self.model.to(self.device)\r\n+\r\n+        # Training configuration\r\n+        self.batch_size = config.get('batch_size', 32)\r\n+        self.learning_rate = config.get('learning_rate', 1e-3)\r\n+        self.max_epochs = config.get('max_epochs', 100)\r\n+        self.gradient_clip_val = config.get('gradient_clipping', 1.0)\r\n+\r\n+        # Setup optimizer and scheduler\r\n+        self.optimizer = self._setup_optimizer()\r\n+        self.scheduler = self._setup_scheduler()\r\n+        self.criterion = self._setup_criterion()\r\n+\r\n+    def _setup_optimizer(self) -> Optimizer:\r\n+        \"\"\"Initialize optimizer with improved defaults.\"\"\"\r\n+        optimizer_name = self.config.get('optimizer', 'adamw').lower()  # Default changed to AdamW\r\n+        optimizer_config = self.config.get('optimizer_config', {\r\n+            'weight_decay': 0.01,  # Add L2 regularization\r\n+            'betas': (0.9, 0.999),\r\n+            'eps': 1e-8\r\n+        })\r\n+\r\n+        optimizers: Dict[str, Optimizer] = {\r\n+            'adam': Adam(\r\n+                self.model.parameters(),\r\n+                lr=self.learning_rate,\r\n+                **optimizer_config\r\n+            ),\r\n+            'adamw': AdamW(\r\n+                self.model.parameters(),\r\n+                lr=self.learning_rate,\r\n+                **optimizer_config\r\n+            ),\r\n+            'sgd': SGD(\r\n+                self.model.parameters(),\r\n+                lr=self.learning_rate,\r\n+                momentum=0.9,\r\n+                **optimizer_config\r\n+            )\r\n+        }\r\n+\r\n+        if optimizer_name not in optimizers:\r\n+            raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\r\n+\r\n+        return optimizers[optimizer_name]\r\n+\r\n+    def _setup_scheduler(self) -> Optional[Union[lr_scheduler.StepLR, lr_scheduler.CosineAnnealingLR, lr_scheduler.ReduceLROnPlateau]]:\r\n+        \"\"\"Initialize learning rate scheduler with improved defaults.\"\"\"\r\n+        scheduler_name = self.config.get('scheduler', 'cosine')  # Default changed to cosine\r\n+        \r\n+        scheduler_config = self.config.get('scheduler_config', {\r\n+            'T_max': self.max_epochs,\r\n+            'eta_min': 1e-6\r\n+        })\r\n+\r\n+        schedulers = {\r\n+            'step': optim.lr_scheduler.StepLR(\r\n+                self.optimizer,\r\n+                step_size=30,\r\n+                gamma=0.1\r\n+            ),\r\n+            'cosine': optim.lr_scheduler.CosineAnnealingLR(\r\n+                self.optimizer,\r\n+                **scheduler_config\r\n+            ),\r\n+            'plateau': optim.lr_scheduler.ReduceLROnPlateau(\r\n+                self.optimizer,\r\n+                mode='min',\r\n+                factor=0.5,\r\n+                patience=5,\r\n+                min_lr=1e-6\r\n+            )\r\n+        }\r\n+\r\n+        if scheduler_name not in schedulers:\r\n+            raise ValueError(f\"Unknown scheduler: {scheduler_name}\")\r\n+\r\n+        return schedulers[scheduler_name]\r\n+\r\n+\r\n+    def _setup_criterion(self) -> nn.Module:\r\n+        \"\"\"Initialize loss function based on config.\"\"\"\r\n+        criterion_name = self.config.get('criterion', 'mse').lower()\r\n+        criterion_config = self.config.get('criterion_config', {})\r\n+\r\n+        criteria: Dict[str, nn.Module] = {\r\n+            'mse': nn.MSELoss(**criterion_config),\r\n+            'mae': nn.L1Loss(**criterion_config),\r\n+            'mape': MAPE(**criterion_config)\r\n+        }\r\n+\r\n+        if criterion_name not in criteria:\r\n+            raise ValueError(f\"Unknown criterion: {criterion_name}\")\r\n+\r\n+        return criteria[criterion_name]\r\n+\r\n+    def train(\r\n+            self,\r\n+            train_dataset: TorchDataset,\r\n+            validation_dataset: Optional[TorchDataset] = None\r\n+    ) -> TrainingReport:\r\n+        \"\"\"Train the model.\"\"\"\r\n+        train_loader = DataLoader(\r\n+            train_dataset,\r\n+            batch_size=self.batch_size,\r\n+            shuffle=True\r\n+        )\r\n+\r\n+        val_loader = None\r\n+        if validation_dataset is not None:\r\n+            val_loader = DataLoader(\r\n+                validation_dataset,\r\n+                batch_size=self.batch_size\r\n+            )\r\n+\r\n+        train_losses = []\r\n+        val_losses = []\r\n+        learning_rates = []\r\n+\r\n+        report = TrainingReport(\r\n+            train_losses=train_losses,\r\n+            val_losses=val_losses,\r\n+            learning_rates=learning_rates,\r\n+            epochs=self.max_epochs\r\n+        )\r\n+\r\n+        for epoch in range(self.max_epochs):\r\n+            self.model.train()\r\n+            train_loss = 0.0\r\n+\r\n+            # Add epoch progress logging\r\n+            print(f\"\\nEpoch [{epoch+1}/{self.max_epochs}]\")\r\n+            \r\n+            for batch_idx, (data, target) in enumerate(train_loader):\r\n+                try:\r\n+                    data, target = data.to(self.device), target.to(self.device)\r\n+                    self.optimizer.zero_grad()\r\n+                    output = self.model(data)\r\n+                    loss = self.criterion(output, target)\r\n+                    loss.backward()\r\n+                    self.optimizer.step()\r\n+\r\n+                    train_loss += loss.item()\r\n+\r\n+                    # Print batch progress every 50 batches\r\n+                    if (batch_idx + 1) % 50 == 0:\r\n+                        print(f\"Batch [{batch_idx+1}/{len(train_loader)}] - Loss: {loss.item():.4f}\")\r\n+\r\n+                except RuntimeError as e:\r\n+                    print(f\"Error in batch {batch_idx}: {str(e)}\")\r\n+                    raise e\r\n+\r\n+            train_loss /= len(train_loader)\r\n+            train_losses.append(train_loss)\r\n+\r\n+            # Get current learning rate\r\n+            current_lr = self.optimizer.param_groups[0]['lr']\r\n+            learning_rates.append(current_lr)\r\n+\r\n+            # Print epoch summary\r\n+            print(f\"Training Loss: {train_loss:.4f}\")\r\n+            print(f\"Learning Rate: {current_lr:.6f}\")\r\n+\r\n+            # Validation phase\r\n+            if val_loader:\r\n+                val_loss = self._validate(val_loader)\r\n+                val_losses.append(val_loss)\r\n+                print(f\"Validation Loss: {val_loss:.4f}\")\r\n+\r\n+                # Update learning rate\r\n+                if self.scheduler is not None:\r\n+                    if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\r\n+                        self.scheduler.step(val_loss)\r\n+                    else:\r\n+                        self.scheduler.step()\r\n+\r\n+        return report\r\n+\r\n+    def _validate(self, val_loader: DataLoader) -> float:\r\n+        \"\"\"Validate the model.\"\"\"\r\n+        self.model.eval()\r\n+        val_loss = 0.0\r\n+\r\n+        with torch.no_grad():\r\n+            for data, target in val_loader:\r\n+                data, target = data.to(self.device), target.to(self.device)\r\n+                output = self.model(data)\r\n+                val_loss += self.criterion(output, target).item()\r\n+\r\n+        return val_loss / len(val_loader)\r\n+\r\n+    def predict(self, dataset: TorchDataset) -> tuple[torch.Tensor, torch.Tensor]:\r\n+        \"\"\"Make predictions using the model.\"\"\"\r\n+        data_loader = DataLoader(dataset, batch_size=self.batch_size)\r\n+\r\n+        self.model.eval()\r\n+        predictions: List[torch.Tensor] = []\r\n+        targets: List[torch.Tensor] = []\r\n+\r\n+        with torch.no_grad():\r\n+            for data, target in data_loader:\r\n+                data = data.to(self.device)\r\n+                predictions.append(self.model(data).cpu())\r\n+                targets.append(target)\r\n+\r\n+        return torch.cat(predictions), torch.cat(targets)\r\n+    def save(self, path: Union[str, Path]) -> None:\r\n+        \"\"\"Save model state.\"\"\"\r\n+        torch.save({ # type: ignore\r\n+            'model_state_dict': self.model.state_dict(),\r\n+            'optimizer_state_dict': self.optimizer.state_dict(),\r\n+            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\r\n+            'config': self.config\r\n+        }, path)\r\n+\r\n+    def load(self, path: Union[str, Union[str, Path]]) -> None:\r\n+        \"\"\"Load model state.\"\"\"\r\n+        checkpoint = torch.load(path) # type: ignore\r\n+        self.model.load_state_dict(checkpoint['model_state_dict'])\r\n+        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n+        if self.scheduler and checkpoint['scheduler_state_dict']:\r\n+            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\r\n"
                },
                {
                    "date": 1733065741839,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,52 +133,56 @@\n             self,\r\n             train_dataset: TorchDataset,\r\n             validation_dataset: Optional[TorchDataset] = None\r\n     ) -> TrainingReport:\r\n-        \"\"\"Train the model.\"\"\"\r\n+        \"\"\"Train the model with improved training loop.\"\"\"\r\n         train_loader = DataLoader(\r\n             train_dataset,\r\n             batch_size=self.batch_size,\r\n-            shuffle=True\r\n+            shuffle=True,\r\n+            num_workers=4,\r\n+            pin_memory=True\r\n         )\r\n \r\n         val_loader = None\r\n         if validation_dataset is not None:\r\n             val_loader = DataLoader(\r\n                 validation_dataset,\r\n-                batch_size=self.batch_size\r\n+                batch_size=self.batch_size,\r\n+                num_workers=4,\r\n+                pin_memory=True\r\n             )\r\n \r\n         train_losses = []\r\n         val_losses = []\r\n         learning_rates = []\r\n+        best_val_loss = float('inf')\r\n+        patience_counter = 0\r\n+        early_stopping_patience = self.config.get('early_stopping_patience', 15)\r\n \r\n-        report = TrainingReport(\r\n-            train_losses=train_losses,\r\n-            val_losses=val_losses,\r\n-            learning_rates=learning_rates,\r\n-            epochs=self.max_epochs\r\n-        )\r\n-\r\n         for epoch in range(self.max_epochs):\r\n             self.model.train()\r\n             train_loss = 0.0\r\n-\r\n-            # Add epoch progress logging\r\n-            print(f\"\\nEpoch [{epoch+1}/{self.max_epochs}]\")\r\n             \r\n             for batch_idx, (data, target) in enumerate(train_loader):\r\n                 try:\r\n                     data, target = data.to(self.device), target.to(self.device)\r\n                     self.optimizer.zero_grad()\r\n+                    \r\n                     output = self.model(data)\r\n                     loss = self.criterion(output, target)\r\n                     loss.backward()\r\n+                    \r\n+                    # Gradient clipping\r\n+                    if self.gradient_clip_val > 0:\r\n+                        torch.nn.utils.clip_grad_norm_(\r\n+                            self.model.parameters(), \r\n+                            self.gradient_clip_val\r\n+                        )\r\n+                    \r\n                     self.optimizer.step()\r\n-\r\n                     train_loss += loss.item()\r\n \r\n-                    # Print batch progress every 50 batches\r\n                     if (batch_idx + 1) % 50 == 0:\r\n                         print(f\"Batch [{batch_idx+1}/{len(train_loader)}] - Loss: {loss.item():.4f}\")\r\n \r\n                 except RuntimeError as e:\r\n@@ -186,282 +190,53 @@\n                     raise e\r\n \r\n             train_loss /= len(train_loader)\r\n             train_losses.append(train_loss)\r\n-\r\n+            \r\n             # Get current learning rate\r\n             current_lr = self.optimizer.param_groups[0]['lr']\r\n             learning_rates.append(current_lr)\r\n \r\n-            # Print epoch summary\r\n-            print(f\"Training Loss: {train_loss:.4f}\")\r\n-            print(f\"Learning Rate: {current_lr:.6f}\")\r\n-\r\n             # Validation phase\r\n             if val_loader:\r\n                 val_loss = self._validate(val_loader)\r\n                 val_losses.append(val_loss)\r\n-                print(f\"Validation Loss: {val_loss:.4f}\")\r\n+                \r\n+                # Early stopping check\r\n+                if val_loss < best_val_loss:\r\n+                    best_val_loss = val_loss\r\n+                    patience_counter = 0\r\n+                    # Save best model\r\n+                    torch.save(self.model.state_dict(), 'best_model.pt')\r\n+                else:\r\n+                    patience_counter += 1\r\n \r\n                 # Update learning rate\r\n                 if self.scheduler is not None:\r\n                     if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\r\n                         self.scheduler.step(val_loss)\r\n                     else:\r\n                         self.scheduler.step()\r\n \r\n-        return report\r\n+                print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\")\r\n \r\n-    def _validate(self, val_loader: DataLoader) -> float:\r\n-        \"\"\"Validate the model.\"\"\"\r\n-        self.model.eval()\r\n-        val_loss = 0.0\r\n+                # Early stopping\r\n+                if patience_counter >= early_stopping_patience:\r\n+                    print(f\"Early stopping triggered after {epoch+1} epochs\")\r\n+                    # Load best model\r\n+                    self.model.load_state_dict(torch.load('best_model.pt'))\r\n+                    break\r\n+            else:\r\n+                print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, LR: {current_lr:.6f}\")\r\n \r\n-        with torch.no_grad():\r\n-            for data, target in val_loader:\r\n-                data, target = data.to(self.device), target.to(self.device)\r\n-                output = self.model(data)\r\n-                val_loss += self.criterion(output, target).item()\r\n-\r\n-        return val_loss / len(val_loader)\r\n-\r\n-    def predict(self, dataset: TorchDataset) -> tuple[torch.Tensor, torch.Tensor]:\r\n-        \"\"\"Make predictions using the model.\"\"\"\r\n-        data_loader = DataLoader(dataset, batch_size=self.batch_size)\r\n-\r\n-        self.model.eval()\r\n-        predictions: List[torch.Tensor] = []\r\n-        targets: List[torch.Tensor] = []\r\n-\r\n-        with torch.no_grad():\r\n-            for data, target in data_loader:\r\n-                data = data.to(self.device)\r\n-                predictions.append(self.model(data).cpu())\r\n-                targets.append(target)\r\n-\r\n-        return torch.cat(predictions), torch.cat(targets)\r\n-    def save(self, path: Union[str, Path]) -> None:\r\n-        \"\"\"Save model state.\"\"\"\r\n-        torch.save({ # type: ignore\r\n-            'model_state_dict': self.model.state_dict(),\r\n-            'optimizer_state_dict': self.optimizer.state_dict(),\r\n-            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\r\n-            'config': self.config\r\n-        }, path)\r\n-\r\n-    def load(self, path: Union[str, Union[str, Path]]) -> None:\r\n-        \"\"\"Load model state.\"\"\"\r\n-        checkpoint = torch.load(path) # type: ignore\r\n-        self.model.load_state_dict(checkpoint['model_state_dict'])\r\n-        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n-        if self.scheduler and checkpoint['scheduler_state_dict']:\r\n-            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\r\n-# models/wrappers/pytorch_wrapper.py\r\n-from __future__ import annotations\r\n-from pathlib import Path\r\n-from typing import Union\r\n-from typing import Union\r\n-from typing import Any, Dict, List, Optional, Tuple, Union, Type\r\n-import torch\r\n-from torch import nn\r\n-from os import PathLike\r\n-from torch.optim.adam import Adam\r\n-from torch.optim.adamw import AdamW\r\n-from torch.optim.sgd import SGD\r\n-from torch.optim import lr_scheduler\r\n-import torch.optim as optim\r\n-from torch.optim.optimizer import Optimizer\r\n-from torch.utils.data import Dataset as TorchDataset  # Rename to avoid conflict\r\n-from torch.utils.data import DataLoader\r\n-\r\n-from models.losses.custom_losses import MAPE\r\n-from training.base.base_trainer import TrainingEpoch\r\n-from training.reports.training_report import TrainingReport\r\n-\r\n-from ..interfaces import WrapperInterface\r\n-from ..base.base_model import BaseModel\r\n-from ..registry.model_types import ModelType\r\n-\r\n-\r\n-class PyTorchWrapper(WrapperInterface):\r\n-    \"\"\"Wrapper for PyTorch models providing consistent training and inference interface.\"\"\"\r\n-\r\n-    def __init__(self, model: BaseModel, model_type: ModelType, config: Dict[str, Any]):\r\n-        self.model = model\r\n-        self.model_type = model_type\r\n-        self.config = config\r\n-        self.device = config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\r\n-        self.model.to(self.device)\r\n-\r\n-        # Training configuration\r\n-        self.batch_size = config.get('batch_size', 32)\r\n-        self.learning_rate = config.get('learning_rate', 1e-3)\r\n-        self.max_epochs = config.get('max_epochs', 100)\r\n-        self.gradient_clip_val = config.get('gradient_clipping', 1.0)\r\n-\r\n-        # Setup optimizer and scheduler\r\n-        self.optimizer = self._setup_optimizer()\r\n-        self.scheduler = self._setup_scheduler()\r\n-        self.criterion = self._setup_criterion()\r\n-\r\n-    def _setup_optimizer(self) -> Optimizer:\r\n-        \"\"\"Initialize optimizer with improved defaults.\"\"\"\r\n-        optimizer_name = self.config.get('optimizer', 'adamw').lower()  # Default changed to AdamW\r\n-        optimizer_config = self.config.get('optimizer_config', {\r\n-            'weight_decay': 0.01,  # Add L2 regularization\r\n-            'betas': (0.9, 0.999),\r\n-            'eps': 1e-8\r\n-        })\r\n-\r\n-        optimizers: Dict[str, Optimizer] = {\r\n-            'adam': Adam(\r\n-                self.model.parameters(),\r\n-                lr=self.learning_rate,\r\n-                **optimizer_config\r\n-            ),\r\n-            'adamw': AdamW(\r\n-                self.model.parameters(),\r\n-                lr=self.learning_rate,\r\n-                **optimizer_config\r\n-            ),\r\n-            'sgd': SGD(\r\n-                self.model.parameters(),\r\n-                lr=self.learning_rate,\r\n-                momentum=0.9,\r\n-                **optimizer_config\r\n-            )\r\n-        }\r\n-\r\n-        if optimizer_name not in optimizers:\r\n-            raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\r\n-\r\n-        return optimizers[optimizer_name]\r\n-\r\n-    def _setup_scheduler(self) -> Optional[Union[lr_scheduler.StepLR, lr_scheduler.CosineAnnealingLR, lr_scheduler.ReduceLROnPlateau]]:\r\n-        scheduler_name = self.config.get('scheduler', None)\r\n-        if not scheduler_name:\r\n-            return None\r\n-\r\n-        scheduler_config = self.config.get('scheduler_config', {})\r\n-\r\n-        schedulers: Dict[str, Union[lr_scheduler.StepLR, lr_scheduler.CosineAnnealingLR, lr_scheduler.ReduceLROnPlateau]] = {\r\n-            'step': optim.lr_scheduler.StepLR(\r\n-                self.optimizer,\r\n-                **scheduler_config\r\n-            ),\r\n-            'cosine': optim.lr_scheduler.CosineAnnealingLR(\r\n-                self.optimizer,\r\n-                **scheduler_config\r\n-            ),\r\n-            'plateau': optim.lr_scheduler.ReduceLROnPlateau(\r\n-                self.optimizer,\r\n-                **scheduler_config\r\n-            )\r\n-        }\r\n-\r\n-        if scheduler_name not in schedulers:\r\n-            raise ValueError(f\"Unknown scheduler: {scheduler_name}\")\r\n-\r\n-        return schedulers[scheduler_name]\r\n-\r\n-    def _setup_criterion(self) -> nn.Module:\r\n-        \"\"\"Initialize loss function based on config.\"\"\"\r\n-        criterion_name = self.config.get('criterion', 'mse').lower()\r\n-        criterion_config = self.config.get('criterion_config', {})\r\n-\r\n-        criteria: Dict[str, nn.Module] = {\r\n-            'mse': nn.MSELoss(**criterion_config),\r\n-            'mae': nn.L1Loss(**criterion_config),\r\n-            'mape': MAPE(**criterion_config)\r\n-        }\r\n-\r\n-        if criterion_name not in criteria:\r\n-            raise ValueError(f\"Unknown criterion: {criterion_name}\")\r\n-\r\n-        return criteria[criterion_name]\r\n-\r\n-    def train(\r\n-            self,\r\n-            train_dataset: TorchDataset,\r\n-            validation_dataset: Optional[TorchDataset] = None\r\n-    ) -> TrainingReport:\r\n-        \"\"\"Train the model.\"\"\"\r\n-        train_loader = DataLoader(\r\n-            train_dataset,\r\n-            batch_size=self.batch_size,\r\n-            shuffle=True\r\n-        )\r\n-\r\n-        val_loader = None\r\n-        if validation_dataset is not None:\r\n-            val_loader = DataLoader(\r\n-                validation_dataset,\r\n-                batch_size=self.batch_size\r\n-            )\r\n-\r\n-        train_losses = []\r\n-        val_losses = []\r\n-        learning_rates = []\r\n-\r\n-        report = TrainingReport(\r\n+        return TrainingReport(\r\n             train_losses=train_losses,\r\n             val_losses=val_losses,\r\n             learning_rates=learning_rates,\r\n-            epochs=self.max_epochs\r\n+            epochs=epoch + 1\r\n         )\r\n \r\n-        for epoch in range(self.max_epochs):\r\n-            self.model.train()\r\n-            train_loss = 0.0\r\n \r\n-            # Add epoch progress logging\r\n-            print(f\"\\nEpoch [{epoch+1}/{self.max_epochs}]\")\r\n-            \r\n-            for batch_idx, (data, target) in enumerate(train_loader):\r\n-                try:\r\n-                    data, target = data.to(self.device), target.to(self.device)\r\n-                    self.optimizer.zero_grad()\r\n-                    output = self.model(data)\r\n-                    loss = self.criterion(output, target)\r\n-                    loss.backward()\r\n-                    self.optimizer.step()\r\n-\r\n-                    train_loss += loss.item()\r\n-\r\n-                    # Print batch progress every 50 batches\r\n-                    if (batch_idx + 1) % 50 == 0:\r\n-                        print(f\"Batch [{batch_idx+1}/{len(train_loader)}] - Loss: {loss.item():.4f}\")\r\n-\r\n-                except RuntimeError as e:\r\n-                    print(f\"Error in batch {batch_idx}: {str(e)}\")\r\n-                    raise e\r\n-\r\n-            train_loss /= len(train_loader)\r\n-            train_losses.append(train_loss)\r\n-\r\n-            # Get current learning rate\r\n-            current_lr = self.optimizer.param_groups[0]['lr']\r\n-            learning_rates.append(current_lr)\r\n-\r\n-            # Print epoch summary\r\n-            print(f\"Training Loss: {train_loss:.4f}\")\r\n-            print(f\"Learning Rate: {current_lr:.6f}\")\r\n-\r\n-            # Validation phase\r\n-            if val_loader:\r\n-                val_loss = self._validate(val_loader)\r\n-                val_losses.append(val_loss)\r\n-                print(f\"Validation Loss: {val_loss:.4f}\")\r\n-\r\n-                # Update learning rate\r\n-                if self.scheduler is not None:\r\n-                    if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\r\n-                        self.scheduler.step(val_loss)\r\n-                    else:\r\n-                        self.scheduler.step()\r\n-\r\n-        return report\r\n-\r\n     def _validate(self, val_loader: DataLoader) -> float:\r\n         \"\"\"Validate the model.\"\"\"\r\n         self.model.eval()\r\n         val_loss = 0.0\r\n"
                },
                {
                    "date": 1733065756531,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,14 @@\n         return val_loss / len(val_loader)\r\n \r\n     def predict(self, dataset: TorchDataset) -> tuple[torch.Tensor, torch.Tensor]:\r\n         \"\"\"Make predictions using the model.\"\"\"\r\n-        data_loader = DataLoader(dataset, batch_size=self.batch_size)\r\n+        data_loader = DataLoader(\r\n+            dataset, \r\n+            batch_size=self.batch_size,\r\n+            num_workers=4,\r\n+            pin_memory=True\r\n+        )\r\n \r\n         self.model.eval()\r\n         predictions: List[torch.Tensor] = []\r\n         targets: List[torch.Tensor] = []\r\n@@ -263,8 +268,9 @@\n                 predictions.append(self.model(data).cpu())\r\n                 targets.append(target)\r\n \r\n         return torch.cat(predictions), torch.cat(targets)\r\n+\r\n     def save(self, path: Union[str, Path]) -> None:\r\n         \"\"\"Save model state.\"\"\"\r\n         torch.save({ # type: ignore\r\n             'model_state_dict': self.model.state_dict(),\r\n"
                },
                {
                    "date": 1733067828078,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,38 +47,45 @@\n         self.criterion = self._setup_criterion()\r\n \r\n     def _setup_optimizer(self) -> Optimizer:\r\n         \"\"\"Initialize optimizer with improved defaults.\"\"\"\r\n-        optimizer_name = self.config.get('optimizer', 'adamw').lower()  # Default changed to AdamW\r\n-        optimizer_config = self.config.get('optimizer_config', {\r\n-            'weight_decay': 0.01,  # Add L2 regularization\r\n-            'betas': (0.9, 0.999),\r\n-            'eps': 1e-8\r\n-        })\r\n-\r\n-        optimizers: Dict[str, Optimizer] = {\r\n-            'adam': Adam(\r\n-                self.model.parameters(),\r\n-                lr=self.learning_rate,\r\n-                **optimizer_config\r\n-            ),\r\n-            'adamw': AdamW(\r\n-                self.model.parameters(),\r\n-                lr=self.learning_rate,\r\n-                **optimizer_config\r\n-            ),\r\n-            'sgd': SGD(\r\n-                self.model.parameters(),\r\n-                lr=self.learning_rate,\r\n-                momentum=0.9,\r\n-                **optimizer_config\r\n-            )\r\n+        optimizer_name = self.config.get('optimizer', 'adamw').lower()\r\n+        optimizer_config = self.config.get('optimizer_config', {})\r\n+        \r\n+        # Base parameters all optimizers support\r\n+        base_params = {\r\n+            'lr': self.learning_rate,\r\n+            'weight_decay': optimizer_config.get('weight_decay', 0.01)\r\n         }\r\n \r\n-        if optimizer_name not in optimizers:\r\n+        # Create optimizer with appropriate parameters\r\n+        if optimizer_name == 'sgd':\r\n+            sgd_params = {\r\n+                'momentum': optimizer_config.get('momentum', 0.9),\r\n+                'dampening': optimizer_config.get('dampening', 0),\r\n+                'nesterov': optimizer_config.get('nesterov', False)\r\n+            }\r\n+            return SGD(self.model.parameters(), **base_params, **sgd_params)\r\n+            \r\n+        elif optimizer_name == 'adam':\r\n+            adam_params = {\r\n+                'betas': optimizer_config.get('betas', (0.9, 0.999)),\r\n+                'eps': optimizer_config.get('eps', 1e-8),\r\n+                'amsgrad': optimizer_config.get('amsgrad', False)\r\n+            }\r\n+            return Adam(self.model.parameters(), **base_params, **adam_params)\r\n+            \r\n+        elif optimizer_name == 'adamw':\r\n+            adamw_params = {\r\n+                'betas': optimizer_config.get('betas', (0.9, 0.999)),\r\n+                'eps': optimizer_config.get('eps', 1e-8),\r\n+                'amsgrad': optimizer_config.get('amsgrad', False)\r\n+            }\r\n+            return AdamW(self.model.parameters(), **base_params, **adamw_params)\r\n+            \r\n+        else:\r\n             raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\r\n \r\n-        return optimizers[optimizer_name]\r\n \r\n     def _setup_scheduler(self) -> Optional[Union[lr_scheduler.StepLR, lr_scheduler.CosineAnnealingLR, lr_scheduler.ReduceLROnPlateau]]:\r\n         \"\"\"Initialize learning rate scheduler with improved defaults.\"\"\"\r\n         scheduler_name = self.config.get('scheduler', 'cosine')  # Default changed to cosine\r\n"
                }
            ],
            "date": 1733003057468,
            "name": "Commit-0",
            "content": "# models/wrappers/pytorch_wrapper.py\r\nfrom __future__ import annotations\r\nfrom pathlib import Path\r\nfrom typing import Union\r\nfrom typing import Union\r\nfrom typing import Any, Dict, List, Optional, Tuple, Union, Type\r\nimport torch\r\nfrom torch import nn\r\nfrom os import PathLike\r\nfrom torch.optim.adam import Adam\r\nfrom torch.optim.adamw import AdamW\r\nfrom torch.optim.sgd import SGD\r\nfrom torch.optim import lr_scheduler\r\nimport torch.optim as optim\r\nfrom torch.optim.optimizer import Optimizer\r\nfrom torch.utils.data import Dataset as TorchDataset  # Rename to avoid conflict\r\nfrom torch.utils.data import DataLoader\r\n\r\nfrom models.losses.custom_losses import MAPE\r\nfrom training.base.base_trainer import TrainingEpoch\r\nfrom training.reports.training_report import TrainingReport\r\n\r\nfrom ..interfaces import WrapperInterface\r\nfrom ..base.base_model import BaseModel\r\nfrom ..registry.model_types import ModelType\r\n\r\n\r\nclass PyTorchWrapper(WrapperInterface):\r\n    \"\"\"Wrapper for PyTorch models providing consistent training and inference interface.\"\"\"\r\n\r\n    def __init__(self, model: BaseModel, model_type: ModelType, config: Dict[str, Any]):\r\n        self.model = model\r\n        self.model_type = model_type\r\n        self.config = config\r\n        self.device = config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\r\n        self.model.to(self.device)\r\n\r\n        # Training configuration\r\n        self.batch_size = config.get('batch_size', 32)\r\n        self.learning_rate = config.get('learning_rate', 1e-3)\r\n        self.max_epochs = config.get('max_epochs', 100)\r\n\r\n        # Setup optimizer and scheduler\r\n        self.optimizer = self._setup_optimizer()\r\n        self.scheduler = self._setup_scheduler()\r\n        self.criterion = self._setup_criterion()\r\n\r\n    def _setup_optimizer(self) -> Optimizer:\r\n        \"\"\"Initialize optimizer based on config.\"\"\"\r\n        optimizer_name = self.config.get('optimizer', 'adam').lower()\r\n        optimizer_config = self.config.get('optimizer_config', {})\r\n\r\n        optimizers: Dict[str, Optimizer] = {\r\n            'adam': Adam(\r\n                self.model.parameters(),\r\n                lr=self.learning_rate,\r\n                **optimizer_config\r\n            ),\r\n            'adamw': AdamW(\r\n                self.model.parameters(),\r\n                lr=self.learning_rate,\r\n                **optimizer_config\r\n            ),\r\n            'sgd': SGD(\r\n                self.model.parameters(),\r\n                lr=self.learning_rate,\r\n                **optimizer_config\r\n            )\r\n        }\r\n\r\n        if optimizer_name not in optimizers:\r\n            raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\r\n\r\n        return optimizers[optimizer_name]\r\n\r\n    def _setup_scheduler(self) -> Optional[Union[lr_scheduler.StepLR, lr_scheduler.CosineAnnealingLR, lr_scheduler.ReduceLROnPlateau]]:\r\n        scheduler_name = self.config.get('scheduler', None)\r\n        if not scheduler_name:\r\n            return None\r\n\r\n        scheduler_config = self.config.get('scheduler_config', {})\r\n\r\n        schedulers: Dict[str, Union[lr_scheduler.StepLR, lr_scheduler.CosineAnnealingLR, lr_scheduler.ReduceLROnPlateau]] = {\r\n            'step': optim.lr_scheduler.StepLR(\r\n                self.optimizer,\r\n                **scheduler_config\r\n            ),\r\n            'cosine': optim.lr_scheduler.CosineAnnealingLR(\r\n                self.optimizer,\r\n                **scheduler_config\r\n            ),\r\n            'plateau': optim.lr_scheduler.ReduceLROnPlateau(\r\n                self.optimizer,\r\n                **scheduler_config\r\n            )\r\n        }\r\n\r\n        if scheduler_name not in schedulers:\r\n            raise ValueError(f\"Unknown scheduler: {scheduler_name}\")\r\n\r\n        return schedulers[scheduler_name]\r\n\r\n    def _setup_criterion(self) -> nn.Module:\r\n        \"\"\"Initialize loss function based on config.\"\"\"\r\n        criterion_name = self.config.get('criterion', 'mse').lower()\r\n        criterion_config = self.config.get('criterion_config', {})\r\n\r\n        criteria: Dict[str, nn.Module] = {\r\n            'mse': nn.MSELoss(**criterion_config),\r\n            'mae': nn.L1Loss(**criterion_config),\r\n            'mape': MAPE(**criterion_config)\r\n        }\r\n\r\n        if criterion_name not in criteria:\r\n            raise ValueError(f\"Unknown criterion: {criterion_name}\")\r\n\r\n        return criteria[criterion_name]\r\n\r\n    def train(\r\n            self,\r\n            train_dataset: TorchDataset,\r\n            validation_dataset: Optional[TorchDataset] = None\r\n    ) -> TrainingReport:\r\n        \"\"\"Train the model.\"\"\"\r\n        train_loader = DataLoader(\r\n            train_dataset,\r\n            batch_size=self.batch_size,\r\n            shuffle=True\r\n        )\r\n\r\n        val_loader = None\r\n        if validation_dataset is not None:\r\n            val_loader = DataLoader(\r\n                validation_dataset,\r\n                batch_size=self.batch_size\r\n            )\r\n\r\n        train_losses = []\r\n        val_losses = []\r\n        learning_rates = []\r\n\r\n        report = TrainingReport(\r\n            train_losses=train_losses,\r\n            val_losses=val_losses,\r\n            learning_rates=learning_rates,\r\n            epochs=self.max_epochs\r\n        )\r\n\r\n        for epoch in range(self.max_epochs):\r\n            self.model.train()\r\n            train_loss = 0.0\r\n\r\n            # Add epoch progress logging\r\n            print(f\"\\nEpoch [{epoch+1}/{self.max_epochs}]\")\r\n            \r\n            for batch_idx, (data, target) in enumerate(train_loader):\r\n                try:\r\n                    data, target = data.to(self.device), target.to(self.device)\r\n                    self.optimizer.zero_grad()\r\n                    output = self.model(data)\r\n                    loss = self.criterion(output, target)\r\n                    loss.backward()\r\n                    self.optimizer.step()\r\n\r\n                    train_loss += loss.item()\r\n\r\n                    # Print batch progress every 50 batches\r\n                    if (batch_idx + 1) % 50 == 0:\r\n                        print(f\"Batch [{batch_idx+1}/{len(train_loader)}] - Loss: {loss.item():.4f}\")\r\n\r\n                except RuntimeError as e:\r\n                    print(f\"Error in batch {batch_idx}: {str(e)}\")\r\n                    raise e\r\n\r\n            train_loss /= len(train_loader)\r\n            train_losses.append(train_loss)\r\n\r\n            # Get current learning rate\r\n            current_lr = self.optimizer.param_groups[0]['lr']\r\n            learning_rates.append(current_lr)\r\n\r\n            # Print epoch summary\r\n            print(f\"Training Loss: {train_loss:.4f}\")\r\n            print(f\"Learning Rate: {current_lr:.6f}\")\r\n\r\n            # Validation phase\r\n            if val_loader:\r\n                val_loss = self._validate(val_loader)\r\n                val_losses.append(val_loss)\r\n                print(f\"Validation Loss: {val_loss:.4f}\")\r\n\r\n                # Update learning rate\r\n                if self.scheduler is not None:\r\n                    if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):\r\n                        self.scheduler.step(val_loss)\r\n                    else:\r\n                        self.scheduler.step()\r\n\r\n        return report\r\n\r\n    def _validate(self, val_loader: DataLoader) -> float:\r\n        \"\"\"Validate the model.\"\"\"\r\n        self.model.eval()\r\n        val_loss = 0.0\r\n\r\n        with torch.no_grad():\r\n            for data, target in val_loader:\r\n                data, target = data.to(self.device), target.to(self.device)\r\n                output = self.model(data)\r\n                val_loss += self.criterion(output, target).item()\r\n\r\n        return val_loss / len(val_loader)\r\n\r\n    def predict(self, dataset: TorchDataset) -> tuple[torch.Tensor, torch.Tensor]:\r\n        \"\"\"Make predictions using the model.\"\"\"\r\n        data_loader = DataLoader(dataset, batch_size=self.batch_size)\r\n\r\n        self.model.eval()\r\n        predictions: List[torch.Tensor] = []\r\n        targets: List[torch.Tensor] = []\r\n\r\n        with torch.no_grad():\r\n            for data, target in data_loader:\r\n                data = data.to(self.device)\r\n                predictions.append(self.model(data).cpu())\r\n                targets.append(target)\r\n\r\n        return torch.cat(predictions), torch.cat(targets)\r\n    def save(self, path: Union[str, Path]) -> None:\r\n        \"\"\"Save model state.\"\"\"\r\n        torch.save({ # type: ignore\r\n            'model_state_dict': self.model.state_dict(),\r\n            'optimizer_state_dict': self.optimizer.state_dict(),\r\n            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\r\n            'config': self.config\r\n        }, path)\r\n\r\n    def load(self, path: Union[str, Union[str, Path]]) -> None:\r\n        \"\"\"Load model state.\"\"\"\r\n        checkpoint = torch.load(path) # type: ignore\r\n        self.model.load_state_dict(checkpoint['model_state_dict'])\r\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n        if self.scheduler and checkpoint['scheduler_state_dict']:\r\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\r\n"
        }
    ]
}