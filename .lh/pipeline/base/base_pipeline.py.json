{
    "sourceFile": "pipeline/base/base_pipeline.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1733003036851,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733003049227,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,123 @@\n+# pipeline/base/base_pipeline.py\r\n+from abc import ABC, abstractmethod\r\n+from typing import Optional, Tuple\r\n+from pathlib import Path\r\n+\r\n+from torch.utils.data import Dataset\r\n+\r\n+from data_loading.loaders.time_series_loader import TimeInterval, TimeSeriesLoader\r\n+\r\n+from ..config.pipeline_config import PipelineConfig\r\n+from experiments.experiment import Experiment\r\n+\r\n+class BasePipeline(ABC):\r\n+    \"\"\"Base class for all pipeline implementations.\"\"\"\r\n+\r\n+    def __init__(self, config: PipelineConfig):\r\n+        \"\"\"\r\n+        Args:\r\n+            config: Configuration for dataset, model, and training\r\n+        \"\"\"\r\n+        self.config = config\r\n+        self.data_loader: Optional[TimeSeriesLoader] = None\r\n+        self.experiment: Optional[Experiment] = None\r\n+        self.training_time: float = 0.0\r\n+        self.test_time: float = 0.0\r\n+\r\n+    @abstractmethod\r\n+    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n+        \"\"\"Initialize and prepare the data loader.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def prepare_datasets(self) -> Tuple[Dataset, Dataset, Dataset]:\r\n+        \"\"\"\r\n+        Prepare train, validation and test datasets.\r\n+\r\n+        Returns:\r\n+            Tuple of (train_dataset, val_dataset, test_dataset)\r\n+        \"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def setup_model(self) -> None:\r\n+        \"\"\"Setup model and its wrapper.\"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def train_model(self, train_dataset: Dataset, val_dataset: Dataset) -> None:\r\n+        \"\"\"\r\n+        Train the model.\r\n+\r\n+        Args:\r\n+            train_dataset: Training dataset\r\n+            val_dataset: Validation dataset\r\n+        \"\"\"\r\n+        pass\r\n+\r\n+    @abstractmethod\r\n+    def evaluate_model(self, test_dataset: Dataset) -> None:\r\n+        \"\"\"\r\n+        Evaluate model performance.\r\n+\r\n+        Args:\r\n+            test_dataset: Test dataset\r\n+        \"\"\"\r\n+        pass\r\n+\r\n+    def run(self) -> Optional[Experiment]:\r\n+        \"\"\"\r\n+        Execute the complete pipeline.\r\n+\r\n+        Returns:\r\n+            Experiment object containing results and metrics\r\n+        \"\"\"\r\n+        try:\r\n+            # Initialize data loader\r\n+            self.data_loader = self.prepare_data_loader()\r\n+\r\n+            # Prepare datasets\r\n+            train_dataset, val_dataset, test_dataset = self.prepare_datasets()\r\n+\r\n+            # Setup and train model\r\n+            self.setup_model()\r\n+            self.train_model(train_dataset, val_dataset)\r\n+\r\n+            # Evaluate model\r\n+            self.evaluate_model(test_dataset)\r\n+\r\n+            # Save experiment if path provided\r\n+            if self.experiment and self.config.experiment_save_path:\r\n+                self.experiment.save_to_json_file()\r\n+\r\n+            return self.experiment\r\n+\r\n+        except Exception as e:\r\n+            print(f\"Pipeline execution failed: {str(e)}\")\r\n+            raise\r\n+\r\n+    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create time intervals for data splitting.\"\"\"\r\n+        return (\r\n+            TimeInterval(\r\n+                min_date=self.config.train_dates[0],\r\n+                max_date=self.config.train_dates[1]\r\n+            ),\r\n+            TimeInterval(\r\n+                min_date=self.config.val_dates[0],\r\n+                max_date=self.config.val_dates[1]\r\n+            ),\r\n+            TimeInterval(\r\n+                min_date=self.config.test_dates[0],\r\n+                max_date=self.config.test_dates[1]\r\n+            )\r\n+        )\r\n+\r\n+    def _save_model(self, save_path: Path) -> None:\r\n+        \"\"\"\r\n+        Save model artifacts.\r\n+\r\n+        Args:\r\n+            save_path: Path to save model files\r\n+        \"\"\"\r\n+        pass\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733067602810,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,16 +1,25 @@\n # pipeline/base/base_pipeline.py\r\n from abc import ABC, abstractmethod\r\n+from dataclasses import asdict\r\n from typing import Optional, Tuple\r\n from pathlib import Path\r\n \r\n from torch.utils.data import Dataset\r\n+from torch.optim.optimizer import Optimizer\r\n+from torch.optim.adam import Adam\r\n+from torch.optim.sgd import SGD\r\n+from torch.optim.adamw import AdamW\r\n \r\n+\r\n+\r\n+\r\n from data_loading.loaders.time_series_loader import TimeInterval, TimeSeriesLoader\r\n \r\n from ..config.pipeline_config import PipelineConfig\r\n from experiments.experiment import Experiment\r\n \r\n+\r\n class BasePipeline(ABC):\r\n     \"\"\"Base class for all pipeline implementations.\"\"\"\r\n \r\n     def __init__(self, config: PipelineConfig):\r\n@@ -22,9 +31,12 @@\n         self.data_loader: Optional[TimeSeriesLoader] = None\r\n         self.experiment: Optional[Experiment] = None\r\n         self.training_time: float = 0.0\r\n         self.test_time: float = 0.0\r\n+        \r\n+        self.config_dict = asdict(self.config)\r\n \r\n+\r\n     @abstractmethod\r\n     def prepare_data_loader(self) -> TimeSeriesLoader:\r\n         \"\"\"Initialize and prepare the data loader.\"\"\"\r\n         pass\r\n@@ -119,5 +131,38 @@\n \r\n         Args:\r\n             save_path: Path to save model files\r\n         \"\"\"\r\n-        pass\n\\ No newline at end of file\n+        pass\r\n+\r\n+    def _setup_optimizer(self) -> Optimizer:\r\n+        \"\"\"Initialize optimizer with improved defaults.\"\"\"\r\n+        optimizer_name = self.config.get('optimizer', 'adamw').lower()\r\n+        optimizer_config = self.config.get('optimizer_config', {})\r\n+        \r\n+        # Filter optimizer params based on optimizer type\r\n+        if optimizer_name == 'sgd':\r\n+            # Remove Adam-specific parameters for SGD\r\n+            filtered_config = {\r\n+                k: v for k, v in optimizer_config.items() \r\n+                if k in ['weight_decay', 'momentum', 'dampening', 'nesterov']\r\n+            }\r\n+            return SGD(\r\n+                self.model.parameters(),\r\n+                lr=self.learning_rate,\r\n+                **filtered_config\r\n+            )\r\n+        elif optimizer_name in ['adam', 'adamw']:\r\n+            # Keep all parameters for Adam/AdamW\r\n+            optimizer_cls = Adam if optimizer_name == 'adam' else AdamW\r\n+            return optimizer_cls(\r\n+                self.model.parameters(),\r\n+                lr=self.learning_rate,\r\n+                **optimizer_config\r\n+            )\r\n+        else:\r\n+            raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\r\n+        \r\n+    \r\n+    def get_config_value(self, key: str, default: Any = None) -> Any:\r\n+        \"\"\"Helper method to safely get configuration values.\"\"\"\r\n+        return self.config_dict.get(key, default)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733067735660,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,16 +1,25 @@\n # pipeline/base/base_pipeline.py\r\n from abc import ABC, abstractmethod\r\n-from typing import Optional, Tuple\r\n+from dataclasses import asdict\r\n+from typing import Any, Optional, Tuple\r\n from pathlib import Path\r\n \r\n from torch.utils.data import Dataset\r\n+from torch.optim.optimizer import Optimizer\r\n+from torch.optim.adam import Adam\r\n+from torch.optim.sgd import SGD\r\n+from torch.optim.adamw import AdamW\r\n \r\n+\r\n+\r\n+\r\n from data_loading.loaders.time_series_loader import TimeInterval, TimeSeriesLoader\r\n \r\n from ..config.pipeline_config import PipelineConfig\r\n from experiments.experiment import Experiment\r\n \r\n+\r\n class BasePipeline(ABC):\r\n     \"\"\"Base class for all pipeline implementations.\"\"\"\r\n \r\n     def __init__(self, config: PipelineConfig):\r\n@@ -22,9 +31,12 @@\n         self.data_loader: Optional[TimeSeriesLoader] = None\r\n         self.experiment: Optional[Experiment] = None\r\n         self.training_time: float = 0.0\r\n         self.test_time: float = 0.0\r\n+        \r\n+        self.config_dict = asdict(self.config)\r\n \r\n+\r\n     @abstractmethod\r\n     def prepare_data_loader(self) -> TimeSeriesLoader:\r\n         \"\"\"Initialize and prepare the data loader.\"\"\"\r\n         pass\r\n@@ -119,5 +131,10 @@\n \r\n         Args:\r\n             save_path: Path to save model files\r\n         \"\"\"\r\n-        pass\n\\ No newline at end of file\n+        pass\r\n+        \r\n+    \r\n+    def get_config_value(self, key: str, default: Any = None) -> Any:\r\n+        \"\"\"Helper method to safely get configuration values.\"\"\"\r\n+        return self.config_dict.get(key, default)\r\n"
                }
            ],
            "date": 1733003036851,
            "name": "Commit-0",
            "content": "# pipeline/base/base_pipeline.py\r\nfrom abc import ABC, abstractmethod\r\nfrom typing import Optional, Tuple\r\nfrom pathlib import Path\r\n\r\nfrom torch.utils.data import Dataset\r\n\r\nfrom data_loading.loaders.time_series_loader import TimeInterval, TimeSeriesLoader\r\n\r\nfrom ..config.pipeline_config import PipelineConfig\r\nfrom experiments.experiment import Experiment\r\n\r\nclass BasePipeline(ABC):\r\n    \"\"\"Base class for all pipeline implementations.\"\"\"\r\n\r\n    def __init__(self, config: PipelineConfig):\r\n        \"\"\"\r\n        Args:\r\n            config: Configuration for dataset, model, and training\r\n        \"\"\"\r\n        self.config = config\r\n        self.data_loader: Optional[TimeSeriesLoader] = None\r\n        self.experiment: Optional[Experiment] = None\r\n        self.training_time: float = 0.0\r\n        self.test_time: float = 0.0\r\n\r\n    @abstractmethod\r\n    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n        \"\"\"Initialize and prepare the data loader.\"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def prepare_datasets(self) -> Tuple[Dataset, Dataset, Dataset]:\r\n        \"\"\"\r\n        Prepare train, validation and test datasets.\r\n\r\n        Returns:\r\n            Tuple of (train_dataset, val_dataset, test_dataset)\r\n        \"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def setup_model(self) -> None:\r\n        \"\"\"Setup model and its wrapper.\"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def train_model(self, train_dataset: Dataset, val_dataset: Dataset) -> None:\r\n        \"\"\"\r\n        Train the model.\r\n\r\n        Args:\r\n            train_dataset: Training dataset\r\n            val_dataset: Validation dataset\r\n        \"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def evaluate_model(self, test_dataset: Dataset) -> None:\r\n        \"\"\"\r\n        Evaluate model performance.\r\n\r\n        Args:\r\n            test_dataset: Test dataset\r\n        \"\"\"\r\n        pass\r\n\r\n    def run(self) -> Optional[Experiment]:\r\n        \"\"\"\r\n        Execute the complete pipeline.\r\n\r\n        Returns:\r\n            Experiment object containing results and metrics\r\n        \"\"\"\r\n        try:\r\n            # Initialize data loader\r\n            self.data_loader = self.prepare_data_loader()\r\n\r\n            # Prepare datasets\r\n            train_dataset, val_dataset, test_dataset = self.prepare_datasets()\r\n\r\n            # Setup and train model\r\n            self.setup_model()\r\n            self.train_model(train_dataset, val_dataset)\r\n\r\n            # Evaluate model\r\n            self.evaluate_model(test_dataset)\r\n\r\n            # Save experiment if path provided\r\n            if self.experiment and self.config.experiment_save_path:\r\n                self.experiment.save_to_json_file()\r\n\r\n            return self.experiment\r\n\r\n        except Exception as e:\r\n            print(f\"Pipeline execution failed: {str(e)}\")\r\n            raise\r\n\r\n    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n        \"\"\"Create time intervals for data splitting.\"\"\"\r\n        return (\r\n            TimeInterval(\r\n                min_date=self.config.train_dates[0],\r\n                max_date=self.config.train_dates[1]\r\n            ),\r\n            TimeInterval(\r\n                min_date=self.config.val_dates[0],\r\n                max_date=self.config.val_dates[1]\r\n            ),\r\n            TimeInterval(\r\n                min_date=self.config.test_dates[0],\r\n                max_date=self.config.test_dates[1]\r\n            )\r\n        )\r\n\r\n    def _save_model(self, save_path: Path) -> None:\r\n        \"\"\"\r\n        Save model artifacts.\r\n\r\n        Args:\r\n            save_path: Path to save model files\r\n        \"\"\"\r\n        pass"
        }
    ]
}