{
    "sourceFile": "pipeline/implementations/time_series_pipeline.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1733263378476,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733265313060,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,571 @@\n+# pipeline/implementations/time_series_pipeline.py\r\n+from __future__ import annotations\r\n+import time\r\n+from typing import Tuple, Optional, Dict, Any, cast, Union\r\n+from pathlib import Path\r\n+import torch\r\n+import dataclasses\r\n+from argparse import Namespace\r\n+import numpy as np\r\n+from numpy.typing import NDArray\r\n+import pandas as pd  # Add this import statement\r\n+from datetime import date  # Add this import statement\r\n+\r\n+from data_loading.datasets.standard_dataset import StandardDataset\r\n+from data_loading.datasets.transformer_dataset import TransformerDataset\r\n+from data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\n+from evaluation.evaluator import Evaluator\r\n+from evaluation.metrics import MetricConfig\r\n+from experiments.experiment import Experiment\r\n+from models.base.base_wrapper import BaseWrapper\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.wrappers.pytorch_wrapper import PyTorchWrapper\r\n+from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n+from pipeline.base.base_pipeline import BasePipeline\r\n+from training.reports.training_report import TrainingReport\r\n+from torch.utils.data import DataLoader\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n+\r\n+class TimeSeriesPipeline(BasePipeline):\r\n+    \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n+\r\n+    def __init__(self, *args, **kwargs):\r\n+        super().__init__(*args, **kwargs)\r\n+        self.data_loader: Optional[TimeSeriesLoader] = None\r\n+        self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n+        \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"TimeSeriesPipeline\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        \r\n+        # Validate resolutions during initialization\r\n+        self._validate_resolutions()\r\n+        self._progress_callback = None\r\n+        self._batch_callback = None  # Add this line\r\n+\r\n+    def add_progress_callback(self, callback):\r\n+        \"\"\"Add a callback function to monitor training progress.\r\n+        \r\n+        Args:\r\n+            callback: Function that takes (epoch, train_loss, val_loss) as parameters\r\n+        \"\"\"\r\n+        self._progress_callback = callback\r\n+\r\n+    def add_batch_callback(self, callback):\r\n+        \"\"\"Add a callback function to monitor batch-level progress.\r\n+        \r\n+        Args:\r\n+            callback: Function that takes (batch_index, total_batches, loss) as parameters\r\n+        \"\"\"\r\n+        self._batch_callback = callback\r\n+\r\n+    def _validate_resolutions(self) -> None:\r\n+        \"\"\"Validate that resolutions are consistent across configurations.\"\"\"\r\n+        input_res = self.config.dataset_config.input_resolution_minutes\r\n+        forecast_res = self.config.dataset_config.forecast_resolution_minutes\r\n+        \r\n+        # Check model type matches resolution\r\n+        if self.config.model_config.model_type.is_resolution_specific:\r\n+            expected_type = ModelType.get_for_resolution(forecast_res)\r\n+            if self.config.model_config.model_type != expected_type:\r\n+                raise ValueError(\r\n+                    f\"Model type {self.config.model_config.model_type} does not match \"\r\n+                    f\"forecast resolution {forecast_res} minutes. Expected {expected_type}\"\r\n+                )\r\n+\r\n+    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n+        \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n+        self.data_loader = TimeSeriesLoader(\r\n+            time_variable=self.config.dataset_config.time_variable,\r\n+            target_variable=self.config.dataset_config.target_variable\r\n+        )\r\n+        return self.data_loader\r\n+\r\n+    def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n+        \"\"\"Prepare train, validation and test datasets with resolution handling.\"\"\"\r\n+        if not self.data_loader:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n+\r\n+        # Load and potentially resample data\r\n+        df = self.data_loader.load(self.config.data_path)\r\n+        \r\n+        # Resample if input and forecast resolutions differ\r\n+        if self.config.dataset_config.needs_resampling():\r\n+            df = self._resample_data(\r\n+                df,\r\n+                target_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+            )\r\n+\r\n+        # Get time intervals adjusted for resolution\r\n+        train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n+\r\n+        # Split data\r\n+        train_df, val_df, test_df = self.data_loader.split(\r\n+            df,\r\n+            train_interval,\r\n+            val_interval,\r\n+            test_interval\r\n+        )\r\n+\r\n+        # Select appropriate dataset class\r\n+        dataset_cls = self._get_dataset_class()\r\n+\r\n+        # Create datasets\r\n+        train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n+        val_dataset = dataset_cls(\r\n+            val_df,\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n+        )\r\n+        test_dataset = dataset_cls(\r\n+            test_df,\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n+        )\r\n+\r\n+        return train_dataset, val_dataset, test_dataset\r\n+\r\n+    def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n+        \"\"\"Resample data to match the target resolution.\"\"\"\r\n+        if self.data_loader is None:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n+        rule = f'{target_resolution_minutes}T'\r\n+        resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n+        return resampled.reset_index()\r\n+\r\n+    def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n+            return self._create_monthly_aligned_intervals()\r\n+        \r\n+        # For daily predictions, ensure intervals align with day boundaries\r\n+        elif self.config.dataset_config.forecast_resolution_minutes >= 1440:  # Daily\r\n+            return self._create_daily_aligned_intervals()\r\n+            \r\n+        # For hourly and sub-hourly, use standard intervals\r\n+        return self._create_time_intervals()\r\n+\r\n+    def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create monthly aligned intervals.\"\"\"\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create daily aligned intervals.\"\"\"\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create standard time intervals.\"\"\"\r\n+        return self._create_monthly_aligned_intervals()\r\n+\r\n+    def _get_dataset_class(self):\r\n+        \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n+        if self.config.model_config.model_type.is_transformer:\r\n+            if self.config.model_config.model_type.is_resolution_specific:\r\n+                # Use TransformerDataset with resolution-specific optimizations\r\n+                return TransformerDataset\r\n+            return TransformerDataset\r\n+        return StandardDataset\r\n+\r\n+    def setup_model(self) -> None:\r\n+        \"\"\"Setup model and associated wrapper with resolution awareness.\"\"\"\r\n+        # Convert config to dict for factory\r\n+        model_config_dict = dataclasses.asdict(self.config.model_config)\r\n+\r\n+        # Create model using factory\r\n+        model = ModelFactory.create_base_model(\r\n+            self.config.model_config.model_type,\r\n+            model_config_dict\r\n+        )\r\n+\r\n+        # Convert training config to dict\r\n+        training_config_dict = dataclasses.asdict(self.config.training_config)\r\n+\r\n+        # Create appropriate wrapper\r\n+        if self.config.model_config.model_type == ModelType.LINEAR_REGRESSION:\r\n+            wrapper: Union[SklearnWrapper, PyTorchWrapper] = SklearnWrapper(\r\n+                cast(Any, model),\r\n+                self.config.model_config.model_type,\r\n+                training_config_dict\r\n+            )\r\n+        else:\r\n+            wrapper = PyTorchWrapper(\r\n+                model,\r\n+                self.config.model_config.model_type,\r\n+                training_config_dict\r\n+            )\r\n+        \r\n+        self.model_wrapper = wrapper\r\n+\r\n+        self.logger.debug(\"Model setup complete\", {\r\n+            \"device_config\": training_config_dict.get('device', 'Not specified'),\r\n+            \"cuda_available\": torch.cuda.is_available(),\r\n+            \"cuda_device\": torch.cuda.current_device() if torch.cuda.is_available() else None,\r\n+            \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0\r\n+        })\r\n+\r\n+    def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n+        \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+\r\n+        # Make predictions\r\n+        test_start = time.time()\r\n+        predictions, targets = self.model_wrapper.predict(test_dataset)\r\n+        self.test_time = time.time() - test_start\r\n+\r\n+        # Get timestamps\r\n+        timestamps = test_dataset.time_labels\r\n+        if timestamps is None:\r\n+            timestamps = np.arange(len(predictions))\r\n+\r\n+        # Create evaluator with resolution-aware configuration\r\n+        evaluator = Evaluator(\r\n+            scaler=self.config.dataset_config.time_series_scaler,\r\n+            metric_config=MetricConfig(),  # Add this if needed\r\n+            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+        )\r\n+\r\n+        # Evaluate predictions\r\n+        evaluation = evaluator.evaluate(\r\n+            predictions=predictions,\r\n+            targets=targets,\r\n+            timestamps=cast(NDArray, timestamps),\r\n+            num_variables=1\r\n+        )\r\n+\r\n+        # Create experiment\r\n+        self.experiment = Experiment(\r\n+            model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n+            evaluation=evaluation,\r\n+            training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n+            training_report=self.training_report,\r\n+            training_time=self.training_time,\r\n+            test_time=self.test_time\r\n+        )\r\n+\r\n+        # Save model if path provided\r\n+        if self.config.model_save_path:\r\n+            self._save_model(self.config.model_save_path)\r\n+\r\n+    @property\r\n+    def training_report(self) -> TrainingReport:  # Change return type\r\n+        \"\"\"Get training report.\"\"\"\r\n+        if not hasattr(self, '_training_report'):\r\n+            self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n+        return self._training_report\r\n+\r\n+    def train_model(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Tuple[PyTorchWrapper, TrainingReport]:\r\n+        \"\"\"Train model implementation.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n+\r\n+        device = self.model_wrapper.device\r\n+        self.logger.info(f\"Starting model training\", {\"device\": str(device)})\r\n+\r\n+        training_report = TrainingReport(\r\n+            train_losses=[],\r\n+            val_losses=[],\r\n+            learning_rates=[],\r\n+            epochs=0,\r\n+            additional_metrics={}\r\n+        )\r\n+\r\n+        # Initialize evaluator for metric calculations\r\n+        evaluator = Evaluator(\r\n+            scaler=self.config.dataset_config.time_series_scaler,\r\n+            metric_config=MetricConfig(\r\n+                resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+            ),\r\n+            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+        )\r\n+        \r\n+        self.logger.debug(\"CUDA memory status\", {\r\n+            \"memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\r\n+            \"memory_cached_gb\": torch.cuda.memory_reserved() / 1e9\r\n+        })\r\n+\r\n+        for epoch in range(self.config.training_config.max_epochs):\r\n+            if epoch == 0 or epoch % 10 == 0:  # Print every 10 epochs\r\n+                self.logger.debug(f\"Epoch status\", {\r\n+                    \"epoch\": epoch,\r\n+                    \"model_device\": str(next(self.model_wrapper.model.parameters()).device),\r\n+                    \"cuda_memory_gb\": torch.cuda.memory_allocated() / 1e9\r\n+                })\r\n+\r\n+            epoch_loss = 0.0\r\n+            num_batches = 0\r\n+            \r\n+            # Set model to training mode\r\n+            if hasattr(self.model_wrapper.model, 'train'):\r\n+                self.model_wrapper.model.train()\r\n+                \r\n+            epoch_predictions = []\r\n+            epoch_targets = []\r\n+            \r\n+            # Process each batch\r\n+            for batch_idx, (batch_input, batch_target) in enumerate(train_loader):\r\n+                # Move batch to correct device\r\n+                if isinstance(batch_input, tuple):\r\n+                    batch_input = tuple(b.to(device) for b in batch_input)\r\n+                else:\r\n+                    batch_input = batch_input.to(device)\r\n+                batch_target = batch_target.to(device)\r\n+                \r\n+                loss = self._process_training_batch(batch_input, batch_target)\r\n+                epoch_loss += loss\r\n+                num_batches += 1\r\n+\r\n+                # Call batch callback\r\n+                if self._batch_callback:\r\n+                    self._batch_callback(batch_idx, len(train_loader), float(loss))\r\n+\r\n+                # Store predictions for evaluation\r\n+                with torch.no_grad():\r\n+                    if isinstance(batch_input, tuple):\r\n+                        predictions = self.model_wrapper.model(*[b.to(device) for b in batch_input])\r\n+                    else:\r\n+                        predictions = self.model_wrapper.model(batch_input.to(device))\r\n+                    epoch_predictions.append(predictions.cpu())\r\n+                    epoch_targets.append(batch_target.cpu())\r\n+\r\n+                \r\n+            # Process epoch results\r\n+            avg_epoch_loss = epoch_loss / num_batches\r\n+            training_report.train_losses.append(avg_epoch_loss)\r\n+            \r\n+            # Concatenate predictions and targets\r\n+            epoch_predictions = torch.cat(epoch_predictions)\r\n+            epoch_targets = torch.cat(epoch_targets)\r\n+            \r\n+            # Validation phase\r\n+            val_loss = None\r\n+            val_predictions = None\r\n+            val_targets = None\r\n+            if val_loader:\r\n+                val_loss, val_predictions, val_targets = self._validate_model(val_loader)\r\n+                training_report.val_losses.append(val_loss)\r\n+            \r\n+            # Call progress callback if set\r\n+            if self._progress_callback:\r\n+                self._progress_callback(\r\n+                    epoch=epoch,\r\n+                    train_loss=float(avg_epoch_loss),\r\n+                    val_loss=float(val_loss) if val_loss is not None else None,\r\n+                    y_pred=val_predictions if val_predictions is not None else epoch_predictions,\r\n+                    y_true=val_targets if val_targets is not None else epoch_targets\r\n+                )\r\n+            \r\n+            # Log progress\r\n+            if (epoch + 1) % 10 == 0:\r\n+                self.logger.info(\"Training progress\", {\r\n+                    \"epoch\": epoch + 1,\r\n+                    \"total_epochs\": self.config.training_config.max_epochs,\r\n+                    \"loss\": float(avg_epoch_loss)\r\n+                })\r\n+        \r\n+        return self.model_wrapper, training_report\r\n+\r\n+    def _process_training_batch(self, batch_input: Any, batch_target: torch.Tensor) -> float:\r\n+        \"\"\"Process a training batch with proper gradient handling.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+\r\n+        try:\r\n+            # Handle transformer models\r\n+            if (isinstance(self.model_wrapper, PyTorchWrapper) and \r\n+                self.config.model_config.model_type.is_transformer and \r\n+                isinstance(batch_input, tuple)):\r\n+                \r\n+                src, tgt = batch_input\r\n+                device = self.model_wrapper.device\r\n+                dtype = self.model_wrapper.dtype\r\n+                \r\n+                # Ensure inputs have gradients enabled\r\n+                if not src.requires_grad:\r\n+                    src.requires_grad_(True)\r\n+                if not tgt.requires_grad:\r\n+                    tgt.requires_grad_(True)\r\n+                \r\n+                # Generate masks\r\n+                src_mask = self.model_wrapper.model.generate_square_subsequent_mask(src.size(1)).to(\r\n+                    device=device, dtype=dtype\r\n+                )\r\n+                tgt_mask = self.model_wrapper.model.generate_square_subsequent_mask(tgt.size(1)).to(\r\n+                    device=device, dtype=dtype\r\n+                )\r\n+                \r\n+                # Log gradient status for debugging\r\n+                self.logger.debug(\"Batch gradient status\", {\r\n+                    \"src_requires_grad\": src.requires_grad,\r\n+                    \"tgt_requires_grad\": tgt.requires_grad,\r\n+                    \"src_is_leaf\": src.is_leaf,\r\n+                    \"tgt_is_leaf\": tgt.is_leaf\r\n+                })\r\n+                \r\n+                loss = self.model_wrapper.training_step(\r\n+                    (src, tgt), \r\n+                    batch_target,\r\n+                    src_mask=src_mask,\r\n+                    tgt_mask=tgt_mask,\r\n+                    use_reentrant=False  # Explicitly set checkpointing parameter\r\n+                )\r\n+                return float(loss)\r\n+\r\n+            # Handle regular models\r\n+            if isinstance(batch_input, tuple):\r\n+                batch_input = batch_input[0]\r\n+                if not batch_input.requires_grad:\r\n+                    batch_input.requires_grad_(True)\r\n+                    \r\n+            loss = self.model_wrapper.training_step(batch_input, batch_target)\r\n+            return float(loss)\r\n+\r\n+        except RuntimeError as e:\r\n+            error_info = {\r\n+                \"error\": str(e),\r\n+                \"target_shape\": batch_target.shape,\r\n+                \"target_device\": str(batch_target.device),\r\n+                \"target_dtype\": str(batch_target.dtype)\r\n+            }\r\n+            \r\n+            if isinstance(batch_input, tuple):\r\n+                error_info.update({\r\n+                    \"src_shape\": batch_input[0].shape,\r\n+                    \"src_device\": str(batch_input[0].device),\r\n+                    \"src_requires_grad\": batch_input[0].requires_grad,\r\n+                    \"tgt_shape\": batch_input[1].shape if len(batch_input) > 1 else None,\r\n+                    \"tgt_device\": str(batch_input[1].device) if len(batch_input) > 1 else None,\r\n+                    \"tgt_requires_grad\": batch_input[1].requires_grad if len(batch_input) > 1 else None\r\n+                })\r\n+            else:\r\n+                error_info.update({\r\n+                    \"input_shape\": batch_input.shape,\r\n+                    \"input_device\": str(batch_input.device),\r\n+                    \"input_requires_grad\": batch_input.requires_grad\r\n+                })\r\n+                \r\n+            self.logger.error(\"Training batch processing failed\", error_info)\r\n+            raise\r\n+    \r\n+    def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n+        \"\"\"Handle validation phase of training.\"\"\"\r\n+        val_loss, _, _ = self._validate_model(val_loader)\r\n+        if not hasattr(training_report, 'val_losses'):\r\n+            training_report.val_losses = []\r\n+        training_report.val_losses.append(float(val_loss))\r\n+        \r\n+        self.logger.debug(\"Validation completed\", {\r\n+            \"validation_loss\": float(val_loss),\r\n+            \"epoch\": len(training_report.val_losses)\r\n+        })\r\n+\r\n+    def _validate_model(self, val_loader: DataLoader) -> Tuple[float, Optional[torch.Tensor], Optional[torch.Tensor]]:\r\n+        \"\"\"Perform validation and return loss and predictions.\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        total_loss = 0.0\r\n+        num_batches = 0\r\n+        all_predictions = []\r\n+        all_targets = []\r\n+        \r\n+        # Get device for PyTorch models\r\n+        device = None\r\n+        is_transformer = False\r\n+        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            device = next(self.model_wrapper.model.parameters()).device\r\n+            is_transformer = self.config.model_config.model_type.is_transformer\r\n+\r\n+        with torch.no_grad():\r\n+            for batch_input, batch_target in val_loader:\r\n+                # Handle device placement for PyTorch models\r\n+                if device is not None:\r\n+                    if isinstance(batch_input, tuple):\r\n+                        batch_input = tuple(b.to(device) for b in batch_input)\r\n+                    else:\r\n+                        batch_input = batch_input.to(device)\r\n+                    batch_target = batch_target.to(device)\r\n+\r\n+                # Get predictions\r\n+                if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                    predictions = self.model_wrapper.model(batch_input) if not is_transformer else \\\r\n+                                self.model_wrapper.model(*batch_input)\r\n+                else:\r\n+                    # For sklearn models\r\n+                    if isinstance(batch_input, tuple):\r\n+                        batch_input = batch_input[0]\r\n+                    predictions = torch.tensor(self.model_wrapper.model.predict(batch_input.numpy()))\r\n+\r\n+                # Calculate and accumulate loss\r\n+                loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                total_loss += float(loss)  # Ensure we get a float\r\n+                num_batches += 1\r\n+\r\n+                # Store predictions and targets (move to CPU if needed)\r\n+                all_predictions.append(predictions.cpu() if device is not None else predictions)\r\n+                all_targets.append(batch_target.cpu() if device is not None else batch_target)\r\n+\r\n+        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\r\n+        predictions = torch.cat(all_predictions) if all_predictions else None\r\n+        targets = torch.cat(all_targets) if all_targets else None\r\n+\r\n+        return avg_loss, predictions, targets\r\n+\r\n+    def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n+        \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n+        X_list, y_list = [], []\r\n+        for batch_input, batch_target in val_loader:\r\n+            X_list.append(batch_input.numpy())\r\n+            y_list.append(batch_target.numpy())\r\n+        return np.vstack(X_list), np.vstack(y_list)\r\n+\r\n+    def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n+        \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n+        if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n+            return False\r\n+            \r\n+        patience = self.config.training_config.early_stopping_patience\r\n+        recent_losses = training_report.val_losses[-patience:]\r\n+        min_loss_idx = recent_losses.index(min(recent_losses))\r\n+        \r\n+        return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n+\r\n+    def training_step(\r\n+        self, \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_target: torch.Tensor\r\n+    ) -> float:\r\n+        \"\"\"Modified training step to handle transformer input format\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        if isinstance(batch_input, tuple):\r\n+            if self.config.model_config.model_type.is_transformer:\r\n+                src = batch_input[0]\r\n+                tgt = batch_target[:, :-1]\r\n+                tgt_y = batch_target[:, 1:]\r\n+                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n+            batch_input = batch_input[0]\r\n+            \r\n+        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733263378476,
            "name": "Commit-0",
            "content": "# pipeline/implementations/time_series_pipeline.py\r\nfrom __future__ import annotations\r\nimport time\r\nfrom typing import Tuple, Optional, Dict, Any, cast, Union\r\nfrom pathlib import Path\r\nimport torch\r\nimport dataclasses\r\nfrom argparse import Namespace\r\nimport numpy as np\r\nfrom numpy.typing import NDArray\r\nimport pandas as pd  # Add this import statement\r\nfrom datetime import date  # Add this import statement\r\n\r\nfrom data_loading.datasets.standard_dataset import StandardDataset\r\nfrom data_loading.datasets.transformer_dataset import TransformerDataset\r\nfrom data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\nfrom evaluation.evaluator import Evaluator\r\nfrom evaluation.metrics import MetricConfig\r\nfrom experiments.experiment import Experiment\r\nfrom models.base.base_wrapper import BaseWrapper\r\nfrom models.registry.factory import ModelFactory\r\nfrom models.registry.model_types import ModelType\r\nfrom models.wrappers.pytorch_wrapper import PyTorchWrapper\r\nfrom models.wrappers.sklearn_wrapper import SklearnWrapper\r\nfrom pipeline.base.base_pipeline import BasePipeline\r\nfrom training.reports.training_report import TrainingReport\r\nfrom torch.utils.data import DataLoader\r\nfrom utils.logging.logger import Logger\r\nfrom utils.logging.config import LoggerConfig, LogLevel\r\n\r\nclass TimeSeriesPipeline(BasePipeline):\r\n    \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.data_loader: Optional[TimeSeriesLoader] = None\r\n        self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n        \r\n        # Initialize logger\r\n        logger_config = LoggerConfig(\r\n            level=LogLevel.INFO,\r\n            component_name=\"TimeSeriesPipeline\",\r\n            include_timestamp=True\r\n        )\r\n        self.logger = Logger.get_logger(__name__, logger_config)\r\n        \r\n        # Validate resolutions during initialization\r\n        self._validate_resolutions()\r\n        self._progress_callback = None\r\n        self._batch_callback = None  # Add this line\r\n\r\n    def add_progress_callback(self, callback):\r\n        \"\"\"Add a callback function to monitor training progress.\r\n        \r\n        Args:\r\n            callback: Function that takes (epoch, train_loss, val_loss) as parameters\r\n        \"\"\"\r\n        self._progress_callback = callback\r\n\r\n    def add_batch_callback(self, callback):\r\n        \"\"\"Add a callback function to monitor batch-level progress.\r\n        \r\n        Args:\r\n            callback: Function that takes (batch_index, total_batches, loss) as parameters\r\n        \"\"\"\r\n        self._batch_callback = callback\r\n\r\n    def _validate_resolutions(self) -> None:\r\n        \"\"\"Validate that resolutions are consistent across configurations.\"\"\"\r\n        input_res = self.config.dataset_config.input_resolution_minutes\r\n        forecast_res = self.config.dataset_config.forecast_resolution_minutes\r\n        \r\n        # Check model type matches resolution\r\n        if self.config.model_config.model_type.is_resolution_specific:\r\n            expected_type = ModelType.get_for_resolution(forecast_res)\r\n            if self.config.model_config.model_type != expected_type:\r\n                raise ValueError(\r\n                    f\"Model type {self.config.model_config.model_type} does not match \"\r\n                    f\"forecast resolution {forecast_res} minutes. Expected {expected_type}\"\r\n                )\r\n\r\n    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n        \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n        self.data_loader = TimeSeriesLoader(\r\n            time_variable=self.config.dataset_config.time_variable,\r\n            target_variable=self.config.dataset_config.target_variable\r\n        )\r\n        return self.data_loader\r\n\r\n    def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n        \"\"\"Prepare train, validation and test datasets with resolution handling.\"\"\"\r\n        if not self.data_loader:\r\n            raise ValueError(\"Data loader not initialized\")\r\n\r\n        # Load and potentially resample data\r\n        df = self.data_loader.load(self.config.data_path)\r\n        \r\n        # Resample if input and forecast resolutions differ\r\n        if self.config.dataset_config.needs_resampling():\r\n            df = self._resample_data(\r\n                df,\r\n                target_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n            )\r\n\r\n        # Get time intervals adjusted for resolution\r\n        train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n\r\n        # Split data\r\n        train_df, val_df, test_df = self.data_loader.split(\r\n            df,\r\n            train_interval,\r\n            val_interval,\r\n            test_interval\r\n        )\r\n\r\n        # Select appropriate dataset class\r\n        dataset_cls = self._get_dataset_class()\r\n\r\n        # Create datasets\r\n        train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n        val_dataset = dataset_cls(\r\n            val_df,\r\n            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n        )\r\n        test_dataset = dataset_cls(\r\n            test_df,\r\n            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n        )\r\n\r\n        return train_dataset, val_dataset, test_dataset\r\n\r\n    def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n        \"\"\"Resample data to match the target resolution.\"\"\"\r\n        if self.data_loader is None:\r\n            raise ValueError(\"Data loader not initialized\")\r\n        rule = f'{target_resolution_minutes}T'\r\n        resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n        return resampled.reset_index()\r\n\r\n    def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n        if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n            return self._create_monthly_aligned_intervals()\r\n        \r\n        # For daily predictions, ensure intervals align with day boundaries\r\n        elif self.config.dataset_config.forecast_resolution_minutes >= 1440:  # Daily\r\n            return self._create_daily_aligned_intervals()\r\n            \r\n        # For hourly and sub-hourly, use standard intervals\r\n        return self._create_time_intervals()\r\n\r\n    def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n        \"\"\"Create monthly aligned intervals.\"\"\"\r\n        train_start, train_end = self.config.train_dates\r\n        val_start, val_end = self.config.val_dates\r\n        test_start, test_end = self.config.test_dates\r\n        \r\n        return (\r\n            TimeInterval(min_date=train_start, max_date=train_end),\r\n            TimeInterval(min_date=val_start, max_date=val_end),\r\n            TimeInterval(min_date=test_start, max_date=test_end)\r\n        )\r\n\r\n    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n        \"\"\"Create daily aligned intervals.\"\"\"\r\n        train_start, train_end = self.config.train_dates\r\n        val_start, val_end = self.config.val_dates\r\n        test_start, test_end = self.config.test_dates\r\n        \r\n        return (\r\n            TimeInterval(min_date=train_start, max_date=train_end),\r\n            TimeInterval(min_date=val_start, max_date=val_end),\r\n            TimeInterval(min_date=test_start, max_date=test_end)\r\n        )\r\n\r\n    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n        \"\"\"Create standard time intervals.\"\"\"\r\n        return self._create_monthly_aligned_intervals()\r\n\r\n    def _get_dataset_class(self):\r\n        \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n        if self.config.model_config.model_type.is_transformer:\r\n            if self.config.model_config.model_type.is_resolution_specific:\r\n                # Use TransformerDataset with resolution-specific optimizations\r\n                return TransformerDataset\r\n            return TransformerDataset\r\n        return StandardDataset\r\n\r\n    def setup_model(self) -> None:\r\n        \"\"\"Setup model and associated wrapper with resolution awareness.\"\"\"\r\n        # Convert config to dict for factory\r\n        model_config_dict = dataclasses.asdict(self.config.model_config)\r\n\r\n        # Create model using factory\r\n        model = ModelFactory.create_base_model(\r\n            self.config.model_config.model_type,\r\n            model_config_dict\r\n        )\r\n\r\n        # Convert training config to dict\r\n        training_config_dict = dataclasses.asdict(self.config.training_config)\r\n\r\n        # Create appropriate wrapper\r\n        if self.config.model_config.model_type == ModelType.LINEAR_REGRESSION:\r\n            wrapper: Union[SklearnWrapper, PyTorchWrapper] = SklearnWrapper(\r\n                cast(Any, model),\r\n                self.config.model_config.model_type,\r\n                training_config_dict\r\n            )\r\n        else:\r\n            wrapper = PyTorchWrapper(\r\n                model,\r\n                self.config.model_config.model_type,\r\n                training_config_dict\r\n            )\r\n        \r\n        self.model_wrapper = wrapper\r\n\r\n        self.logger.debug(\"Model setup complete\", {\r\n            \"device_config\": training_config_dict.get('device', 'Not specified'),\r\n            \"cuda_available\": torch.cuda.is_available(),\r\n            \"cuda_device\": torch.cuda.current_device() if torch.cuda.is_available() else None,\r\n            \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0\r\n        })\r\n\r\n    def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n        \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n        if not self.model_wrapper:\r\n            raise ValueError(\"Model wrapper not initialized\")\r\n\r\n        # Make predictions\r\n        test_start = time.time()\r\n        predictions, targets = self.model_wrapper.predict(test_dataset)\r\n        self.test_time = time.time() - test_start\r\n\r\n        # Get timestamps\r\n        timestamps = test_dataset.time_labels\r\n        if timestamps is None:\r\n            timestamps = np.arange(len(predictions))\r\n\r\n        # Create evaluator with resolution-aware configuration\r\n        evaluator = Evaluator(\r\n            scaler=self.config.dataset_config.time_series_scaler,\r\n            metric_config=MetricConfig(),  # Add this if needed\r\n            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n        )\r\n\r\n        # Evaluate predictions\r\n        evaluation = evaluator.evaluate(\r\n            predictions=predictions,\r\n            targets=targets,\r\n            timestamps=cast(NDArray, timestamps),\r\n            num_variables=1\r\n        )\r\n\r\n        # Create experiment\r\n        self.experiment = Experiment(\r\n            model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n            evaluation=evaluation,\r\n            training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n            training_report=self.training_report,\r\n            training_time=self.training_time,\r\n            test_time=self.test_time\r\n        )\r\n\r\n        # Save model if path provided\r\n        if self.config.model_save_path:\r\n            self._save_model(self.config.model_save_path)\r\n\r\n    @property\r\n    def training_report(self) -> TrainingReport:  # Change return type\r\n        \"\"\"Get training report.\"\"\"\r\n        if not hasattr(self, '_training_report'):\r\n            self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n        return self._training_report\r\n\r\n    def train_model(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Tuple[PyTorchWrapper, TrainingReport]:\r\n        \"\"\"Train model implementation.\"\"\"\r\n        if not self.model_wrapper:\r\n            raise ValueError(\"Model wrapper not initialized\")\r\n        if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n            raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n\r\n        device = self.model_wrapper.device\r\n        self.logger.info(f\"Starting model training\", {\"device\": str(device)})\r\n\r\n        training_report = TrainingReport(\r\n            train_losses=[],\r\n            val_losses=[],\r\n            learning_rates=[],\r\n            epochs=0,\r\n            additional_metrics={}\r\n        )\r\n\r\n        # Initialize evaluator for metric calculations\r\n        evaluator = Evaluator(\r\n            scaler=self.config.dataset_config.time_series_scaler,\r\n            metric_config=MetricConfig(\r\n                resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n            ),\r\n            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n        )\r\n        \r\n        self.logger.debug(\"CUDA memory status\", {\r\n            \"memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\r\n            \"memory_cached_gb\": torch.cuda.memory_reserved() / 1e9\r\n        })\r\n\r\n        for epoch in range(self.config.training_config.max_epochs):\r\n            if epoch == 0 or epoch % 10 == 0:  # Print every 10 epochs\r\n                self.logger.debug(f\"Epoch status\", {\r\n                    \"epoch\": epoch,\r\n                    \"model_device\": str(next(self.model_wrapper.model.parameters()).device),\r\n                    \"cuda_memory_gb\": torch.cuda.memory_allocated() / 1e9\r\n                })\r\n\r\n            epoch_loss = 0.0\r\n            num_batches = 0\r\n            \r\n            # Set model to training mode\r\n            if hasattr(self.model_wrapper.model, 'train'):\r\n                self.model_wrapper.model.train()\r\n                \r\n            epoch_predictions = []\r\n            epoch_targets = []\r\n            \r\n            # Process each batch\r\n            for batch_idx, (batch_input, batch_target) in enumerate(train_loader):\r\n                # Move batch to correct device\r\n                if isinstance(batch_input, tuple):\r\n                    batch_input = tuple(b.to(device) for b in batch_input)\r\n                else:\r\n                    batch_input = batch_input.to(device)\r\n                batch_target = batch_target.to(device)\r\n                \r\n                loss = self._process_training_batch(batch_input, batch_target)\r\n                epoch_loss += loss\r\n                num_batches += 1\r\n\r\n                # Call batch callback\r\n                if self._batch_callback:\r\n                    self._batch_callback(batch_idx, len(train_loader), float(loss))\r\n\r\n                # Store predictions for evaluation\r\n                with torch.no_grad():\r\n                    if isinstance(batch_input, tuple):\r\n                        predictions = self.model_wrapper.model(*[b.to(device) for b in batch_input])\r\n                    else:\r\n                        predictions = self.model_wrapper.model(batch_input.to(device))\r\n                    epoch_predictions.append(predictions.cpu())\r\n                    epoch_targets.append(batch_target.cpu())\r\n\r\n                \r\n            # Process epoch results\r\n            avg_epoch_loss = epoch_loss / num_batches\r\n            training_report.train_losses.append(avg_epoch_loss)\r\n            \r\n            # Concatenate predictions and targets\r\n            epoch_predictions = torch.cat(epoch_predictions)\r\n            epoch_targets = torch.cat(epoch_targets)\r\n            \r\n            # Validation phase\r\n            val_loss = None\r\n            val_predictions = None\r\n            val_targets = None\r\n            if val_loader:\r\n                val_loss, val_predictions, val_targets = self._validate_model(val_loader)\r\n                training_report.val_losses.append(val_loss)\r\n            \r\n            # Call progress callback if set\r\n            if self._progress_callback:\r\n                self._progress_callback(\r\n                    epoch=epoch,\r\n                    train_loss=float(avg_epoch_loss),\r\n                    val_loss=float(val_loss) if val_loss is not None else None,\r\n                    y_pred=val_predictions if val_predictions is not None else epoch_predictions,\r\n                    y_true=val_targets if val_targets is not None else epoch_targets\r\n                )\r\n            \r\n            # Log progress\r\n            if (epoch + 1) % 10 == 0:\r\n                self.logger.info(\"Training progress\", {\r\n                    \"epoch\": epoch + 1,\r\n                    \"total_epochs\": self.config.training_config.max_epochs,\r\n                    \"loss\": float(avg_epoch_loss)\r\n                })\r\n        \r\n        return self.model_wrapper, training_report\r\n\r\n    def _process_training_batch(self, batch_input: Any, batch_target: torch.Tensor) -> float:\r\n        \"\"\"Process a single training batch with correct dtype handling.\"\"\"\r\n        if not self.model_wrapper:\r\n            raise ValueError(\"Model wrapper not initialized\")\r\n\r\n        try:\r\n            # Handle transformer models\r\n            if (isinstance(self.model_wrapper, PyTorchWrapper) and \r\n                self.config.model_config.model_type.is_transformer and \r\n                isinstance(batch_input, tuple)):\r\n                \r\n                src, tgt = batch_input\r\n                device = self.model_wrapper.device\r\n                dtype = self.model_wrapper.dtype\r\n                \r\n                # Generate masks only for PyTorch transformer models\r\n                src_mask = self.model_wrapper.model.generate_square_subsequent_mask(src.size(1)).to(\r\n                    device=device, dtype=dtype\r\n                )\r\n                tgt_mask = self.model_wrapper.model.generate_square_subsequent_mask(tgt.size(1)).to(\r\n                    device=device, dtype=dtype\r\n                )\r\n                \r\n                loss = self.model_wrapper.training_step(\r\n                    (src, tgt), \r\n                    batch_target,\r\n                    src_mask=src_mask,\r\n                    tgt_mask=tgt_mask\r\n                )\r\n                return float(loss)\r\n\r\n            # Handle regular models \r\n            if isinstance(batch_input, tuple):\r\n                batch_input = batch_input[0]\r\n                \r\n            loss = self.model_wrapper.training_step(batch_input, batch_target)\r\n            return float(loss)\r\n\r\n        except RuntimeError as e:\r\n            error_info = {\r\n                \"error\": str(e),\r\n                \"target_shape\": batch_target.shape,\r\n                \"target_device\": str(batch_target.device),\r\n                \"target_dtype\": str(batch_target.dtype)\r\n            }\r\n            \r\n            if isinstance(batch_input, tuple):\r\n                error_info.update({\r\n                    \"src_shape\": batch_input[0].shape,\r\n                    \"src_device\": str(batch_input[0].device),\r\n                    \"tgt_shape\": batch_input[1].shape if len(batch_input) > 1 else None,\r\n                    \"tgt_device\": str(batch_input[1].device) if len(batch_input) > 1 else None\r\n                })\r\n            else:\r\n                error_info.update({\r\n                    \"input_shape\": batch_input.shape,\r\n                    \"input_device\": str(batch_input.device) \r\n                })\r\n                \r\n            self.logger.error(\"Training batch processing failed\", error_info)\r\n            raise\r\n\r\n    def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n        \"\"\"Handle validation phase of training.\"\"\"\r\n        val_loss, _, _ = self._validate_model(val_loader)\r\n        if not hasattr(training_report, 'val_losses'):\r\n            training_report.val_losses = []\r\n        training_report.val_losses.append(float(val_loss))\r\n        \r\n        self.logger.debug(\"Validation completed\", {\r\n            \"validation_loss\": float(val_loss),\r\n            \"epoch\": len(training_report.val_losses)\r\n        })\r\n\r\n    def _validate_model(self, val_loader: DataLoader) -> Tuple[float, Optional[torch.Tensor], Optional[torch.Tensor]]:\r\n        \"\"\"Perform validation and return loss and predictions.\"\"\"\r\n        if self.model_wrapper is None:\r\n            raise ValueError(\"Model wrapper not initialized\")\r\n        \r\n        total_loss = 0.0\r\n        num_batches = 0\r\n        all_predictions = []\r\n        all_targets = []\r\n        \r\n        # Get device for PyTorch models\r\n        device = None\r\n        is_transformer = False\r\n        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n            device = next(self.model_wrapper.model.parameters()).device\r\n            is_transformer = self.config.model_config.model_type.is_transformer\r\n\r\n        with torch.no_grad():\r\n            for batch_input, batch_target in val_loader:\r\n                # Handle device placement for PyTorch models\r\n                if device is not None:\r\n                    if isinstance(batch_input, tuple):\r\n                        batch_input = tuple(b.to(device) for b in batch_input)\r\n                    else:\r\n                        batch_input = batch_input.to(device)\r\n                    batch_target = batch_target.to(device)\r\n\r\n                # Get predictions\r\n                if isinstance(self.model_wrapper, PyTorchWrapper):\r\n                    predictions = self.model_wrapper.model(batch_input) if not is_transformer else \\\r\n                                self.model_wrapper.model(*batch_input)\r\n                else:\r\n                    # For sklearn models\r\n                    if isinstance(batch_input, tuple):\r\n                        batch_input = batch_input[0]\r\n                    predictions = torch.tensor(self.model_wrapper.model.predict(batch_input.numpy()))\r\n\r\n                # Calculate and accumulate loss\r\n                loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n                total_loss += float(loss)  # Ensure we get a float\r\n                num_batches += 1\r\n\r\n                # Store predictions and targets (move to CPU if needed)\r\n                all_predictions.append(predictions.cpu() if device is not None else predictions)\r\n                all_targets.append(batch_target.cpu() if device is not None else batch_target)\r\n\r\n        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\r\n        predictions = torch.cat(all_predictions) if all_predictions else None\r\n        targets = torch.cat(all_targets) if all_targets else None\r\n\r\n        return avg_loss, predictions, targets\r\n\r\n    def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n        \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n        X_list, y_list = [], []\r\n        for batch_input, batch_target in val_loader:\r\n            X_list.append(batch_input.numpy())\r\n            y_list.append(batch_target.numpy())\r\n        return np.vstack(X_list), np.vstack(y_list)\r\n\r\n    def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n        \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n        if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n            return False\r\n            \r\n        patience = self.config.training_config.early_stopping_patience\r\n        recent_losses = training_report.val_losses[-patience:]\r\n        min_loss_idx = recent_losses.index(min(recent_losses))\r\n        \r\n        return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n\r\n    def training_step(\r\n        self, \r\n        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n        batch_target: torch.Tensor\r\n    ) -> float:\r\n        \"\"\"Modified training step to handle transformer input format\"\"\"\r\n        if self.model_wrapper is None:\r\n            raise ValueError(\"Model wrapper not initialized\")\r\n        \r\n        if isinstance(batch_input, tuple):\r\n            if self.config.model_config.model_type.is_transformer:\r\n                src = batch_input[0]\r\n                tgt = batch_target[:, :-1]\r\n                tgt_y = batch_target[:, 1:]\r\n                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n            batch_input = batch_input[0]\r\n            \r\n        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))"
        }
    ]
}