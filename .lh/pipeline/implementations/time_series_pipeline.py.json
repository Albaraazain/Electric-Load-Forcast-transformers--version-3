{
    "sourceFile": "pipeline/implementations/time_series_pipeline.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 21,
            "patches": [
                {
                    "date": 1733091623393,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733175989414,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+# path: pipeline/implementations/time_series_pipeline.py\r\n from __future__ import annotations\r\n \r\n import time\r\n from typing import Tuple, Optional, Dict, Any, cast, Union\r\n"
                },
                {
                    "date": 1733177049822,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,6 @@\n-# path: pipeline/implementations/time_series_pipeline.py\r\n+# pipeline/implementations/time_series_pipeline.py\r\n from __future__ import annotations\r\n-\r\n import time\r\n from typing import Tuple, Optional, Dict, Any, cast, Union\r\n from pathlib import Path\r\n import torch\r\n@@ -22,68 +21,108 @@\n from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n from pipeline.base.base_pipeline import BasePipeline\r\n \r\n class TimeSeriesPipeline(BasePipeline):\r\n+    \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n+\r\n     def __init__(self, *args, **kwargs):\r\n         super().__init__(*args, **kwargs)\r\n         self.data_loader: Optional[TimeSeriesLoader] = None\r\n         self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n+        \r\n+        # Validate resolutions during initialization\r\n+        self._validate_resolutions()\r\n \r\n+    def _validate_resolutions(self) -> None:\r\n+        \"\"\"Validate that resolutions are consistent across configurations.\"\"\"\r\n+        input_res = self.config.dataset_config.input_resolution_minutes\r\n+        forecast_res = self.config.dataset_config.forecast_resolution_minutes\r\n+        \r\n+        # Check model type matches resolution\r\n+        if self.config.model_config.model_type.is_resolution_specific:\r\n+            expected_type = ModelType.get_for_resolution(forecast_res)\r\n+            if self.config.model_config.model_type != expected_type:\r\n+                raise ValueError(\r\n+                    f\"Model type {self.config.model_config.model_type} does not match \"\r\n+                    f\"forecast resolution {forecast_res} minutes. Expected {expected_type}\"\r\n+                )\r\n+\r\n     def prepare_data_loader(self) -> TimeSeriesLoader:\r\n-        \"\"\"Initialize and configure the time series data loader.\"\"\"\r\n+        \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n         self.data_loader = TimeSeriesLoader(\r\n             time_variable=self.config.dataset_config.time_variable,\r\n-            target_variable=self.config.dataset_config.target_variable\r\n+            target_variable=self.config.dataset_config.target_variable,\r\n+            input_resolution=self.config.dataset_config.input_resolution_minutes,\r\n+            target_resolution=self.config.dataset_config.forecast_resolution_minutes\r\n         )\r\n         return self.data_loader\r\n \r\n     def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n-        \"\"\"Prepare train, validation and test datasets.\"\"\"\r\n+        \"\"\"Prepare train, validation and test datasets with resolution handling.\"\"\"\r\n         if not self.data_loader:\r\n             raise ValueError(\"Data loader not initialized\")\r\n \r\n-        # Load data\r\n+        # Load and potentially resample data\r\n         df = self.data_loader.load(self.config.data_path)\r\n+        \r\n+        # Resample if input and forecast resolutions differ\r\n+        if self.config.dataset_config.needs_resampling():\r\n+            df = self.data_loader.resample_data(\r\n+                df,\r\n+                target_resolution=self.config.dataset_config.forecast_resolution_minutes\r\n+            )\r\n \r\n-        # Get time intervals\r\n-        train_interval, val_interval, test_interval = self._create_time_intervals()\r\n+        # Get time intervals adjusted for resolution\r\n+        train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n \r\n         # Split data\r\n         train_df, val_df, test_df = self.data_loader.split(\r\n-            df,  # Add df as first argument\r\n+            df,\r\n             train_interval,\r\n             val_interval,\r\n             test_interval\r\n         )\r\n \r\n-        # Create appropriate dataset types based on model\r\n-        dataset_cls = (\r\n-            TransformerDataset\r\n-            if self.config.model_config.model_type.is_transformer\r\n-            else StandardDataset\r\n-        )\r\n+        # Select appropriate dataset class\r\n+        dataset_cls = self._get_dataset_class()\r\n \r\n         # Create datasets\r\n         train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n         val_dataset = dataset_cls(\r\n             val_df,\r\n-            dataclasses.replace(\r\n-                self.config.dataset_config,\r\n-                is_training_set=False\r\n-            )\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n         )\r\n         test_dataset = dataset_cls(\r\n             test_df,\r\n-            dataclasses.replace(\r\n-                self.config.dataset_config,\r\n-                is_training_set=False\r\n-            )\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n         )\r\n \r\n         return train_dataset, val_dataset, test_dataset\r\n \r\n+    def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create time intervals that align with the chosen resolution.\"\"\"\r\n+        # For monthly predictions, ensure intervals align with month boundaries\r\n+        if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n+            return self._create_monthly_aligned_intervals()\r\n+        \r\n+        # For daily predictions, ensure intervals align with day boundaries\r\n+        elif self.config.dataset_config.forecast_resolution_minutes >= 1440:  # Daily\r\n+            return self._create_daily_aligned_intervals()\r\n+            \r\n+        # For hourly and sub-hourly, use standard intervals\r\n+        return self._create_time_intervals()\r\n+\r\n+    def _get_dataset_class(self):\r\n+        \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n+        if self.config.model_config.model_type.is_transformer:\r\n+            if self.config.model_config.model_type.is_resolution_specific:\r\n+                # Use TransformerDataset with resolution-specific optimizations\r\n+                return TransformerDataset\r\n+            return TransformerDataset\r\n+        return StandardDataset\r\n+\r\n     def setup_model(self) -> None:\r\n-        \"\"\"Setup model and associated wrapper.\"\"\"\r\n+        \"\"\"Setup model and associated wrapper with resolution awareness.\"\"\"\r\n         # Convert config to dict for factory\r\n         model_config_dict = dataclasses.asdict(self.config.model_config)\r\n \r\n         # Create model using factory\r\n@@ -108,55 +147,40 @@\n                 self.config.model_config.model_type,\r\n                 training_config_dict\r\n             )\r\n         \r\n-        # Assign wrapper with proper type annotation\r\n         self.model_wrapper = wrapper\r\n \r\n-    def train_model(self, train_dataset: StandardDataset | TransformerDataset,\r\n-                    val_dataset: StandardDataset | TransformerDataset) -> None:\r\n-        \"\"\"Train the model and record metrics.\"\"\"\r\n-        if not self.model_wrapper:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-\r\n-        training_start = time.time()\r\n-\r\n-        self.training_report = self.model_wrapper.train(\r\n-            train_dataset,\r\n-            val_dataset\r\n-        )\r\n-\r\n-        self.training_time = time.time() - training_start\r\n-\r\n     def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n-        \"\"\"Evaluate model and create experiment.\"\"\"\r\n+        \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n         if not self.model_wrapper:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n \r\n         # Make predictions\r\n         test_start = time.time()\r\n         predictions, targets = self.model_wrapper.predict(test_dataset)\r\n         self.test_time = time.time() - test_start\r\n \r\n-        # Get timestamps with null check\r\n+        # Get timestamps\r\n         timestamps = test_dataset.time_labels\r\n         if timestamps is None:\r\n-            timestamps = np.arange(len(predictions))  # Fallback to indices if no timestamps\r\n+            timestamps = np.arange(len(predictions))\r\n \r\n-        # Create evaluator with the dataset's scaler\r\n+        # Create evaluator with resolution-aware configuration\r\n         evaluator = Evaluator(\r\n-            scaler=self.config.dataset_config.time_series_scaler\r\n+            scaler=self.config.dataset_config.time_series_scaler,\r\n+            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n         )\r\n \r\n         # Evaluate predictions\r\n         evaluation = evaluator.evaluate(\r\n             predictions=predictions,\r\n             targets=targets,\r\n-            timestamps=cast(NDArray, timestamps),  # Cast to satisfy type checker\r\n-            num_variables=1  # Assuming single variable prediction\r\n+            timestamps=cast(NDArray, timestamps),\r\n+            num_variables=1\r\n         )\r\n \r\n-        # Create experiment with type-safe arguments\r\n+        # Create experiment\r\n         self.experiment = Experiment(\r\n             model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n             evaluation=evaluation,\r\n             training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n@@ -166,27 +190,5 @@\n         )\r\n \r\n         # Save model if path provided\r\n         if self.config.model_save_path:\r\n-            self._save_model(str(self.config.model_save_path))\r\n-\r\n-    def _save_model(self, save_path: str) -> None:\r\n-        \"\"\"Save model artifacts.\"\"\"\r\n-        if not self.model_wrapper:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-\r\n-        # Save PyTorch model\r\n-        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n-            if hasattr(self.model_wrapper.model, 'state_dict'):\r\n-                torch.save(\r\n-                    self.model_wrapper.model.state_dict(),\r\n-                    f\"{save_path}/model.pt\"\r\n-                )\r\n-\r\n-            # Save scaler if exists\r\n-            if self.config.dataset_config.time_series_scaler:\r\n-                import pickle\r\n-                with open(f\"{save_path}/scaler.pkl\", \"wb\") as f:\r\n-                    pickle.dump(\r\n-                        self.config.dataset_config.time_series_scaler,\r\n-                        f\r\n-                    )\n\\ No newline at end of file\n+            self._save_model(str(self.config.model_save_path))\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733177367450,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,10 @@\n import dataclasses\r\n from argparse import Namespace\r\n import numpy as np\r\n from numpy.typing import NDArray\r\n+import pandas as pd  # Add this import statement\r\n+from datetime import date  # Add this import statement\r\n \r\n from data_loading.datasets.standard_dataset import StandardDataset\r\n from data_loading.datasets.transformer_dataset import TransformerDataset\r\n from data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\n@@ -49,11 +51,9 @@\n     def prepare_data_loader(self) -> TimeSeriesLoader:\r\n         \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n         self.data_loader = TimeSeriesLoader(\r\n             time_variable=self.config.dataset_config.time_variable,\r\n-            target_variable=self.config.dataset_config.target_variable,\r\n-            input_resolution=self.config.dataset_config.input_resolution_minutes,\r\n-            target_resolution=self.config.dataset_config.forecast_resolution_minutes\r\n+            target_variable=self.config.dataset_config.target_variable\r\n         )\r\n         return self.data_loader\r\n \r\n     def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n@@ -65,11 +65,11 @@\n         df = self.data_loader.load(self.config.data_path)\r\n         \r\n         # Resample if input and forecast resolutions differ\r\n         if self.config.dataset_config.needs_resampling():\r\n-            df = self.data_loader.resample_data(\r\n+            df = self._resample_data(\r\n                 df,\r\n-                target_resolution=self.config.dataset_config.forecast_resolution_minutes\r\n+                target_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n             )\r\n \r\n         # Get time intervals adjusted for resolution\r\n         train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n@@ -97,11 +97,15 @@\n         )\r\n \r\n         return train_dataset, val_dataset, test_dataset\r\n \r\n+    def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n+        \"\"\"Resample data to match the target resolution.\"\"\"\r\n+        rule = f'{target_resolution_minutes}T'\r\n+        resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n+        return resampled.reset_index()\r\n+\r\n     def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        \"\"\"Create time intervals that align with the chosen resolution.\"\"\"\r\n-        # For monthly predictions, ensure intervals align with month boundaries\r\n         if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n             return self._create_monthly_aligned_intervals()\r\n         \r\n         # For daily predictions, ensure intervals align with day boundaries\r\n@@ -110,8 +114,30 @@\n             \r\n         # For hourly and sub-hourly, use standard intervals\r\n         return self._create_time_intervals()\r\n \r\n+    def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create monthly aligned intervals.\"\"\"\r\n+        # Implementation for creating monthly aligned intervals\r\n+        return TimeInterval(min_date=..., max_date=...), TimeInterval(min_date=..., max_date=...), TimeInterval(min_date=..., max_date=...)\r\n+\r\n+    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        test_start, test_end = self.config.test_dates\r\n+    \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create daily aligned intervals.\"\"\"\r\n+        return self._create_monthly_aligned_intervals()  # Same logic for now\r\n+\r\n+    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create standard time intervals.\"\"\"\r\n+        return self._create_monthly_aligned_intervals()  # Same logic for now\r\n+\r\n     def _get_dataset_class(self):\r\n         \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n         if self.config.model_config.model_type.is_transformer:\r\n             if self.config.model_config.model_type.is_resolution_specific:\r\n@@ -167,9 +193,9 @@\n \r\n         # Create evaluator with resolution-aware configuration\r\n         evaluator = Evaluator(\r\n             scaler=self.config.dataset_config.time_series_scaler,\r\n-            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+            forecast_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes  # Fix parameter name\r\n         )\r\n \r\n         # Evaluate predictions\r\n         evaluation = evaluator.evaluate(\r\n@@ -190,5 +216,12 @@\n         )\r\n \r\n         # Save model if path provided\r\n         if self.config.model_save_path:\r\n-            self._save_model(str(self.config.model_save_path))\n\\ No newline at end of file\n+            self._save_model(self.config.model_save_path)\r\n+\r\n+    @property\r\n+    def training_report(self) -> Dict[str, Any]:\r\n+        \"\"\"Get training report.\"\"\"\r\n+        if not hasattr(self, '_training_report'):\r\n+            self._training_report = {}\r\n+        return self._training_report\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733177655154,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,15 +14,17 @@\n from data_loading.datasets.standard_dataset import StandardDataset\r\n from data_loading.datasets.transformer_dataset import TransformerDataset\r\n from data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\n from evaluation.evaluator import Evaluator\r\n+from evaluation.metrics import MetricConfig\r\n from experiments.experiment import Experiment\r\n from models.base.base_wrapper import BaseWrapper\r\n from models.registry.factory import ModelFactory\r\n from models.registry.model_types import ModelType\r\n from models.wrappers.pytorch_wrapper import PyTorchWrapper\r\n from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n from pipeline.base.base_pipeline import BasePipeline\r\n+from training.reports.training_report import TrainingReport\r\n \r\n class TimeSeriesPipeline(BasePipeline):\r\n     \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n \r\n@@ -99,8 +101,10 @@\n         return train_dataset, val_dataset, test_dataset\r\n \r\n     def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n         \"\"\"Resample data to match the target resolution.\"\"\"\r\n+        if self.data_loader is None:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n         rule = f'{target_resolution_minutes}T'\r\n         resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n         return resampled.reset_index()\r\n \r\n@@ -116,27 +120,33 @@\n         return self._create_time_intervals()\r\n \r\n     def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n         \"\"\"Create monthly aligned intervals.\"\"\"\r\n-        # Implementation for creating monthly aligned intervals\r\n-        return TimeInterval(min_date=..., max_date=...), TimeInterval(min_date=..., max_date=...), TimeInterval(min_date=..., max_date=...)\r\n-\r\n-    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n         test_start, test_end = self.config.test_dates\r\n-    \r\n+        \r\n         return (\r\n             TimeInterval(min_date=train_start, max_date=train_end),\r\n             TimeInterval(min_date=val_start, max_date=val_end),\r\n             TimeInterval(min_date=test_start, max_date=test_end)\r\n         )\r\n \r\n     def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n         \"\"\"Create daily aligned intervals.\"\"\"\r\n-        return self._create_monthly_aligned_intervals()  # Same logic for now\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n \r\n     def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n         \"\"\"Create standard time intervals.\"\"\"\r\n-        return self._create_monthly_aligned_intervals()  # Same logic for now\r\n+        return self._create_monthly_aligned_intervals()\r\n \r\n     def _get_dataset_class(self):\r\n         \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n         if self.config.model_config.model_type.is_transformer:\r\n@@ -193,9 +203,10 @@\n \r\n         # Create evaluator with resolution-aware configuration\r\n         evaluator = Evaluator(\r\n             scaler=self.config.dataset_config.time_series_scaler,\r\n-            forecast_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes  # Fix parameter name\r\n+            metric_config=MetricConfig(),  # Add this if needed\r\n+            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n         )\r\n \r\n         # Evaluate predictions\r\n         evaluation = evaluator.evaluate(\r\n@@ -219,9 +230,9 @@\n         if self.config.model_save_path:\r\n             self._save_model(self.config.model_save_path)\r\n \r\n     @property\r\n-    def training_report(self) -> Dict[str, Any]:\r\n+    def training_report(self) -> TrainingReport:  # Change return type\r\n         \"\"\"Get training report.\"\"\"\r\n         if not hasattr(self, '_training_report'):\r\n-            self._training_report = {}\r\n+            self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n         return self._training_report\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733178396124,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,8 +23,9 @@\n from models.wrappers.pytorch_wrapper import PyTorchWrapper\r\n from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n from pipeline.base.base_pipeline import BasePipeline\r\n from training.reports.training_report import TrainingReport\r\n+from torch.utils.data import DataLoader\r\n \r\n class TimeSeriesPipeline(BasePipeline):\r\n     \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n \r\n@@ -234,5 +235,86 @@\n     def training_report(self) -> TrainingReport:  # Change return type\r\n         \"\"\"Get training report.\"\"\"\r\n         if not hasattr(self, '_training_report'):\r\n             self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n-        return self._training_report\n\\ No newline at end of file\n+        return self._training_report\r\n+\r\n+    def train_model(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Tuple[PyTorchWrapper, TrainingReport]:\r\n+        \"\"\"Train model implementation.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n+\r\n+        # Initialize training report\r\n+        training_report = TrainingReport(train_losses=[])\r\n+        \r\n+        # Training loop\r\n+        for epoch in range(self.config.training_config.max_epochs):\r\n+            epoch_loss = 0.0\r\n+            num_batches = 0\r\n+            \r\n+            # Set model to training mode\r\n+            if hasattr(self.model_wrapper.model, 'train'):\r\n+                self.model_wrapper.model.train()\r\n+            \r\n+            # Training loop\r\n+            for batch_input, batch_target in train_loader:\r\n+                loss = self.model_wrapper.training_step(batch_input, batch_target)\r\n+                epoch_loss += loss\r\n+                num_batches += 1\r\n+            \r\n+            avg_epoch_loss = epoch_loss / num_batches\r\n+            training_report.train_losses.append(avg_epoch_loss)\r\n+            \r\n+            # Validation if provided\r\n+            if val_loader:\r\n+                val_loss = self._validate_model(val_loader)\r\n+                if not hasattr(training_report, 'val_losses'):\r\n+                    training_report.val_losses = []\r\n+                training_report.val_losses.append(val_loss)\r\n+                \r\n+                if self.config.training_config.use_early_stopping:\r\n+                    if self._should_stop_early(training_report):\r\n+                        break\r\n+            \r\n+            # Log progress\r\n+            if (epoch + 1) % 10 == 0:\r\n+                print(f\"Epoch {epoch + 1}/{self.config.training_config.max_epochs}, \"\r\n+                      f\"Loss: {avg_epoch_loss:.4f}\")\r\n+        \r\n+        return self.model_wrapper, training_report\r\n+\r\n+    def _validate_model(self, val_loader: DataLoader) -> float:\r\n+        \"\"\"Perform validation.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+            \r\n+        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            # PyTorch specific validation\r\n+            self.model_wrapper.model.eval()\r\n+        \r\n+        total_loss = 0.0\r\n+        num_batches = 0\r\n+        \r\n+        with torch.no_grad():\r\n+            for batch_input, batch_target in val_loader:\r\n+                loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                total_loss += loss\r\n+                num_batches += 1\r\n+        \r\n+        # Set back to training mode if it's a PyTorch model\r\n+        if hasattr(self.model_wrapper.model, 'train'):\r\n+            self.model_wrapper.model.train()\r\n+            \r\n+        return total_loss / num_batches if num_batches > 0 else float('inf')\r\n+\r\n+    def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n+        \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n+        if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n+            return False\r\n+            \r\n+        patience = self.config.training_config.early_stopping_patience\r\n+        recent_losses = training_report.val_losses[-patience:]\r\n+        min_loss_idx = recent_losses.index(min(recent_losses))\r\n+        \r\n+        return min_loss_idx == 0  # Stop if best loss was patience steps ago\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733178435867,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -290,25 +290,40 @@\n             raise ValueError(\"Model wrapper not initialized\")\r\n             \r\n         if isinstance(self.model_wrapper, PyTorchWrapper):\r\n             # PyTorch specific validation\r\n-            self.model_wrapper.model.eval()\r\n-        \r\n-        total_loss = 0.0\r\n-        num_batches = 0\r\n-        \r\n-        with torch.no_grad():\r\n-            for batch_input, batch_target in val_loader:\r\n-                loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n-                total_loss += loss\r\n-                num_batches += 1\r\n-        \r\n-        # Set back to training mode if it's a PyTorch model\r\n-        if hasattr(self.model_wrapper.model, 'train'):\r\n-            self.model_wrapper.model.train()\r\n+            if hasattr(self.model_wrapper.model, 'eval'):\r\n+                self.model_wrapper.model.eval()\r\n             \r\n-        return total_loss / num_batches if num_batches > 0 else float('inf')\r\n+            total_loss = 0.0\r\n+            num_batches = 0\r\n+            \r\n+            with torch.no_grad():\r\n+                for batch_input, batch_target in val_loader:\r\n+                    if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                        loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                        total_loss += loss\r\n+                        num_batches += 1\r\n+            \r\n+            # Set back to training mode if PyTorch model\r\n+            if hasattr(self.model_wrapper.model, 'train'):\r\n+                self.model_wrapper.model.train()\r\n+                \r\n+            return total_loss / num_batches if num_batches > 0 else float('inf')\r\n+        else:\r\n+            # For sklearn models, compute validation loss directly\r\n+            X_val, y_val = self._prepare_validation_data(val_loader)\r\n+            y_pred = self.model_wrapper.model.predict(X_val)\r\n+            return float(np.mean((y_pred - y_val) ** 2))  # MSE loss\r\n \r\n+    def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n+        \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n+        X_list, y_list = [], []\r\n+        for batch_input, batch_target in val_loader:\r\n+            X_list.append(batch_input.numpy())\r\n+            y_list.append(batch_target.numpy())\r\n+        return np.vstack(X_list), np.vstack(y_list)\r\n+\r\n     def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n         \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n         if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n             return False\r\n"
                },
                {
                    "date": 1733181072650,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -244,10 +244,16 @@\n             raise ValueError(\"Model wrapper not initialized\")\r\n         if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n             raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n \r\n-        # Initialize training report\r\n-        training_report = TrainingReport(train_losses=[])\r\n+        # Initialize training report with additional_metrics\r\n+        training_report = TrainingReport(\r\n+            train_losses=[],\r\n+            val_losses=[],\r\n+            learning_rates=[],\r\n+            epochs=0,\r\n+            additional_metrics={}\r\n+        )\r\n         \r\n         # Training loop\r\n         for epoch in range(self.config.training_config.max_epochs):\r\n             epoch_loss = 0.0\r\n@@ -255,36 +261,50 @@\n             \r\n             # Set model to training mode\r\n             if hasattr(self.model_wrapper.model, 'train'):\r\n                 self.model_wrapper.model.train()\r\n-            \r\n-            # Training loop\r\n+                \r\n+            # Process each batch\r\n             for batch_input, batch_target in train_loader:\r\n-                loss = self.model_wrapper.training_step(batch_input, batch_target)\r\n+                loss = self._process_training_batch(batch_input, batch_target)\r\n                 epoch_loss += loss\r\n                 num_batches += 1\r\n-            \r\n+                \r\n+            # Process epoch results\r\n             avg_epoch_loss = epoch_loss / num_batches\r\n             training_report.train_losses.append(avg_epoch_loss)\r\n             \r\n-            # Validation if provided\r\n+            # Validation phase\r\n             if val_loader:\r\n-                val_loss = self._validate_model(val_loader)\r\n-                if not hasattr(training_report, 'val_losses'):\r\n-                    training_report.val_losses = []\r\n-                training_report.val_losses.append(val_loss)\r\n-                \r\n-                if self.config.training_config.use_early_stopping:\r\n-                    if self._should_stop_early(training_report):\r\n-                        break\r\n+                self._handle_validation(val_loader, training_report)\r\n+                if self.config.training_config.use_early_stopping and self._should_stop_early(training_report):\r\n+                    break\r\n             \r\n             # Log progress\r\n             if (epoch + 1) % 10 == 0:\r\n                 print(f\"Epoch {epoch + 1}/{self.config.training_config.max_epochs}, \"\r\n                       f\"Loss: {avg_epoch_loss:.4f}\")\r\n         \r\n         return self.model_wrapper, training_report\r\n \r\n+    def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+                              batch_target: torch.Tensor) -> float:\r\n+        \"\"\"Process a single training batch.\"\"\"\r\n+        if self.config.model_config.model_type.is_transformer:\r\n+            src = batch_input\r\n+            tgt = batch_target[:, :-1]\r\n+            tgt_y = batch_target[:, 1:]\r\n+            return self.model_wrapper.training_step((src, tgt), tgt_y)\r\n+        \r\n+        return self.model_wrapper.training_step(batch_input, batch_target)\r\n+\r\n+    def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n+        \"\"\"Handle validation phase of training.\"\"\"\r\n+        val_loss = self._validate_model(val_loader)\r\n+        if not hasattr(training_report, 'val_losses'):\r\n+            training_report.val_losses = []\r\n+        training_report.val_losses.append(val_loss)\r\n+\r\n     def _validate_model(self, val_loader: DataLoader) -> float:\r\n         \"\"\"Perform validation.\"\"\"\r\n         if not self.model_wrapper:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n@@ -331,5 +351,23 @@\n         patience = self.config.training_config.early_stopping_patience\r\n         recent_losses = training_report.val_losses[-patience:]\r\n         min_loss_idx = recent_losses.index(min(recent_losses))\r\n         \r\n-        return min_loss_idx == 0  # Stop if best loss was patience steps ago\n\\ No newline at end of file\n+        return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n+\r\n+    def training_step(self, batch_input, batch_target):\r\n+        \"\"\"Modified training step to handle transformer input format\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        # Modify the input format for transformer models\r\n+        if self.config.model_config.model_type.is_transformer:\r\n+            # For transformer models, we need both source and target sequences\r\n+            src = batch_input\r\n+            tgt = batch_target[:, :-1]  # Use all but last time step as target input\r\n+            tgt_y = batch_target[:, 1:]  # Use all but first time step as target output\r\n+            loss = self.model_wrapper.training_step((src, tgt), tgt_y)\r\n+        else:\r\n+            # For non-transformer models, use normal format\r\n+            loss = self.model_wrapper.training_step(batch_input, batch_target)\r\n+        \r\n+        return loss\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733181170283,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -244,15 +244,15 @@\n             raise ValueError(\"Model wrapper not initialized\")\r\n         if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n             raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n \r\n-        # Initialize training report with additional_metrics\r\n+        # Initialize training report with metrics\r\n         training_report = TrainingReport(\r\n             train_losses=[],\r\n             val_losses=[],\r\n             learning_rates=[],\r\n             epochs=0,\r\n-            additional_metrics={}\r\n+            metrics={}\r\n         )\r\n         \r\n         # Training loop\r\n         for epoch in range(self.config.training_config.max_epochs):\r\n@@ -288,14 +288,19 @@\n \r\n     def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n                               batch_target: torch.Tensor) -> float:\r\n         \"\"\"Process a single training batch.\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+            \r\n         if self.config.model_config.model_type.is_transformer:\r\n-            src = batch_input\r\n+            src = batch_input if isinstance(batch_input, torch.Tensor) else batch_input[0]\r\n             tgt = batch_target[:, :-1]\r\n             tgt_y = batch_target[:, 1:]\r\n             return self.model_wrapper.training_step((src, tgt), tgt_y)\r\n         \r\n+        if isinstance(batch_input, tuple):\r\n+            batch_input = batch_input[0]  # Take first tensor if tuple\r\n         return self.model_wrapper.training_step(batch_input, batch_target)\r\n \r\n     def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n         \"\"\"Handle validation phase of training.\"\"\"\r\n@@ -353,9 +358,9 @@\n         min_loss_idx = recent_losses.index(min(recent_losses))\r\n         \r\n         return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n \r\n-    def training_step(self, batch_input, batch_target):\r\n+    def training_step(self, batch_input: torch.Tensor, batch_target: torch.Tensor) -> float:\r\n         \"\"\"Modified training step to handle transformer input format\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n         \r\n@@ -364,10 +369,14 @@\n             # For transformer models, we need both source and target sequences\r\n             src = batch_input\r\n             tgt = batch_target[:, :-1]  # Use all but last time step as target input\r\n             tgt_y = batch_target[:, 1:]  # Use all but first time step as target output\r\n+            if not isinstance(self.model_wrapper, (PyTorchWrapper, SklearnWrapper)):\r\n+                raise TypeError(\"Invalid model wrapper type\")\r\n             loss = self.model_wrapper.training_step((src, tgt), tgt_y)\r\n         else:\r\n             # For non-transformer models, use normal format\r\n+            if not isinstance(self.model_wrapper, (PyTorchWrapper, SklearnWrapper)):\r\n+                raise TypeError(\"Invalid model wrapper type\")\r\n             loss = self.model_wrapper.training_step(batch_input, batch_target)\r\n         \r\n         return loss\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733181354238,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -250,9 +250,9 @@\n             train_losses=[],\r\n             val_losses=[],\r\n             learning_rates=[],\r\n             epochs=0,\r\n-            metrics={}\r\n+            additional_metrics={}  # Use additional_metrics instead of metrics\r\n         )\r\n         \r\n         # Training loop\r\n         for epoch in range(self.config.training_config.max_epochs):\r\n@@ -285,23 +285,26 @@\n                       f\"Loss: {avg_epoch_loss:.4f}\")\r\n         \r\n         return self.model_wrapper, training_report\r\n \r\n-    def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n-                              batch_target: torch.Tensor) -> float:\r\n+    def _process_training_batch(\r\n+        self, \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_target: torch.Tensor\r\n+    ) -> float:\r\n         \"\"\"Process a single training batch.\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n             \r\n-        if self.config.model_config.model_type.is_transformer:\r\n-            src = batch_input if isinstance(batch_input, torch.Tensor) else batch_input[0]\r\n-            tgt = batch_target[:, :-1]\r\n-            tgt_y = batch_target[:, 1:]\r\n-            return self.model_wrapper.training_step((src, tgt), tgt_y)\r\n-        \r\n         if isinstance(batch_input, tuple):\r\n+            if self.config.model_config.model_type.is_transformer:\r\n+                src = batch_input[0]\r\n+                tgt = batch_target[:, :-1]\r\n+                tgt_y = batch_target[:, 1:]\r\n+                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n             batch_input = batch_input[0]  # Take first tensor if tuple\r\n-        return self.model_wrapper.training_step(batch_input, batch_target)\r\n+        \r\n+        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\r\n \r\n     def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n         \"\"\"Handle validation phase of training.\"\"\"\r\n         val_loss = self._validate_model(val_loader)\r\n@@ -358,25 +361,22 @@\n         min_loss_idx = recent_losses.index(min(recent_losses))\r\n         \r\n         return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n \r\n-    def training_step(self, batch_input: torch.Tensor, batch_target: torch.Tensor) -> float:\r\n+    def training_step(\r\n+        self, \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_target: torch.Tensor\r\n+    ) -> float:\r\n         \"\"\"Modified training step to handle transformer input format\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n         \r\n-        # Modify the input format for transformer models\r\n-        if self.config.model_config.model_type.is_transformer:\r\n-            # For transformer models, we need both source and target sequences\r\n-            src = batch_input\r\n-            tgt = batch_target[:, :-1]  # Use all but last time step as target input\r\n-            tgt_y = batch_target[:, 1:]  # Use all but first time step as target output\r\n-            if not isinstance(self.model_wrapper, (PyTorchWrapper, SklearnWrapper)):\r\n-                raise TypeError(\"Invalid model wrapper type\")\r\n-            loss = self.model_wrapper.training_step((src, tgt), tgt_y)\r\n-        else:\r\n-            # For non-transformer models, use normal format\r\n\\ No newline at end of file\n-            if not isinstance(self.model_wrapper, (PyTorchWrapper, SklearnWrapper)):\r\n-                raise TypeError(\"Invalid model wrapper type\")\r\n-            loss = self.model_wrapper.training_step(batch_input, batch_target)\r\n-        \r\n-        return loss\n+        if isinstance(batch_input, tuple):\r\n+            if self.config.model_config.model_type.is_transformer:\r\n+                src = batch_input[0]\r\n+                tgt = batch_target[:, :-1]\r\n+                tgt_y = batch_target[:, 1:]\r\n+                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n+            batch_input = batch_input[0]\r\n+            \r\n+        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733181782997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -244,16 +244,18 @@\n             raise ValueError(\"Model wrapper not initialized\")\r\n         if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n             raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n \r\n-        # Initialize training report with metrics\r\n+        # Initialize training report\r\n         training_report = TrainingReport(\r\n             train_losses=[],\r\n             val_losses=[],\r\n             learning_rates=[],\r\n             epochs=0,\r\n-            additional_metrics={}  # Use additional_metrics instead of metrics\r\n+            additional_metrics={}\r\n         )\r\n+\r\n+        is_transformer = self.config.model_config.model_type.is_transformer\r\n         \r\n         # Training loop\r\n         for epoch in range(self.config.training_config.max_epochs):\r\n             epoch_loss = 0.0\r\n@@ -264,9 +266,16 @@\n                 self.model_wrapper.model.train()\r\n                 \r\n             # Process each batch\r\n             for batch_input, batch_target in train_loader:\r\n-                loss = self._process_training_batch(batch_input, batch_target)\r\n+                if is_transformer:\r\n+                    # For transformer models, construct input tuple\r\n+                    tgt_input = batch_target[:, :-1]  # Use all but last step as target input\r\n+                    tgt_output = batch_target[:, 1:]  # Use all but first step as target output\r\n+                    loss = self._process_training_batch((batch_input, tgt_input), tgt_output)\r\n+                else:\r\n+                    loss = self._process_training_batch(batch_input, batch_target)\r\n+                    \r\n                 epoch_loss += loss\r\n                 num_batches += 1\r\n                 \r\n             # Process epoch results\r\n@@ -287,23 +296,23 @@\n         return self.model_wrapper, training_report\r\n \r\n     def _process_training_batch(\r\n         self, \r\n-        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n         batch_target: torch.Tensor\r\n     ) -> float:\r\n         \"\"\"Process a single training batch.\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n             \r\n+        if self.config.model_config.model_type.is_transformer:\r\n+            if not isinstance(batch_input, tuple):\r\n+                raise ValueError(\"Transformer models expect a tuple of (src, tgt) sequences\")\r\n+            src, tgt = batch_input\r\n+            return cast(float, self.model_wrapper.training_step((src, tgt), batch_target))\r\n+        \r\n         if isinstance(batch_input, tuple):\r\n-            if self.config.model_config.model_type.is_transformer:\r\n-                src = batch_input[0]\r\n-                tgt = batch_target[:, :-1]\r\n-                tgt_y = batch_target[:, 1:]\r\n-                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n             batch_input = batch_input[0]  # Take first tensor if tuple\r\n-        \r\n         return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\r\n \r\n     def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n         \"\"\"Handle validation phase of training.\"\"\"\r\n@@ -323,26 +332,32 @@\n                 self.model_wrapper.model.eval()\r\n             \r\n             total_loss = 0.0\r\n             num_batches = 0\r\n+            is_transformer = self.config.model_config.model_type.is_transformer\r\n             \r\n             with torch.no_grad():\r\n                 for batch_input, batch_target in val_loader:\r\n-                    if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                    if is_transformer:\r\n+                        src = batch_input\r\n+                        tgt = batch_target[:, :-1]\r\n+                        tgt_y = batch_target[:, 1:]\r\n+                        loss = self.model_wrapper.validation_step((src, tgt), tgt_y)\r\n+                    else:\r\n                         loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n-                        total_loss += loss\r\n-                        num_batches += 1\r\n+                    total_loss += loss\r\n+                    num_batches += 1\r\n             \r\n             # Set back to training mode if PyTorch model\r\n             if hasattr(self.model_wrapper.model, 'train'):\r\n                 self.model_wrapper.model.train()\r\n                 \r\n             return total_loss / num_batches if num_batches > 0 else float('inf')\r\n-        else:\r\n-            # For sklearn models, compute validation loss directly\r\n-            X_val, y_val = self._prepare_validation_data(val_loader)\r\n-            y_pred = self.model_wrapper.model.predict(X_val)\r\n-            return float(np.mean((y_pred - y_val) ** 2))  # MSE loss\r\n+        \r\n+        # For sklearn models, compute validation loss directly\r\n+        X_val, y_val = self._prepare_validation_data(val_loader)\r\n+        y_pred = self.model_wrapper.model.predict(X_val)\r\n+        return float(np.mean((y_pred - y_val) ** 2))  # MSE loss\r\n \r\n     def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n         \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n         X_list, y_list = [], []\r\n@@ -363,19 +378,18 @@\n         return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n \r\n     def training_step(\r\n         self, \r\n-        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n         batch_target: torch.Tensor\r\n     ) -> float:\r\n         \"\"\"Modified training step to handle transformer input format\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n         \r\n         if isinstance(batch_input, tuple):\r\n             if self.config.model_config.model_type.is_transformer:\r\n-                src = batch_input[0]\r\n-                tgt = batch_target[:, :-1]\r\n+                src, tgt = batch_input\r\n                 tgt_y = batch_target[:, 1:]\r\n                 return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n             batch_input = batch_input[0]\r\n             \r\n"
                },
                {
                    "date": 1733181846186,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -266,17 +266,22 @@\n                 self.model_wrapper.model.train()\r\n                 \r\n             # Process each batch\r\n             for batch_input, batch_target in train_loader:\r\n+                # Handle transformer sequence data\r\n                 if is_transformer:\r\n-                    # For transformer models, construct input tuple\r\n-                    tgt_input = batch_target[:, :-1]  # Use all but last step as target input\r\n-                    tgt_output = batch_target[:, 1:]  # Use all but first step as target output\r\n-                    loss = self._process_training_batch((batch_input, tgt_input), tgt_output)\r\n+                    src = batch_input\r\n+                    tgt = batch_target[:, :-1]  # Input sequence\r\n+                    tgt_y = batch_target[:, 1:]  # Target sequence\r\n+                    # Forward the data directly to the model wrapper\r\n+                    if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                        loss = self.model_wrapper.training_step((src, tgt), tgt_y)\r\n+                    else:\r\n+                        loss = self._process_training_batch(batch_input, batch_target)\r\n                 else:\r\n                     loss = self._process_training_batch(batch_input, batch_target)\r\n                     \r\n-                epoch_loss += loss\r\n+                epoch_loss += cast(float, loss)\r\n                 num_batches += 1\r\n                 \r\n             # Process epoch results\r\n             avg_epoch_loss = epoch_loss / num_batches\r\n@@ -296,23 +301,15 @@\n         return self.model_wrapper, training_report\r\n \r\n     def _process_training_batch(\r\n         self, \r\n-        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n+        batch_input: torch.Tensor,  # Simplified type hint\r\n         batch_target: torch.Tensor\r\n     ) -> float:\r\n-        \"\"\"Process a single training batch.\"\"\"\r\n+        \"\"\"Process a single training batch for non-transformer models.\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n-            \r\n-        if self.config.model_config.model_type.is_transformer:\r\n-            if not isinstance(batch_input, tuple):\r\n-                raise ValueError(\"Transformer models expect a tuple of (src, tgt) sequences\")\r\n-            src, tgt = batch_input\r\n-            return cast(float, self.model_wrapper.training_step((src, tgt), batch_target))\r\n         \r\n-        if isinstance(batch_input, tuple):\r\n-            batch_input = batch_input[0]  # Take first tensor if tuple\r\n         return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\r\n \r\n     def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n         \"\"\"Handle validation phase of training.\"\"\"\r\n@@ -336,16 +333,18 @@\n             is_transformer = self.config.model_config.model_type.is_transformer\r\n             \r\n             with torch.no_grad():\r\n                 for batch_input, batch_target in val_loader:\r\n+                    # Handle transformer sequence data\r\n                     if is_transformer:\r\n                         src = batch_input\r\n                         tgt = batch_target[:, :-1]\r\n                         tgt_y = batch_target[:, 1:]\r\n                         loss = self.model_wrapper.validation_step((src, tgt), tgt_y)\r\n                     else:\r\n                         loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n-                    total_loss += loss\r\n+                        \r\n+                    total_loss += cast(float, loss)\r\n                     num_batches += 1\r\n             \r\n             # Set back to training mode if PyTorch model\r\n             if hasattr(self.model_wrapper.model, 'train'):\r\n"
                },
                {
                    "date": 1733181855513,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,382 @@\n+# pipeline/implementations/time_series_pipeline.py\r\n+from __future__ import annotations\r\n+import time\r\n+from typing import Tuple, Optional, Dict, Any, cast, Union\r\n+from pathlib import Path\r\n+import torch\r\n+import dataclasses\r\n+from argparse import Namespace\r\n+import numpy as np\r\n+from numpy.typing import NDArray\r\n+import pandas as pd  # Add this import statement\r\n+from datetime import date  # Add this import statement\r\n+\r\n+from data_loading.datasets.standard_dataset import StandardDataset\r\n+from data_loading.datasets.transformer_dataset import TransformerDataset\r\n+from data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\n+from evaluation.evaluator import Evaluator\r\n+from evaluation.metrics import MetricConfig\r\n+from experiments.experiment import Experiment\r\n+from models.base.base_wrapper import BaseWrapper\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.wrappers.pytorch_wrapper import PyTorchWrapper\r\n+from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n+from pipeline.base.base_pipeline import BasePipeline\r\n+from training.reports.training_report import TrainingReport\r\n+from torch.utils.data import DataLoader\r\n+\r\n+class TimeSeriesPipeline(BasePipeline):\r\n+    \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n+\r\n+    def __init__(self, *args, **kwargs):\r\n+        super().__init__(*args, **kwargs)\r\n+        self.data_loader: Optional[TimeSeriesLoader] = None\r\n+        self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n+        \r\n+        # Validate resolutions during initialization\r\n+        self._validate_resolutions()\r\n+\r\n+    def _validate_resolutions(self) -> None:\r\n+        \"\"\"Validate that resolutions are consistent across configurations.\"\"\"\r\n+        input_res = self.config.dataset_config.input_resolution_minutes\r\n+        forecast_res = self.config.dataset_config.forecast_resolution_minutes\r\n+        \r\n+        # Check model type matches resolution\r\n+        if self.config.model_config.model_type.is_resolution_specific:\r\n+            expected_type = ModelType.get_for_resolution(forecast_res)\r\n+            if self.config.model_config.model_type != expected_type:\r\n+                raise ValueError(\r\n+                    f\"Model type {self.config.model_config.model_type} does not match \"\r\n+                    f\"forecast resolution {forecast_res} minutes. Expected {expected_type}\"\r\n+                )\r\n+\r\n+    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n+        \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n+        self.data_loader = TimeSeriesLoader(\r\n+            time_variable=self.config.dataset_config.time_variable,\r\n+            target_variable=self.config.dataset_config.target_variable\r\n+        )\r\n+        return self.data_loader\r\n+\r\n+    def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n+        \"\"\"Prepare train, validation and test datasets with resolution handling.\"\"\"\r\n+        if not self.data_loader:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n+\r\n+        # Load and potentially resample data\r\n+        df = self.data_loader.load(self.config.data_path)\r\n+        \r\n+        # Resample if input and forecast resolutions differ\r\n+        if self.config.dataset_config.needs_resampling():\r\n+            df = self._resample_data(\r\n+                df,\r\n+                target_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+            )\r\n+\r\n+        # Get time intervals adjusted for resolution\r\n+        train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n+\r\n+        # Split data\r\n+        train_df, val_df, test_df = self.data_loader.split(\r\n+            df,\r\n+            train_interval,\r\n+            val_interval,\r\n+            test_interval\r\n+        )\r\n+\r\n+        # Select appropriate dataset class\r\n+        dataset_cls = self._get_dataset_class()\r\n+\r\n+        # Create datasets\r\n+        train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n+        val_dataset = dataset_cls(\r\n+            val_df,\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n+        )\r\n+        test_dataset = dataset_cls(\r\n+            test_df,\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n+        )\r\n+\r\n+        return train_dataset, val_dataset, test_dataset\r\n+\r\n+    def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n+        \"\"\"Resample data to match the target resolution.\"\"\"\r\n+        if self.data_loader is None:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n+        rule = f'{target_resolution_minutes}T'\r\n+        resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n+        return resampled.reset_index()\r\n+\r\n+    def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n+            return self._create_monthly_aligned_intervals()\r\n+        \r\n+        # For daily predictions, ensure intervals align with day boundaries\r\n+        elif self.config.dataset_config.forecast_resolution_minutes >= 1440:  # Daily\r\n+            return self._create_daily_aligned_intervals()\r\n+            \r\n+        # For hourly and sub-hourly, use standard intervals\r\n+        return self._create_time_intervals()\r\n+\r\n+    def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create monthly aligned intervals.\"\"\"\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create daily aligned intervals.\"\"\"\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create standard time intervals.\"\"\"\r\n+        return self._create_monthly_aligned_intervals()\r\n+\r\n+    def _get_dataset_class(self):\r\n+        \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n+        if self.config.model_config.model_type.is_transformer:\r\n+            if self.config.model_config.model_type.is_resolution_specific:\r\n+                # Use TransformerDataset with resolution-specific optimizations\r\n+                return TransformerDataset\r\n+            return TransformerDataset\r\n+        return StandardDataset\r\n+\r\n+    def setup_model(self) -> None:\r\n+        \"\"\"Setup model and associated wrapper with resolution awareness.\"\"\"\r\n+        # Convert config to dict for factory\r\n+        model_config_dict = dataclasses.asdict(self.config.model_config)\r\n+\r\n+        # Create model using factory\r\n+        model = ModelFactory.create_base_model(\r\n+            self.config.model_config.model_type,\r\n+            model_config_dict\r\n+        )\r\n+\r\n+        # Convert training config to dict\r\n+        training_config_dict = dataclasses.asdict(self.config.training_config)\r\n+\r\n+        # Create appropriate wrapper\r\n+        if self.config.model_config.model_type == ModelType.LINEAR_REGRESSION:\r\n+            wrapper: Union[SklearnWrapper, PyTorchWrapper] = SklearnWrapper(\r\n+                cast(Any, model),\r\n+                self.config.model_config.model_type,\r\n+                training_config_dict\r\n+            )\r\n+        else:\r\n+            wrapper = PyTorchWrapper(\r\n+                model,\r\n+                self.config.model_config.model_type,\r\n+                training_config_dict\r\n+            )\r\n+        \r\n+        self.model_wrapper = wrapper\r\n+\r\n+    def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n+        \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+\r\n+        # Make predictions\r\n+        test_start = time.time()\r\n+        predictions, targets = self.model_wrapper.predict(test_dataset)\r\n+        self.test_time = time.time() - test_start\r\n+\r\n+        # Get timestamps\r\n+        timestamps = test_dataset.time_labels\r\n+        if timestamps is None:\r\n+            timestamps = np.arange(len(predictions))\r\n+\r\n+        # Create evaluator with resolution-aware configuration\r\n+        evaluator = Evaluator(\r\n+            scaler=self.config.dataset_config.time_series_scaler,\r\n+            metric_config=MetricConfig(),  # Add this if needed\r\n+            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+        )\r\n+\r\n+        # Evaluate predictions\r\n+        evaluation = evaluator.evaluate(\r\n+            predictions=predictions,\r\n+            targets=targets,\r\n+            timestamps=cast(NDArray, timestamps),\r\n+            num_variables=1\r\n+        )\r\n+\r\n+        # Create experiment\r\n+        self.experiment = Experiment(\r\n+            model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n+            evaluation=evaluation,\r\n+            training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n+            training_report=self.training_report,\r\n+            training_time=self.training_time,\r\n+            test_time=self.test_time\r\n+        )\r\n+\r\n+        # Save model if path provided\r\n+        if self.config.model_save_path:\r\n+            self._save_model(self.config.model_save_path)\r\n+\r\n+    @property\r\n+    def training_report(self) -> TrainingReport:  # Change return type\r\n+        \"\"\"Get training report.\"\"\"\r\n+        if not hasattr(self, '_training_report'):\r\n+            self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n+        return self._training_report\r\n+\r\n+    def train_model(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Tuple[PyTorchWrapper, TrainingReport]:\r\n+        \"\"\"Train model implementation.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n+\r\n+        # Initialize training report with metrics\r\n+        training_report = TrainingReport(\r\n+            train_losses=[],\r\n+            val_losses=[],\r\n+            learning_rates=[],\r\n+            epochs=0,\r\n+            additional_metrics={}  # Use additional_metrics instead of metrics\r\n+        )\r\n+        \r\n+        # Training loop\r\n+        for epoch in range(self.config.training_config.max_epochs):\r\n+            epoch_loss = 0.0\r\n+            num_batches = 0\r\n+            \r\n+            # Set model to training mode\r\n+            if hasattr(self.model_wrapper.model, 'train'):\r\n+                self.model_wrapper.model.train()\r\n+                \r\n+            # Process each batch\r\n+            for batch_input, batch_target in train_loader:\r\n+                loss = self._process_training_batch(batch_input, batch_target)\r\n+                epoch_loss += loss\r\n+                num_batches += 1\r\n+                \r\n+            # Process epoch results\r\n+            avg_epoch_loss = epoch_loss / num_batches\r\n+            training_report.train_losses.append(avg_epoch_loss)\r\n+            \r\n+            # Validation phase\r\n+            if val_loader:\r\n+                self._handle_validation(val_loader, training_report)\r\n+                if self.config.training_config.use_early_stopping and self._should_stop_early(training_report):\r\n+                    break\r\n+            \r\n+            # Log progress\r\n+            if (epoch + 1) % 10 == 0:\r\n+                print(f\"Epoch {epoch + 1}/{self.config.training_config.max_epochs}, \"\r\n+                      f\"Loss: {avg_epoch_loss:.4f}\")\r\n+        \r\n+        return self.model_wrapper, training_report\r\n+\r\n+    def _process_training_batch(\r\n+        self, \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_target: torch.Tensor\r\n+    ) -> float:\r\n+        \"\"\"Process a single training batch.\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+            \r\n+        if isinstance(batch_input, tuple):\r\n+            if self.config.model_config.model_type.is_transformer:\r\n+                src = batch_input[0]\r\n+                tgt = batch_target[:, :-1]\r\n+                tgt_y = batch_target[:, 1:]\r\n+                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n+            batch_input = batch_input[0]  # Take first tensor if tuple\r\n+        \r\n+        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\r\n+\r\n+    def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n+        \"\"\"Handle validation phase of training.\"\"\"\r\n+        val_loss = self._validate_model(val_loader)\r\n+        if not hasattr(training_report, 'val_losses'):\r\n+            training_report.val_losses = []\r\n+        training_report.val_losses.append(val_loss)\r\n+\r\n+    def _validate_model(self, val_loader: DataLoader) -> float:\r\n+        \"\"\"Perform validation.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+            \r\n+        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            # PyTorch specific validation\r\n+            if hasattr(self.model_wrapper.model, 'eval'):\r\n+                self.model_wrapper.model.eval()\r\n+            \r\n+            total_loss = 0.0\r\n+            num_batches = 0\r\n+            \r\n+            with torch.no_grad():\r\n+                for batch_input, batch_target in val_loader:\r\n+                    if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                        loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                        total_loss += loss\r\n+                        num_batches += 1\r\n+            \r\n+            # Set back to training mode if PyTorch model\r\n+            if hasattr(self.model_wrapper.model, 'train'):\r\n+                self.model_wrapper.model.train()\r\n+                \r\n+            return total_loss / num_batches if num_batches > 0 else float('inf')\r\n+        else:\r\n+            # For sklearn models, compute validation loss directly\r\n+            X_val, y_val = self._prepare_validation_data(val_loader)\r\n+            y_pred = self.model_wrapper.model.predict(X_val)\r\n+            return float(np.mean((y_pred - y_val) ** 2))  # MSE loss\r\n+\r\n+    def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n+        \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n+        X_list, y_list = [], []\r\n+        for batch_input, batch_target in val_loader:\r\n+            X_list.append(batch_input.numpy())\r\n+            y_list.append(batch_target.numpy())\r\n+        return np.vstack(X_list), np.vstack(y_list)\r\n+\r\n+    def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n+        \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n+        if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n+            return False\r\n+            \r\n+        patience = self.config.training_config.early_stopping_patience\r\n+        recent_losses = training_report.val_losses[-patience:]\r\n+        min_loss_idx = recent_losses.index(min(recent_losses))\r\n+        \r\n+        return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n+\r\n+    def training_step(\r\n+        self, \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_target: torch.Tensor\r\n+    ) -> float:\r\n+        \"\"\"Modified training step to handle transformer input format\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        if isinstance(batch_input, tuple):\r\n+            if self.config.model_config.model_type.is_transformer:\r\n+                src = batch_input[0]\r\n+                tgt = batch_target[:, :-1]\r\n+                tgt_y = batch_target[:, 1:]\r\n+                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n+            batch_input = batch_input[0]\r\n+            \r\n+        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733182140791,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -378,400 +378,5 @@\n                 tgt_y = batch_target[:, 1:]\r\n                 return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n             batch_input = batch_input[0]\r\n             \r\n-        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n-# pipeline/implementations/time_series_pipeline.py\r\n-from __future__ import annotations\r\n-import time\r\n-from typing import Tuple, Optional, Dict, Any, cast, Union\r\n-from pathlib import Path\r\n-import torch\r\n-import dataclasses\r\n-from argparse import Namespace\r\n-import numpy as np\r\n-from numpy.typing import NDArray\r\n-import pandas as pd  # Add this import statement\r\n-from datetime import date  # Add this import statement\r\n-\r\n-from data_loading.datasets.standard_dataset import StandardDataset\r\n-from data_loading.datasets.transformer_dataset import TransformerDataset\r\n-from data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\n-from evaluation.evaluator import Evaluator\r\n-from evaluation.metrics import MetricConfig\r\n-from experiments.experiment import Experiment\r\n-from models.base.base_wrapper import BaseWrapper\r\n-from models.registry.factory import ModelFactory\r\n-from models.registry.model_types import ModelType\r\n-from models.wrappers.pytorch_wrapper import PyTorchWrapper\r\n-from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n-from pipeline.base.base_pipeline import BasePipeline\r\n-from training.reports.training_report import TrainingReport\r\n-from torch.utils.data import DataLoader\r\n-\r\n-class TimeSeriesPipeline(BasePipeline):\r\n-    \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n-\r\n-    def __init__(self, *args, **kwargs):\r\n-        super().__init__(*args, **kwargs)\r\n-        self.data_loader: Optional[TimeSeriesLoader] = None\r\n-        self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n-        \r\n-        # Validate resolutions during initialization\r\n-        self._validate_resolutions()\r\n-\r\n-    def _validate_resolutions(self) -> None:\r\n-        \"\"\"Validate that resolutions are consistent across configurations.\"\"\"\r\n-        input_res = self.config.dataset_config.input_resolution_minutes\r\n-        forecast_res = self.config.dataset_config.forecast_resolution_minutes\r\n-        \r\n-        # Check model type matches resolution\r\n-        if self.config.model_config.model_type.is_resolution_specific:\r\n-            expected_type = ModelType.get_for_resolution(forecast_res)\r\n-            if self.config.model_config.model_type != expected_type:\r\n-                raise ValueError(\r\n-                    f\"Model type {self.config.model_config.model_type} does not match \"\r\n-                    f\"forecast resolution {forecast_res} minutes. Expected {expected_type}\"\r\n-                )\r\n-\r\n-    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n-        \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n-        self.data_loader = TimeSeriesLoader(\r\n-            time_variable=self.config.dataset_config.time_variable,\r\n-            target_variable=self.config.dataset_config.target_variable\r\n-        )\r\n-        return self.data_loader\r\n-\r\n-    def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n-        \"\"\"Prepare train, validation and test datasets with resolution handling.\"\"\"\r\n-        if not self.data_loader:\r\n-            raise ValueError(\"Data loader not initialized\")\r\n-\r\n-        # Load and potentially resample data\r\n-        df = self.data_loader.load(self.config.data_path)\r\n-        \r\n-        # Resample if input and forecast resolutions differ\r\n-        if self.config.dataset_config.needs_resampling():\r\n-            df = self._resample_data(\r\n-                df,\r\n-                target_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n-            )\r\n-\r\n-        # Get time intervals adjusted for resolution\r\n-        train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n-\r\n-        # Split data\r\n-        train_df, val_df, test_df = self.data_loader.split(\r\n-            df,\r\n-            train_interval,\r\n-            val_interval,\r\n-            test_interval\r\n-        )\r\n-\r\n-        # Select appropriate dataset class\r\n-        dataset_cls = self._get_dataset_class()\r\n-\r\n-        # Create datasets\r\n-        train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n-        val_dataset = dataset_cls(\r\n-            val_df,\r\n-            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n-        )\r\n-        test_dataset = dataset_cls(\r\n-            test_df,\r\n-            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n-        )\r\n-\r\n-        return train_dataset, val_dataset, test_dataset\r\n-\r\n-    def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n-        \"\"\"Resample data to match the target resolution.\"\"\"\r\n-        if self.data_loader is None:\r\n-            raise ValueError(\"Data loader not initialized\")\r\n-        rule = f'{target_resolution_minutes}T'\r\n-        resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n-        return resampled.reset_index()\r\n-\r\n-    def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n-            return self._create_monthly_aligned_intervals()\r\n-        \r\n-        # For daily predictions, ensure intervals align with day boundaries\r\n-        elif self.config.dataset_config.forecast_resolution_minutes >= 1440:  # Daily\r\n-            return self._create_daily_aligned_intervals()\r\n-            \r\n-        # For hourly and sub-hourly, use standard intervals\r\n-        return self._create_time_intervals()\r\n-\r\n-    def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        \"\"\"Create monthly aligned intervals.\"\"\"\r\n-        train_start, train_end = self.config.train_dates\r\n-        val_start, val_end = self.config.val_dates\r\n-        test_start, test_end = self.config.test_dates\r\n-        \r\n-        return (\r\n-            TimeInterval(min_date=train_start, max_date=train_end),\r\n-            TimeInterval(min_date=val_start, max_date=val_end),\r\n-            TimeInterval(min_date=test_start, max_date=test_end)\r\n-        )\r\n-\r\n-    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        \"\"\"Create daily aligned intervals.\"\"\"\r\n-        train_start, train_end = self.config.train_dates\r\n-        val_start, val_end = self.config.val_dates\r\n-        test_start, test_end = self.config.test_dates\r\n-        \r\n-        return (\r\n-            TimeInterval(min_date=train_start, max_date=train_end),\r\n-            TimeInterval(min_date=val_start, max_date=val_end),\r\n-            TimeInterval(min_date=test_start, max_date=test_end)\r\n-        )\r\n-\r\n-    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        \"\"\"Create standard time intervals.\"\"\"\r\n-        return self._create_monthly_aligned_intervals()\r\n-\r\n-    def _get_dataset_class(self):\r\n-        \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n-        if self.config.model_config.model_type.is_transformer:\r\n-            if self.config.model_config.model_type.is_resolution_specific:\r\n-                # Use TransformerDataset with resolution-specific optimizations\r\n-                return TransformerDataset\r\n-            return TransformerDataset\r\n-        return StandardDataset\r\n-\r\n-    def setup_model(self) -> None:\r\n-        \"\"\"Setup model and associated wrapper with resolution awareness.\"\"\"\r\n-        # Convert config to dict for factory\r\n-        model_config_dict = dataclasses.asdict(self.config.model_config)\r\n-\r\n-        # Create model using factory\r\n-        model = ModelFactory.create_base_model(\r\n-            self.config.model_config.model_type,\r\n-            model_config_dict\r\n-        )\r\n-\r\n-        # Convert training config to dict\r\n-        training_config_dict = dataclasses.asdict(self.config.training_config)\r\n-\r\n-        # Create appropriate wrapper\r\n-        if self.config.model_config.model_type == ModelType.LINEAR_REGRESSION:\r\n-            wrapper: Union[SklearnWrapper, PyTorchWrapper] = SklearnWrapper(\r\n-                cast(Any, model),\r\n-                self.config.model_config.model_type,\r\n-                training_config_dict\r\n-            )\r\n-        else:\r\n-            wrapper = PyTorchWrapper(\r\n-                model,\r\n-                self.config.model_config.model_type,\r\n-                training_config_dict\r\n-            )\r\n-        \r\n-        self.model_wrapper = wrapper\r\n-\r\n-    def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n-        \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n-        if not self.model_wrapper:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-\r\n-        # Make predictions\r\n-        test_start = time.time()\r\n-        predictions, targets = self.model_wrapper.predict(test_dataset)\r\n-        self.test_time = time.time() - test_start\r\n-\r\n-        # Get timestamps\r\n-        timestamps = test_dataset.time_labels\r\n-        if timestamps is None:\r\n-            timestamps = np.arange(len(predictions))\r\n-\r\n-        # Create evaluator with resolution-aware configuration\r\n-        evaluator = Evaluator(\r\n-            scaler=self.config.dataset_config.time_series_scaler,\r\n-            metric_config=MetricConfig(),  # Add this if needed\r\n-            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n-        )\r\n-\r\n-        # Evaluate predictions\r\n-        evaluation = evaluator.evaluate(\r\n-            predictions=predictions,\r\n-            targets=targets,\r\n-            timestamps=cast(NDArray, timestamps),\r\n-            num_variables=1\r\n-        )\r\n-\r\n-        # Create experiment\r\n-        self.experiment = Experiment(\r\n-            model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n-            evaluation=evaluation,\r\n-            training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n-            training_report=self.training_report,\r\n-            training_time=self.training_time,\r\n-            test_time=self.test_time\r\n-        )\r\n-\r\n-        # Save model if path provided\r\n-        if self.config.model_save_path:\r\n-            self._save_model(self.config.model_save_path)\r\n-\r\n-    @property\r\n-    def training_report(self) -> TrainingReport:  # Change return type\r\n-        \"\"\"Get training report.\"\"\"\r\n-        if not hasattr(self, '_training_report'):\r\n-            self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n-        return self._training_report\r\n-\r\n-    def train_model(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Tuple[PyTorchWrapper, TrainingReport]:\r\n-        \"\"\"Train model implementation.\"\"\"\r\n-        if not self.model_wrapper:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-        if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n-            raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n-\r\n-        # Initialize training report\r\n-        training_report = TrainingReport(\r\n-            train_losses=[],\r\n-            val_losses=[],\r\n-            learning_rates=[],\r\n-            epochs=0,\r\n-            additional_metrics={}\r\n-        )\r\n-\r\n-        is_transformer = self.config.model_config.model_type.is_transformer\r\n-        \r\n-        # Training loop\r\n-        for epoch in range(self.config.training_config.max_epochs):\r\n-            epoch_loss = 0.0\r\n-            num_batches = 0\r\n-            \r\n-            # Set model to training mode\r\n-            if hasattr(self.model_wrapper.model, 'train'):\r\n-                self.model_wrapper.model.train()\r\n-                \r\n-            # Process each batch\r\n-            for batch_input, batch_target in train_loader:\r\n-                # Handle transformer sequence data\r\n-                if is_transformer:\r\n-                    src = batch_input\r\n-                    tgt = batch_target[:, :-1]  # Input sequence\r\n-                    tgt_y = batch_target[:, 1:]  # Target sequence\r\n-                    # Forward the data directly to the model wrapper\r\n-                    if isinstance(self.model_wrapper, PyTorchWrapper):\r\n-                        loss = self.model_wrapper.training_step((src, tgt), tgt_y)\r\n-                    else:\r\n-                        loss = self._process_training_batch(batch_input, batch_target)\r\n-                else:\r\n-                    loss = self._process_training_batch(batch_input, batch_target)\r\n-                    \r\n-                epoch_loss += cast(float, loss)\r\n-                num_batches += 1\r\n-                \r\n-            # Process epoch results\r\n-            avg_epoch_loss = epoch_loss / num_batches\r\n-            training_report.train_losses.append(avg_epoch_loss)\r\n-            \r\n-            # Validation phase\r\n-            if val_loader:\r\n-                self._handle_validation(val_loader, training_report)\r\n-                if self.config.training_config.use_early_stopping and self._should_stop_early(training_report):\r\n-                    break\r\n-            \r\n-            # Log progress\r\n-            if (epoch + 1) % 10 == 0:\r\n-                print(f\"Epoch {epoch + 1}/{self.config.training_config.max_epochs}, \"\r\n-                      f\"Loss: {avg_epoch_loss:.4f}\")\r\n-        \r\n-        return self.model_wrapper, training_report\r\n-\r\n-    def _process_training_batch(\r\n-        self, \r\n-        batch_input: torch.Tensor,  # Simplified type hint\r\n-        batch_target: torch.Tensor\r\n-    ) -> float:\r\n-        \"\"\"Process a single training batch for non-transformer models.\"\"\"\r\n-        if self.model_wrapper is None:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-        \r\n-        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\r\n-\r\n-    def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n-        \"\"\"Handle validation phase of training.\"\"\"\r\n-        val_loss = self._validate_model(val_loader)\r\n-        if not hasattr(training_report, 'val_losses'):\r\n-            training_report.val_losses = []\r\n-        training_report.val_losses.append(val_loss)\r\n-\r\n-    def _validate_model(self, val_loader: DataLoader) -> float:\r\n-        \"\"\"Perform validation.\"\"\"\r\n-        if not self.model_wrapper:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-            \r\n-        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n-            # PyTorch specific validation\r\n-            if hasattr(self.model_wrapper.model, 'eval'):\r\n-                self.model_wrapper.model.eval()\r\n-            \r\n-            total_loss = 0.0\r\n-            num_batches = 0\r\n-            is_transformer = self.config.model_config.model_type.is_transformer\r\n-            \r\n-            with torch.no_grad():\r\n-                for batch_input, batch_target in val_loader:\r\n-                    # Handle transformer sequence data\r\n-                    if is_transformer:\r\n-                        src = batch_input\r\n-                        tgt = batch_target[:, :-1]\r\n-                        tgt_y = batch_target[:, 1:]\r\n-                        loss = self.model_wrapper.validation_step((src, tgt), tgt_y)\r\n-                    else:\r\n-                        loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n-                        \r\n-                    total_loss += cast(float, loss)\r\n-                    num_batches += 1\r\n-            \r\n-            # Set back to training mode if PyTorch model\r\n-            if hasattr(self.model_wrapper.model, 'train'):\r\n-                self.model_wrapper.model.train()\r\n-                \r\n-            return total_loss / num_batches if num_batches > 0 else float('inf')\r\n-        \r\n-        # For sklearn models, compute validation loss directly\r\n-        X_val, y_val = self._prepare_validation_data(val_loader)\r\n-        y_pred = self.model_wrapper.model.predict(X_val)\r\n-        return float(np.mean((y_pred - y_val) ** 2))  # MSE loss\r\n-\r\n-    def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n-        \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n-        X_list, y_list = [], []\r\n-        for batch_input, batch_target in val_loader:\r\n-            X_list.append(batch_input.numpy())\r\n-            y_list.append(batch_target.numpy())\r\n-        return np.vstack(X_list), np.vstack(y_list)\r\n-\r\n-    def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n-        \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n-        if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n-            return False\r\n-            \r\n-        patience = self.config.training_config.early_stopping_patience\r\n-        recent_losses = training_report.val_losses[-patience:]\r\n-        min_loss_idx = recent_losses.index(min(recent_losses))\r\n-        \r\n-        return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n-\r\n-    def training_step(\r\n-        self, \r\n-        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n-        batch_target: torch.Tensor\r\n-    ) -> float:\r\n-        \"\"\"Modified training step to handle transformer input format\"\"\"\r\n-        if self.model_wrapper is None:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-        \r\n-        if isinstance(batch_input, tuple):\r\n-            if self.config.model_config.model_type.is_transformer:\r\n-                src, tgt = batch_input\r\n-                tgt_y = batch_target[:, 1:]\r\n-                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n-            batch_input = batch_input[0]\r\n-            \r\n         return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733182175834,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -287,25 +287,28 @@\n         return self.model_wrapper, training_report\r\n \r\n     def _process_training_batch(\r\n         self, \r\n-        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n         batch_target: torch.Tensor\r\n     ) -> float:\r\n         \"\"\"Process a single training batch.\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n-            \r\n-        if isinstance(batch_input, tuple):\r\n-            if self.config.model_config.model_type.is_transformer:\r\n-                src = batch_input[0]\r\n-                tgt = batch_target[:, :-1]\r\n-                tgt_y = batch_target[:, 1:]\r\n-                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n-            batch_input = batch_input[0]  # Take first tensor if tuple\r\n         \r\n-        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\r\n+        is_transformer = self.config.model_config.model_type.is_transformer\r\n+        \r\n+        if is_transformer:\r\n+            if not isinstance(batch_input, tuple):\r\n+                raise ValueError(\"Transformer models expect a tuple of (src, tgt) sequences\")\r\n+            src, tgt = batch_input\r\n+            return self.model_wrapper.training_step((src, tgt), batch_target)\r\n+        else:\r\n+            if isinstance(batch_input, tuple):\r\n+                batch_input = batch_input[0]  # Extract the tensor\r\n+            return self.model_wrapper.training_step(batch_input, batch_target)\r\n \r\n+\r\n     def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n         \"\"\"Handle validation phase of training.\"\"\"\r\n         val_loss = self._validate_model(val_loader)\r\n         if not hasattr(training_report, 'val_losses'):\r\n@@ -313,37 +316,32 @@\n         training_report.val_losses.append(val_loss)\r\n \r\n     def _validate_model(self, val_loader: DataLoader) -> float:\r\n         \"\"\"Perform validation.\"\"\"\r\n-        if not self.model_wrapper:\r\n+        if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n-            \r\n-        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n-            # PyTorch specific validation\r\n-            if hasattr(self.model_wrapper.model, 'eval'):\r\n-                self.model_wrapper.model.eval()\r\n-            \r\n-            total_loss = 0.0\r\n-            num_batches = 0\r\n-            \r\n-            with torch.no_grad():\r\n-                for batch_input, batch_target in val_loader:\r\n-                    if isinstance(self.model_wrapper, PyTorchWrapper):\r\n-                        loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n-                        total_loss += loss\r\n-                        num_batches += 1\r\n-            \r\n-            # Set back to training mode if PyTorch model\r\n-            if hasattr(self.model_wrapper.model, 'train'):\r\n-                self.model_wrapper.model.train()\r\n-                \r\n-            return total_loss / num_batches if num_batches > 0 else float('inf')\r\n-        else:\r\n-            # For sklearn models, compute validation loss directly\r\n-            X_val, y_val = self._prepare_validation_data(val_loader)\r\n-            y_pred = self.model_wrapper.model.predict(X_val)\r\n-            return float(np.mean((y_pred - y_val) ** 2))  # MSE loss\r\n+        \r\n+        total_loss = 0.0\r\n+        num_batches = 0\r\n+        is_transformer = self.config.model_config.model_type.is_transformer\r\n \r\n+        with torch.no_grad():\r\n+            for batch_input, batch_target in val_loader:\r\n+                if is_transformer:\r\n+                    if not isinstance(batch_input, tuple):\r\n+                        raise ValueError(\"Transformer models expect a tuple of (src, tgt) sequences\")\r\n+                    src, tgt = batch_input\r\n+                    loss = self.model_wrapper.validation_step((src, tgt), batch_target)\r\n+                else:\r\n+                    if isinstance(batch_input, tuple):\r\n+                        batch_input = batch_input[0]\r\n+                    loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                total_loss += loss\r\n+                num_batches += 1\r\n+        \r\n+        return total_loss / num_batches if num_batches > 0 else float('inf')\r\n+\r\n+\r\n     def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n         \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n         X_list, y_list = [], []\r\n         for batch_input, batch_target in val_loader:\r\n"
                },
                {
                    "date": 1733239296384,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -285,30 +285,31 @@\n                       f\"Loss: {avg_epoch_loss:.4f}\")\r\n         \r\n         return self.model_wrapper, training_report\r\n \r\n-    def _process_training_batch(\r\n-        self, \r\n-        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n-        batch_target: torch.Tensor\r\n-    ) -> float:\r\n-        \"\"\"Process a single training batch.\"\"\"\r\n+    def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n+                              batch_target: torch.Tensor) -> float:\r\n+        \"\"\"Process a single training batch with device tracking.\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n         \r\n-        is_transformer = self.config.model_config.model_type.is_transformer\r\n-        \r\n-        if is_transformer:\r\n-            if not isinstance(batch_input, tuple):\r\n-                raise ValueError(\"Transformer models expect a tuple of (src, tgt) sequences\")\r\n+        print(\"\\nDEBUG - Processing batch:\")\r\n+        if isinstance(batch_input, tuple):\r\n             src, tgt = batch_input\r\n-            return self.model_wrapper.training_step((src, tgt), batch_target)\r\n+            print(f\"Source shape: {src.shape}, device: {src.device}\")\r\n+            print(f\"Target input shape: {tgt.shape}, device: {tgt.device}\")\r\n         else:\r\n-            if isinstance(batch_input, tuple):\r\n-                batch_input = batch_input[0]  # Extract the tensor\r\n-            return self.model_wrapper.training_step(batch_input, batch_target)\r\n+            print(f\"Input shape: {batch_input.shape}, device: {batch_input.device}\")\r\n+        print(f\"Target shape: {batch_target.shape}, device: {batch_target.device}\")\r\n+        \r\n+        if isinstance(batch_input, tuple):\r\n+            if self.config.model_config.model_type.is_transformer:\r\n+                src, tgt = batch_input\r\n+                return self.model_wrapper.training_step((src, tgt), batch_target)\r\n+            batch_input = batch_input[0]\r\n+            \r\n+        return self.model_wrapper.training_step(batch_input, batch_target)\r\n \r\n-\r\n     def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n         \"\"\"Handle validation phase of training.\"\"\"\r\n         val_loss = self._validate_model(val_loader)\r\n         if not hasattr(training_report, 'val_losses'):\r\n"
                },
                {
                    "date": 1733248989379,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -244,15 +244,19 @@\n             raise ValueError(\"Model wrapper not initialized\")\r\n         if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n             raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n \r\n+        # Get device from model wrapper\r\n+        device = self.model_wrapper.device\r\n+        print(f\"\\nDEBUG: Training on device: {device}\")\r\n+\r\n         # Initialize training report with metrics\r\n         training_report = TrainingReport(\r\n             train_losses=[],\r\n             val_losses=[],\r\n             learning_rates=[],\r\n             epochs=0,\r\n-            additional_metrics={}  # Use additional_metrics instead of metrics\r\n+            additional_metrics={}\r\n         )\r\n         \r\n         # Training loop\r\n         for epoch in range(self.config.training_config.max_epochs):\r\n@@ -264,11 +268,19 @@\n                 self.model_wrapper.model.train()\r\n                 \r\n             # Process each batch\r\n             for batch_input, batch_target in train_loader:\r\n+                # Move batch to correct device\r\n+                if isinstance(batch_input, tuple):\r\n+                    batch_input = tuple(b.to(device) for b in batch_input)\r\n+                else:\r\n+                    batch_input = batch_input.to(device)\r\n+                batch_target = batch_target.to(device)\r\n+                \r\n                 loss = self._process_training_batch(batch_input, batch_target)\r\n                 epoch_loss += loss\r\n                 num_batches += 1\r\n+\r\n                 \r\n             # Process epoch results\r\n             avg_epoch_loss = epoch_loss / num_batches\r\n             training_report.train_losses.append(avg_epoch_loss)\r\n"
                },
                {
                    "date": 1733249053832,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -298,30 +298,36 @@\n         \r\n         return self.model_wrapper, training_report\r\n \r\n     def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n-                              batch_target: torch.Tensor) -> float:\r\n+                            batch_target: torch.Tensor) -> float:\r\n         \"\"\"Process a single training batch with device tracking.\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n         \r\n-        print(\"\\nDEBUG - Processing batch:\")\r\n-        if isinstance(batch_input, tuple):\r\n-            src, tgt = batch_input\r\n-            print(f\"Source shape: {src.shape}, device: {src.device}\")\r\n-            print(f\"Target input shape: {tgt.shape}, device: {tgt.device}\")\r\n-        else:\r\n-            print(f\"Input shape: {batch_input.shape}, device: {batch_input.device}\")\r\n-        print(f\"Target shape: {batch_target.shape}, device: {batch_target.device}\")\r\n-        \r\n-        if isinstance(batch_input, tuple):\r\n-            if self.config.model_config.model_type.is_transformer:\r\n+        try:\r\n+            if isinstance(batch_input, tuple):\r\n                 src, tgt = batch_input\r\n-                return self.model_wrapper.training_step((src, tgt), batch_target)\r\n-            batch_input = batch_input[0]\r\n+                if self.config.model_config.model_type.is_transformer:\r\n+                    # Generate masks on correct device\r\n+                    src_mask = self.model_wrapper.model.generate_square_subsequent_mask(src.size(1)).to(src.device)\r\n+                    tgt_mask = self.model_wrapper.model.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\r\n+                    return self.model_wrapper.training_step((src, tgt), batch_target, src_mask=src_mask, tgt_mask=tgt_mask)\r\n+                batch_input = batch_input[0]\r\n+                \r\n+            return self.model_wrapper.training_step(batch_input, batch_target)\r\n             \r\n-        return self.model_wrapper.training_step(batch_input, batch_target)\r\n-\r\n+        except RuntimeError as e:\r\n+            print(f\"\\nERROR in training batch: {str(e)}\")\r\n+            if isinstance(batch_input, tuple):\r\n+                print(f\"Source device: {batch_input[0].device}, shape: {batch_input[0].shape}\")\r\n+                print(f\"Target input device: {batch_input[1].device}, shape: {batch_input[1].shape}\")\r\n+            else:\r\n+                print(f\"Input device: {batch_input.device}, shape: {batch_input.shape}\")\r\n+            print(f\"Target device: {batch_target.device}, shape: {batch_target.shape}\")\r\n+            print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n+            raise\r\n+    \r\n     def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n         \"\"\"Handle validation phase of training.\"\"\"\r\n         val_loss = self._validate_model(val_loader)\r\n         if not hasattr(training_report, 'val_losses'):\r\n"
                },
                {
                    "date": 1733249232451,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,423 @@\n+# pipeline/implementations/time_series_pipeline.py\r\n+from __future__ import annotations\r\n+import time\r\n+from typing import Tuple, Optional, Dict, Any, cast, Union\r\n+from pathlib import Path\r\n+import torch\r\n+import dataclasses\r\n+from argparse import Namespace\r\n+import numpy as np\r\n+from numpy.typing import NDArray\r\n+import pandas as pd  # Add this import statement\r\n+from datetime import date  # Add this import statement\r\n+\r\n+from data_loading.datasets.standard_dataset import StandardDataset\r\n+from data_loading.datasets.transformer_dataset import TransformerDataset\r\n+from data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\n+from evaluation.evaluator import Evaluator\r\n+from evaluation.metrics import MetricConfig\r\n+from experiments.experiment import Experiment\r\n+from models.base.base_wrapper import BaseWrapper\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.wrappers.pytorch_wrapper import PyTorchWrapper\r\n+from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n+from pipeline.base.base_pipeline import BasePipeline\r\n+from training.reports.training_report import TrainingReport\r\n+from torch.utils.data import DataLoader\r\n+\r\n+class TimeSeriesPipeline(BasePipeline):\r\n+    \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n+\r\n+    def __init__(self, *args, **kwargs):\r\n+        super().__init__(*args, **kwargs)\r\n+        self.data_loader: Optional[TimeSeriesLoader] = None\r\n+        self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n+        \r\n+        # Validate resolutions during initialization\r\n+        self._validate_resolutions()\r\n+\r\n+    def _validate_resolutions(self) -> None:\r\n+        \"\"\"Validate that resolutions are consistent across configurations.\"\"\"\r\n+        input_res = self.config.dataset_config.input_resolution_minutes\r\n+        forecast_res = self.config.dataset_config.forecast_resolution_minutes\r\n+        \r\n+        # Check model type matches resolution\r\n+        if self.config.model_config.model_type.is_resolution_specific:\r\n+            expected_type = ModelType.get_for_resolution(forecast_res)\r\n+            if self.config.model_config.model_type != expected_type:\r\n+                raise ValueError(\r\n+                    f\"Model type {self.config.model_config.model_type} does not match \"\r\n+                    f\"forecast resolution {forecast_res} minutes. Expected {expected_type}\"\r\n+                )\r\n+\r\n+    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n+        \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n+        self.data_loader = TimeSeriesLoader(\r\n+            time_variable=self.config.dataset_config.time_variable,\r\n+            target_variable=self.config.dataset_config.target_variable\r\n+        )\r\n+        return self.data_loader\r\n+\r\n+    def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n+        \"\"\"Prepare train, validation and test datasets with resolution handling.\"\"\"\r\n+        if not self.data_loader:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n+\r\n+        # Load and potentially resample data\r\n+        df = self.data_loader.load(self.config.data_path)\r\n+        \r\n+        # Resample if input and forecast resolutions differ\r\n+        if self.config.dataset_config.needs_resampling():\r\n+            df = self._resample_data(\r\n+                df,\r\n+                target_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+            )\r\n+\r\n+        # Get time intervals adjusted for resolution\r\n+        train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n+\r\n+        # Split data\r\n+        train_df, val_df, test_df = self.data_loader.split(\r\n+            df,\r\n+            train_interval,\r\n+            val_interval,\r\n+            test_interval\r\n+        )\r\n+\r\n+        # Select appropriate dataset class\r\n+        dataset_cls = self._get_dataset_class()\r\n+\r\n+        # Create datasets\r\n+        train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n+        val_dataset = dataset_cls(\r\n+            val_df,\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n+        )\r\n+        test_dataset = dataset_cls(\r\n+            test_df,\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n+        )\r\n+\r\n+        return train_dataset, val_dataset, test_dataset\r\n+\r\n+    def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n+        \"\"\"Resample data to match the target resolution.\"\"\"\r\n+        if self.data_loader is None:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n+        rule = f'{target_resolution_minutes}T'\r\n+        resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n+        return resampled.reset_index()\r\n+\r\n+    def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n+            return self._create_monthly_aligned_intervals()\r\n+        \r\n+        # For daily predictions, ensure intervals align with day boundaries\r\n+        elif self.config.dataset_config.forecast_resolution_minutes >= 1440:  # Daily\r\n+            return self._create_daily_aligned_intervals()\r\n+            \r\n+        # For hourly and sub-hourly, use standard intervals\r\n+        return self._create_time_intervals()\r\n+\r\n+    def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create monthly aligned intervals.\"\"\"\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create daily aligned intervals.\"\"\"\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create standard time intervals.\"\"\"\r\n+        return self._create_monthly_aligned_intervals()\r\n+\r\n+    def _get_dataset_class(self):\r\n+        \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n+        if self.config.model_config.model_type.is_transformer:\r\n+            if self.config.model_config.model_type.is_resolution_specific:\r\n+                # Use TransformerDataset with resolution-specific optimizations\r\n+                return TransformerDataset\r\n+            return TransformerDataset\r\n+        return StandardDataset\r\n+\r\n+    def setup_model(self) -> None:\r\n+        \"\"\"Setup model and associated wrapper with resolution awareness.\"\"\"\r\n+        # Convert config to dict for factory\r\n+        model_config_dict = dataclasses.asdict(self.config.model_config)\r\n+\r\n+        # Create model using factory\r\n+        model = ModelFactory.create_base_model(\r\n+            self.config.model_config.model_type,\r\n+            model_config_dict\r\n+        )\r\n+\r\n+        # Convert training config to dict\r\n+        training_config_dict = dataclasses.asdict(self.config.training_config)\r\n+\r\n+        # Create appropriate wrapper\r\n+        if self.config.model_config.model_type == ModelType.LINEAR_REGRESSION:\r\n+            wrapper: Union[SklearnWrapper, PyTorchWrapper] = SklearnWrapper(\r\n+                cast(Any, model),\r\n+                self.config.model_config.model_type,\r\n+                training_config_dict\r\n+            )\r\n+        else:\r\n+            wrapper = PyTorchWrapper(\r\n+                model,\r\n+                self.config.model_config.model_type,\r\n+                training_config_dict\r\n+            )\r\n+        \r\n+        self.model_wrapper = wrapper\r\n+\r\n+    def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n+        \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+\r\n+        # Make predictions\r\n+        test_start = time.time()\r\n+        predictions, targets = self.model_wrapper.predict(test_dataset)\r\n+        self.test_time = time.time() - test_start\r\n+\r\n+        # Get timestamps\r\n+        timestamps = test_dataset.time_labels\r\n+        if timestamps is None:\r\n+            timestamps = np.arange(len(predictions))\r\n+\r\n+        # Create evaluator with resolution-aware configuration\r\n+        evaluator = Evaluator(\r\n+            scaler=self.config.dataset_config.time_series_scaler,\r\n+            metric_config=MetricConfig(),  # Add this if needed\r\n+            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+        )\r\n+\r\n+        # Evaluate predictions\r\n+        evaluation = evaluator.evaluate(\r\n+            predictions=predictions,\r\n+            targets=targets,\r\n+            timestamps=cast(NDArray, timestamps),\r\n+            num_variables=1\r\n+        )\r\n+\r\n+        # Create experiment\r\n+        self.experiment = Experiment(\r\n+            model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n+            evaluation=evaluation,\r\n+            training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n+            training_report=self.training_report,\r\n+            training_time=self.training_time,\r\n+            test_time=self.test_time\r\n+        )\r\n+\r\n+        # Save model if path provided\r\n+        if self.config.model_save_path:\r\n+            self._save_model(self.config.model_save_path)\r\n+\r\n+    @property\r\n+    def training_report(self) -> TrainingReport:  # Change return type\r\n+        \"\"\"Get training report.\"\"\"\r\n+        if not hasattr(self, '_training_report'):\r\n+            self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n+        return self._training_report\r\n+\r\n+    def train_model(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Tuple[PyTorchWrapper, TrainingReport]:\r\n+        \"\"\"Train model implementation.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n+\r\n+        # Get device from model wrapper\r\n+        device = self.model_wrapper.device\r\n+        print(f\"\\nDEBUG: Training on device: {device}\")\r\n+\r\n+        # Initialize training report with metrics\r\n+        training_report = TrainingReport(\r\n+            train_losses=[],\r\n+            val_losses=[],\r\n+            learning_rates=[],\r\n+            epochs=0,\r\n+            additional_metrics={}\r\n+        )\r\n+        \r\n+        # Training loop\r\n+        for epoch in range(self.config.training_config.max_epochs):\r\n+            epoch_loss = 0.0\r\n+            num_batches = 0\r\n+            \r\n+            # Set model to training mode\r\n+            if hasattr(self.model_wrapper.model, 'train'):\r\n+                self.model_wrapper.model.train()\r\n+                \r\n+            # Process each batch\r\n+            for batch_input, batch_target in train_loader:\r\n+                # Move batch to correct device\r\n+                if isinstance(batch_input, tuple):\r\n+                    batch_input = tuple(b.to(device) for b in batch_input)\r\n+                else:\r\n+                    batch_input = batch_input.to(device)\r\n+                batch_target = batch_target.to(device)\r\n+                \r\n+                loss = self._process_training_batch(batch_input, batch_target)\r\n+                epoch_loss += loss\r\n+                num_batches += 1\r\n+\r\n+                \r\n+            # Process epoch results\r\n+            avg_epoch_loss = epoch_loss / num_batches\r\n+            training_report.train_losses.append(avg_epoch_loss)\r\n+            \r\n+            # Validation phase\r\n+            if val_loader:\r\n+                self._handle_validation(val_loader, training_report)\r\n+                if self.config.training_config.use_early_stopping and self._should_stop_early(training_report):\r\n+                    break\r\n+            \r\n+            # Log progress\r\n+            if (epoch + 1) % 10 == 0:\r\n+                print(f\"Epoch {epoch + 1}/{self.config.training_config.max_epochs}, \"\r\n+                      f\"Loss: {avg_epoch_loss:.4f}\")\r\n+        \r\n+        return self.model_wrapper, training_report\r\n+\r\n+    def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n+                            batch_target: torch.Tensor) -> float:\r\n+        \"\"\"Process a single training batch with device tracking.\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        try:\r\n+            # Handle PyTorch models\r\n+            if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                if isinstance(batch_input, tuple):\r\n+                    src, tgt = batch_input\r\n+                    if self.config.model_config.model_type.is_transformer:\r\n+                        # Generate masks on correct device\r\n+                        device = next(self.model_wrapper.model.parameters()).device\r\n+                        src_mask = self.model_wrapper.model.generate_square_subsequent_mask(src.size(1)).to(device)\r\n+                        tgt_mask = self.model_wrapper.model.generate_square_subsequent_mask(tgt.size(1)).to(device)\r\n+                        return self.model_wrapper.training_step((src, tgt), batch_target, src_mask=src_mask, tgt_mask=tgt_mask)\r\n+                    batch_input = batch_input[0]\r\n+                return self.model_wrapper.training_step(batch_input, batch_target)\r\n+            \r\n+            # Handle sklearn models\r\n+            else:\r\n+                if isinstance(batch_input, tuple):\r\n+                    batch_input = batch_input[0]\r\n+                return self.model_wrapper.training_step(batch_input, batch_target)\r\n+                \r\n+        except RuntimeError as e:\r\n+            print(f\"\\nERROR in training batch: {str(e)}\")\r\n+            if isinstance(batch_input, tuple):\r\n+                print(f\"Source device: {batch_input[0].device}, shape: {batch_input[0].shape}\")\r\n+                print(f\"Target input device: {batch_input[1].device}, shape: {batch_input[1].shape}\")\r\n+            else:\r\n+                print(f\"Input device: {batch_input.device}, shape: {batch_input.shape}\")\r\n+            print(f\"Target device: {batch_target.device}, shape: {batch_target.shape}\")\r\n+            if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n+            raise\r\n+    \r\n+    def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n+        \"\"\"Handle validation phase of training.\"\"\"\r\n+        val_loss = self._validate_model(val_loader)\r\n+        if not hasattr(training_report, 'val_losses'):\r\n+            training_report.val_losses = []\r\n+        training_report.val_losses.append(val_loss)\r\n+\r\n+    def _validate_model(self, val_loader: DataLoader) -> float:\r\n+        \"\"\"Perform validation.\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        total_loss = 0.0\r\n+        num_batches = 0\r\n+        is_transformer = self.config.model_config.model_type.is_transformer\r\n+\r\n+        with torch.no_grad():\r\n+            for batch_input, batch_target in val_loader:\r\n+                # Handle PyTorch models\r\n+                if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                    if is_transformer:\r\n+                        if not isinstance(batch_input, tuple):\r\n+                            raise ValueError(\"Transformer models expect a tuple of (src, tgt) sequences\")\r\n+                        src, tgt = batch_input\r\n+                        device = next(self.model_wrapper.model.parameters()).device\r\n+                        src = src.to(device)\r\n+                        tgt = tgt.to(device)\r\n+                        batch_target = batch_target.to(device)\r\n+                        loss = self.model_wrapper.validation_step((src, tgt), batch_target)\r\n+                    else:\r\n+                        if isinstance(batch_input, tuple):\r\n+                            batch_input = batch_input[0]\r\n+                        device = next(self.model_wrapper.model.parameters()).device\r\n+                        batch_input = batch_input.to(device)\r\n+                        batch_target = batch_target.to(device)\r\n+                        loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                # Handle sklearn models\r\n+                else:\r\n+                    if isinstance(batch_input, tuple):\r\n+                        batch_input = batch_input[0]\r\n+                    loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                    \r\n+                total_loss += loss\r\n+                num_batches += 1\r\n+        \r\n+        return total_loss / num_batches if num_batches > 0 else float('inf')\r\n+\r\n+\r\n+    def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n+        \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n+        X_list, y_list = [], []\r\n+        for batch_input, batch_target in val_loader:\r\n+            X_list.append(batch_input.numpy())\r\n+            y_list.append(batch_target.numpy())\r\n+        return np.vstack(X_list), np.vstack(y_list)\r\n+\r\n+    def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n+        \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n+        if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n+            return False\r\n+            \r\n+        patience = self.config.training_config.early_stopping_patience\r\n+        recent_losses = training_report.val_losses[-patience:]\r\n+        min_loss_idx = recent_losses.index(min(recent_losses))\r\n+        \r\n+        return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n+\r\n+    def training_step(\r\n+        self, \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_target: torch.Tensor\r\n+    ) -> float:\r\n+        \"\"\"Modified training step to handle transformer input format\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        if isinstance(batch_input, tuple):\r\n+            if self.config.model_config.model_type.is_transformer:\r\n+                src = batch_input[0]\r\n+                tgt = batch_target[:, :-1]\r\n+                tgt_y = batch_target[:, 1:]\r\n+                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n+            batch_input = batch_input[0]\r\n+            \r\n+        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733249405818,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -186,8 +186,15 @@\n             )\r\n         \r\n         self.model_wrapper = wrapper\r\n \r\n+        print(\"\\nDEBUG: Model setup\")\r\n+        print(f\"Device config: {training_config_dict.get('device', 'Not specified')}\")\r\n+        print(f\"CUDA available: {torch.cuda.is_available()}\")\r\n+        if torch.cuda.is_available():\r\n+            print(f\"Current CUDA device: {torch.cuda.current_device()}\")\r\n+            print(f\"Device count: {torch.cuda.device_count()}\")\r\n+\r\n     def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n         \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n         if not self.model_wrapper:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n@@ -257,10 +264,20 @@\n             epochs=0,\r\n             additional_metrics={}\r\n         )\r\n         \r\n-        # Training loop\r\n+        # Add CUDA debugging info\r\n+        print(\"\\nDEBUG: Training setup\")\r\n+        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\r\n+        print(f\"CUDA memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\r\n+        \r\n+        # Monitor device placement during training\r\n         for epoch in range(self.config.training_config.max_epochs):\r\n+            if epoch == 0 or epoch % 10 == 0:  # Print every 10 epochs\r\n+                print(f\"\\nDEBUG - Epoch {epoch} devices:\")\r\n+                print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n+                print(f\"CUDA memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\r\n+\r\n             epoch_loss = 0.0\r\n             num_batches = 0\r\n             \r\n             # Set model to training mode\r\n@@ -303,8 +320,20 @@\n         \"\"\"Process a single training batch with device tracking.\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n         \r\n+        # Add debug prints\r\n+        if isinstance(batch_input, tuple):\r\n+            print(\"\\nDEBUG - Batch devices:\")\r\n+            print(f\"Source input device: {batch_input[0].device}\")\r\n+            print(f\"Target input device: {batch_input[1].device}\")\r\n+        else:\r\n+            print(f\"\\nDEBUG - Input device: {batch_input.device}\")\r\n+        print(f\"Target device: {batch_target.device}\")\r\n+        \r\n+        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n+        \r\n         try:\r\n             # Handle PyTorch models\r\n             if isinstance(self.model_wrapper, PyTorchWrapper):\r\n                 if isinstance(batch_input, tuple):\r\n@@ -351,8 +380,22 @@\n         total_loss = 0.0\r\n         num_batches = 0\r\n         is_transformer = self.config.model_config.model_type.is_transformer\r\n \r\n+        # Add validation device debugging\r\n+        print(\"\\nDEBUG: Validation devices\")\r\n+        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n+        \r\n+        for batch_input, batch_target in val_loader:\r\n+            if isinstance(batch_input, tuple):\r\n+                print(f\"Source validation device: {batch_input[0].device}\")\r\n+                print(f\"Target validation device: {batch_input[1].device}\")\r\n+            else:\r\n+                print(f\"Input validation device: {batch_input.device}\")\r\n+            print(f\"Target validation device: {batch_target.device}\")\r\n+            break  # Only print first batch\r\n+\r\n         with torch.no_grad():\r\n             for batch_input, batch_target in val_loader:\r\n                 # Handle PyTorch models\r\n                 if isinstance(self.model_wrapper, PyTorchWrapper):\r\n"
                },
                {
                    "date": 1733250795787,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,8 +24,10 @@\n from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n from pipeline.base.base_pipeline import BasePipeline\r\n from training.reports.training_report import TrainingReport\r\n from torch.utils.data import DataLoader\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n \r\n class TimeSeriesPipeline(BasePipeline):\r\n     \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n \r\n@@ -33,8 +35,16 @@\n         super().__init__(*args, **kwargs)\r\n         self.data_loader: Optional[TimeSeriesLoader] = None\r\n         self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n         \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"TimeSeriesPipeline\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        \r\n         # Validate resolutions during initialization\r\n         self._validate_resolutions()\r\n \r\n     def _validate_resolutions(self) -> None:\r\n@@ -186,14 +196,14 @@\n             )\r\n         \r\n         self.model_wrapper = wrapper\r\n \r\n-        print(\"\\nDEBUG: Model setup\")\r\n-        print(f\"Device config: {training_config_dict.get('device', 'Not specified')}\")\r\n-        print(f\"CUDA available: {torch.cuda.is_available()}\")\r\n-        if torch.cuda.is_available():\r\n-            print(f\"Current CUDA device: {torch.cuda.current_device()}\")\r\n-            print(f\"Device count: {torch.cuda.device_count()}\")\r\n+        self.logger.debug(\"Model setup complete\", {\r\n+            \"device_config\": training_config_dict.get('device', 'Not specified'),\r\n+            \"cuda_available\": torch.cuda.is_available(),\r\n+            \"cuda_device\": torch.cuda.current_device() if torch.cuda.is_available() else None,\r\n+            \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0\r\n+        })\r\n \r\n     def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n         \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n         if not self.model_wrapper:\r\n@@ -253,9 +263,9 @@\n             raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n \r\n         # Get device from model wrapper\r\n         device = self.model_wrapper.device\r\n-        print(f\"\\nDEBUG: Training on device: {device}\")\r\n+        self.logger.info(f\"Starting model training\", {\"device\": str(device)})\r\n \r\n         # Initialize training report with metrics\r\n         training_report = TrainingReport(\r\n             train_losses=[],\r\n@@ -264,19 +274,21 @@\n             epochs=0,\r\n             additional_metrics={}\r\n         )\r\n         \r\n-        # Add CUDA debugging info\r\n-        print(\"\\nDEBUG: Training setup\")\r\n-        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\r\n-        print(f\"CUDA memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\r\n+        self.logger.debug(\"CUDA memory status\", {\r\n+            \"memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\r\n+            \"memory_cached_gb\": torch.cuda.memory_reserved() / 1e9\r\n+        })\r\n         \r\n         # Monitor device placement during training\r\n         for epoch in range(self.config.training_config.max_epochs):\r\n             if epoch == 0 or epoch % 10 == 0:  # Print every 10 epochs\r\n-                print(f\"\\nDEBUG - Epoch {epoch} devices:\")\r\n-                print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n-                print(f\"CUDA memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\r\n+                self.logger.debug(f\"Epoch status\", {\r\n+                    \"epoch\": epoch,\r\n+                    \"model_device\": str(next(self.model_wrapper.model.parameters()).device),\r\n+                    \"cuda_memory_gb\": torch.cuda.memory_allocated() / 1e9\r\n+                })\r\n \r\n             epoch_loss = 0.0\r\n             num_batches = 0\r\n             \r\n@@ -309,10 +321,13 @@\n                     break\r\n             \r\n             # Log progress\r\n             if (epoch + 1) % 10 == 0:\r\n-                print(f\"Epoch {epoch + 1}/{self.config.training_config.max_epochs}, \"\r\n-                      f\"Loss: {avg_epoch_loss:.4f}\")\r\n+                self.logger.info(\"Training progress\", {\r\n+                    \"epoch\": epoch + 1,\r\n+                    \"total_epochs\": self.config.training_config.max_epochs,\r\n+                    \"loss\": float(avg_epoch_loss)\r\n+                })\r\n         \r\n         return self.model_wrapper, training_report\r\n \r\n     def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n@@ -320,20 +335,24 @@\n         \"\"\"Process a single training batch with device tracking.\"\"\"\r\n         if self.model_wrapper is None:\r\n             raise ValueError(\"Model wrapper not initialized\")\r\n         \r\n-        # Add debug prints\r\n+        # Debug logging for device placement\r\n+        debug_info = {\r\n+            \"target_device\": str(batch_target.device),\r\n+            \"model_device\": str(next(self.model_wrapper.model.parameters()).device) if isinstance(self.model_wrapper, PyTorchWrapper) else \"N/A\"\r\n+        }\r\n+        \r\n         if isinstance(batch_input, tuple):\r\n-            print(\"\\nDEBUG - Batch devices:\")\r\n-            print(f\"Source input device: {batch_input[0].device}\")\r\n-            print(f\"Target input device: {batch_input[1].device}\")\r\n+            debug_info.update({\r\n+                \"source_input_device\": str(batch_input[0].device),\r\n+                \"target_input_device\": str(batch_input[1].device)\r\n+            })\r\n         else:\r\n-            print(f\"\\nDEBUG - Input device: {batch_input.device}\")\r\n-        print(f\"Target device: {batch_target.device}\")\r\n+            debug_info[\"input_device\"] = str(batch_input.device)\r\n+            \r\n+        self.logger.debug(\"Batch processing details\", debug_info)\r\n         \r\n-        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n-            print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n-        \r\n         try:\r\n             # Handle PyTorch models\r\n             if isinstance(self.model_wrapper, PyTorchWrapper):\r\n                 if isinstance(batch_input, tuple):\r\n@@ -353,25 +372,44 @@\n                     batch_input = batch_input[0]\r\n                 return self.model_wrapper.training_step(batch_input, batch_target)\r\n                 \r\n         except RuntimeError as e:\r\n-            print(f\"\\nERROR in training batch: {str(e)}\")\r\n+            error_info = {\r\n+                \"error\": str(e),\r\n+                \"target_shape\": batch_target.shape,\r\n+                \"target_device\": str(batch_target.device)\r\n+            }\r\n+            \r\n             if isinstance(batch_input, tuple):\r\n-                print(f\"Source device: {batch_input[0].device}, shape: {batch_input[0].shape}\")\r\n-                print(f\"Target input device: {batch_input[1].device}, shape: {batch_input[1].shape}\")\r\n+                error_info.update({\r\n+                    \"source_shape\": batch_input[0].shape,\r\n+                    \"source_device\": str(batch_input[0].device),\r\n+                    \"target_input_shape\": batch_input[1].shape,\r\n+                    \"target_input_device\": str(batch_input[1].device)\r\n+                })\r\n             else:\r\n-                print(f\"Input device: {batch_input.device}, shape: {batch_input.shape}\")\r\n-            print(f\"Target device: {batch_target.device}, shape: {batch_target.shape}\")\r\n+                error_info.update({\r\n+                    \"input_shape\": batch_input.shape,\r\n+                    \"input_device\": str(batch_input.device)\r\n+                })\r\n+                \r\n             if isinstance(self.model_wrapper, PyTorchWrapper):\r\n-                print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n+                error_info[\"model_device\"] = str(next(self.model_wrapper.model.parameters()).device)\r\n+                \r\n+            self.logger.error(\"Training batch processing failed\", error_info)\r\n             raise\r\n     \r\n     def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n         \"\"\"Handle validation phase of training.\"\"\"\r\n         val_loss = self._validate_model(val_loader)\r\n         if not hasattr(training_report, 'val_losses'):\r\n             training_report.val_losses = []\r\n         training_report.val_losses.append(val_loss)\r\n+        \r\n+        self.logger.debug(\"Validation completed\", {\r\n+            \"validation_loss\": float(val_loss),\r\n+            \"epoch\": len(training_report.val_losses)\r\n+        })\r\n \r\n     def _validate_model(self, val_loader: DataLoader) -> float:\r\n         \"\"\"Perform validation.\"\"\"\r\n         if self.model_wrapper is None:\r\n@@ -381,21 +419,27 @@\n         num_batches = 0\r\n         is_transformer = self.config.model_config.model_type.is_transformer\r\n \r\n         # Add validation device debugging\r\n-        print(\"\\nDEBUG: Validation devices\")\r\n+        debug_info = {}\r\n         if isinstance(self.model_wrapper, PyTorchWrapper):\r\n-            print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n+            debug_info[\"model_device\"] = str(next(self.model_wrapper.model.parameters()).device)\r\n         \r\n+        self.logger.debug(\"Starting validation\", debug_info)\r\n+        \r\n         for batch_input, batch_target in val_loader:\r\n             if isinstance(batch_input, tuple):\r\n-                print(f\"Source validation device: {batch_input[0].device}\")\r\n-                print(f\"Target validation device: {batch_input[1].device}\")\r\n+                debug_info.update({\r\n+                    \"source_validation_device\": str(batch_input[0].device),\r\n+                    \"target_validation_device\": str(batch_input[1].device)\r\n+                })\r\n             else:\r\n-                print(f\"Input validation device: {batch_input.device}\")\r\n-            print(f\"Target validation device: {batch_target.device}\")\r\n+                debug_info[\"input_validation_device\"] = str(batch_input.device)\r\n+            debug_info[\"target_validation_device\"] = str(batch_target.device)\r\n             break  # Only print first batch\r\n \r\n+        self.logger.debug(\"Validation batch devices\", debug_info)\r\n+\r\n         with torch.no_grad():\r\n             for batch_input, batch_target in val_loader:\r\n                 # Handle PyTorch models\r\n                 if isinstance(self.model_wrapper, PyTorchWrapper):\r\n@@ -462,404 +506,5 @@\n                 tgt_y = batch_target[:, 1:]\r\n                 return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n             batch_input = batch_input[0]\r\n             \r\n-        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n-# pipeline/implementations/time_series_pipeline.py\r\n-from __future__ import annotations\r\n-import time\r\n-from typing import Tuple, Optional, Dict, Any, cast, Union\r\n-from pathlib import Path\r\n-import torch\r\n-import dataclasses\r\n-from argparse import Namespace\r\n-import numpy as np\r\n-from numpy.typing import NDArray\r\n-import pandas as pd  # Add this import statement\r\n-from datetime import date  # Add this import statement\r\n-\r\n-from data_loading.datasets.standard_dataset import StandardDataset\r\n-from data_loading.datasets.transformer_dataset import TransformerDataset\r\n-from data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\n-from evaluation.evaluator import Evaluator\r\n-from evaluation.metrics import MetricConfig\r\n-from experiments.experiment import Experiment\r\n-from models.base.base_wrapper import BaseWrapper\r\n-from models.registry.factory import ModelFactory\r\n-from models.registry.model_types import ModelType\r\n-from models.wrappers.pytorch_wrapper import PyTorchWrapper\r\n-from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n-from pipeline.base.base_pipeline import BasePipeline\r\n-from training.reports.training_report import TrainingReport\r\n-from torch.utils.data import DataLoader\r\n-\r\n-class TimeSeriesPipeline(BasePipeline):\r\n-    \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n-\r\n-    def __init__(self, *args, **kwargs):\r\n-        super().__init__(*args, **kwargs)\r\n-        self.data_loader: Optional[TimeSeriesLoader] = None\r\n-        self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n-        \r\n-        # Validate resolutions during initialization\r\n-        self._validate_resolutions()\r\n-\r\n-    def _validate_resolutions(self) -> None:\r\n-        \"\"\"Validate that resolutions are consistent across configurations.\"\"\"\r\n-        input_res = self.config.dataset_config.input_resolution_minutes\r\n-        forecast_res = self.config.dataset_config.forecast_resolution_minutes\r\n-        \r\n-        # Check model type matches resolution\r\n-        if self.config.model_config.model_type.is_resolution_specific:\r\n-            expected_type = ModelType.get_for_resolution(forecast_res)\r\n-            if self.config.model_config.model_type != expected_type:\r\n-                raise ValueError(\r\n-                    f\"Model type {self.config.model_config.model_type} does not match \"\r\n-                    f\"forecast resolution {forecast_res} minutes. Expected {expected_type}\"\r\n-                )\r\n-\r\n-    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n-        \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n-        self.data_loader = TimeSeriesLoader(\r\n-            time_variable=self.config.dataset_config.time_variable,\r\n-            target_variable=self.config.dataset_config.target_variable\r\n-        )\r\n-        return self.data_loader\r\n-\r\n-    def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n-        \"\"\"Prepare train, validation and test datasets with resolution handling.\"\"\"\r\n-        if not self.data_loader:\r\n-            raise ValueError(\"Data loader not initialized\")\r\n-\r\n-        # Load and potentially resample data\r\n-        df = self.data_loader.load(self.config.data_path)\r\n-        \r\n-        # Resample if input and forecast resolutions differ\r\n-        if self.config.dataset_config.needs_resampling():\r\n-            df = self._resample_data(\r\n-                df,\r\n-                target_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n-            )\r\n-\r\n-        # Get time intervals adjusted for resolution\r\n-        train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n-\r\n-        # Split data\r\n-        train_df, val_df, test_df = self.data_loader.split(\r\n-            df,\r\n-            train_interval,\r\n-            val_interval,\r\n-            test_interval\r\n-        )\r\n-\r\n-        # Select appropriate dataset class\r\n-        dataset_cls = self._get_dataset_class()\r\n-\r\n-        # Create datasets\r\n-        train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n-        val_dataset = dataset_cls(\r\n-            val_df,\r\n-            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n-        )\r\n-        test_dataset = dataset_cls(\r\n-            test_df,\r\n-            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n-        )\r\n-\r\n-        return train_dataset, val_dataset, test_dataset\r\n-\r\n-    def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n-        \"\"\"Resample data to match the target resolution.\"\"\"\r\n-        if self.data_loader is None:\r\n-            raise ValueError(\"Data loader not initialized\")\r\n-        rule = f'{target_resolution_minutes}T'\r\n-        resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n-        return resampled.reset_index()\r\n-\r\n-    def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n-            return self._create_monthly_aligned_intervals()\r\n-        \r\n-        # For daily predictions, ensure intervals align with day boundaries\r\n-        elif self.config.dataset_config.forecast_resolution_minutes >= 1440:  # Daily\r\n-            return self._create_daily_aligned_intervals()\r\n-            \r\n-        # For hourly and sub-hourly, use standard intervals\r\n-        return self._create_time_intervals()\r\n-\r\n-    def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        \"\"\"Create monthly aligned intervals.\"\"\"\r\n-        train_start, train_end = self.config.train_dates\r\n-        val_start, val_end = self.config.val_dates\r\n-        test_start, test_end = self.config.test_dates\r\n-        \r\n-        return (\r\n-            TimeInterval(min_date=train_start, max_date=train_end),\r\n-            TimeInterval(min_date=val_start, max_date=val_end),\r\n-            TimeInterval(min_date=test_start, max_date=test_end)\r\n-        )\r\n-\r\n-    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        \"\"\"Create daily aligned intervals.\"\"\"\r\n-        train_start, train_end = self.config.train_dates\r\n-        val_start, val_end = self.config.val_dates\r\n-        test_start, test_end = self.config.test_dates\r\n-        \r\n-        return (\r\n-            TimeInterval(min_date=train_start, max_date=train_end),\r\n-            TimeInterval(min_date=val_start, max_date=val_end),\r\n-            TimeInterval(min_date=test_start, max_date=test_end)\r\n-        )\r\n-\r\n-    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n-        \"\"\"Create standard time intervals.\"\"\"\r\n-        return self._create_monthly_aligned_intervals()\r\n-\r\n-    def _get_dataset_class(self):\r\n-        \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n-        if self.config.model_config.model_type.is_transformer:\r\n-            if self.config.model_config.model_type.is_resolution_specific:\r\n-                # Use TransformerDataset with resolution-specific optimizations\r\n-                return TransformerDataset\r\n-            return TransformerDataset\r\n-        return StandardDataset\r\n-\r\n-    def setup_model(self) -> None:\r\n-        \"\"\"Setup model and associated wrapper with resolution awareness.\"\"\"\r\n-        # Convert config to dict for factory\r\n-        model_config_dict = dataclasses.asdict(self.config.model_config)\r\n-\r\n-        # Create model using factory\r\n-        model = ModelFactory.create_base_model(\r\n-            self.config.model_config.model_type,\r\n-            model_config_dict\r\n-        )\r\n-\r\n-        # Convert training config to dict\r\n-        training_config_dict = dataclasses.asdict(self.config.training_config)\r\n-\r\n-        # Create appropriate wrapper\r\n-        if self.config.model_config.model_type == ModelType.LINEAR_REGRESSION:\r\n-            wrapper: Union[SklearnWrapper, PyTorchWrapper] = SklearnWrapper(\r\n-                cast(Any, model),\r\n-                self.config.model_config.model_type,\r\n-                training_config_dict\r\n-            )\r\n-        else:\r\n-            wrapper = PyTorchWrapper(\r\n-                model,\r\n-                self.config.model_config.model_type,\r\n-                training_config_dict\r\n-            )\r\n-        \r\n-        self.model_wrapper = wrapper\r\n-\r\n-    def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n-        \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n-        if not self.model_wrapper:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-\r\n-        # Make predictions\r\n-        test_start = time.time()\r\n-        predictions, targets = self.model_wrapper.predict(test_dataset)\r\n-        self.test_time = time.time() - test_start\r\n-\r\n-        # Get timestamps\r\n-        timestamps = test_dataset.time_labels\r\n-        if timestamps is None:\r\n-            timestamps = np.arange(len(predictions))\r\n-\r\n-        # Create evaluator with resolution-aware configuration\r\n-        evaluator = Evaluator(\r\n-            scaler=self.config.dataset_config.time_series_scaler,\r\n-            metric_config=MetricConfig(),  # Add this if needed\r\n-            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n-        )\r\n-\r\n-        # Evaluate predictions\r\n-        evaluation = evaluator.evaluate(\r\n-            predictions=predictions,\r\n-            targets=targets,\r\n-            timestamps=cast(NDArray, timestamps),\r\n-            num_variables=1\r\n-        )\r\n-\r\n-        # Create experiment\r\n-        self.experiment = Experiment(\r\n-            model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n-            evaluation=evaluation,\r\n-            training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n-            training_report=self.training_report,\r\n-            training_time=self.training_time,\r\n-            test_time=self.test_time\r\n-        )\r\n-\r\n-        # Save model if path provided\r\n-        if self.config.model_save_path:\r\n-            self._save_model(self.config.model_save_path)\r\n-\r\n-    @property\r\n-    def training_report(self) -> TrainingReport:  # Change return type\r\n-        \"\"\"Get training report.\"\"\"\r\n-        if not hasattr(self, '_training_report'):\r\n-            self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n-        return self._training_report\r\n-\r\n-    def train_model(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Tuple[PyTorchWrapper, TrainingReport]:\r\n-        \"\"\"Train model implementation.\"\"\"\r\n-        if not self.model_wrapper:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-        if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n-            raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n-\r\n-        # Get device from model wrapper\r\n-        device = self.model_wrapper.device\r\n-        print(f\"\\nDEBUG: Training on device: {device}\")\r\n-\r\n-        # Initialize training report with metrics\r\n-        training_report = TrainingReport(\r\n-            train_losses=[],\r\n-            val_losses=[],\r\n-            learning_rates=[],\r\n-            epochs=0,\r\n-            additional_metrics={}\r\n-        )\r\n-        \r\n-        # Training loop\r\n-        for epoch in range(self.config.training_config.max_epochs):\r\n-            epoch_loss = 0.0\r\n-            num_batches = 0\r\n-            \r\n-            # Set model to training mode\r\n-            if hasattr(self.model_wrapper.model, 'train'):\r\n-                self.model_wrapper.model.train()\r\n-                \r\n-            # Process each batch\r\n-            for batch_input, batch_target in train_loader:\r\n-                # Move batch to correct device\r\n-                if isinstance(batch_input, tuple):\r\n-                    batch_input = tuple(b.to(device) for b in batch_input)\r\n-                else:\r\n-                    batch_input = batch_input.to(device)\r\n-                batch_target = batch_target.to(device)\r\n-                \r\n-                loss = self._process_training_batch(batch_input, batch_target)\r\n-                epoch_loss += loss\r\n-                num_batches += 1\r\n-\r\n-                \r\n-            # Process epoch results\r\n-            avg_epoch_loss = epoch_loss / num_batches\r\n-            training_report.train_losses.append(avg_epoch_loss)\r\n-            \r\n-            # Validation phase\r\n-            if val_loader:\r\n-                self._handle_validation(val_loader, training_report)\r\n-                if self.config.training_config.use_early_stopping and self._should_stop_early(training_report):\r\n-                    break\r\n-            \r\n-            # Log progress\r\n-            if (epoch + 1) % 10 == 0:\r\n-                print(f\"Epoch {epoch + 1}/{self.config.training_config.max_epochs}, \"\r\n-                      f\"Loss: {avg_epoch_loss:.4f}\")\r\n-        \r\n-        return self.model_wrapper, training_report\r\n-\r\n-    def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n-                            batch_target: torch.Tensor) -> float:\r\n-        \"\"\"Process a single training batch with device tracking.\"\"\"\r\n-        if self.model_wrapper is None:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-        \r\n-        try:\r\n-            if isinstance(batch_input, tuple):\r\n-                src, tgt = batch_input\r\n-                if self.config.model_config.model_type.is_transformer:\r\n-                    # Generate masks on correct device\r\n-                    src_mask = self.model_wrapper.model.generate_square_subsequent_mask(src.size(1)).to(src.device)\r\n-                    tgt_mask = self.model_wrapper.model.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\r\n-                    return self.model_wrapper.training_step((src, tgt), batch_target, src_mask=src_mask, tgt_mask=tgt_mask)\r\n-                batch_input = batch_input[0]\r\n-                \r\n-            return self.model_wrapper.training_step(batch_input, batch_target)\r\n-            \r\n-        except RuntimeError as e:\r\n-            print(f\"\\nERROR in training batch: {str(e)}\")\r\n-            if isinstance(batch_input, tuple):\r\n-                print(f\"Source device: {batch_input[0].device}, shape: {batch_input[0].shape}\")\r\n-                print(f\"Target input device: {batch_input[1].device}, shape: {batch_input[1].shape}\")\r\n-            else:\r\n-                print(f\"Input device: {batch_input.device}, shape: {batch_input.shape}\")\r\n-            print(f\"Target device: {batch_target.device}, shape: {batch_target.shape}\")\r\n-            print(f\"Model device: {next(self.model_wrapper.model.parameters()).device}\")\r\n-            raise\r\n-    \r\n-    def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n-        \"\"\"Handle validation phase of training.\"\"\"\r\n-        val_loss = self._validate_model(val_loader)\r\n-        if not hasattr(training_report, 'val_losses'):\r\n-            training_report.val_losses = []\r\n-        training_report.val_losses.append(val_loss)\r\n-\r\n-    def _validate_model(self, val_loader: DataLoader) -> float:\r\n-        \"\"\"Perform validation.\"\"\"\r\n-        if self.model_wrapper is None:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-        \r\n-        total_loss = 0.0\r\n-        num_batches = 0\r\n-        is_transformer = self.config.model_config.model_type.is_transformer\r\n-\r\n-        with torch.no_grad():\r\n-            for batch_input, batch_target in val_loader:\r\n-                if is_transformer:\r\n-                    if not isinstance(batch_input, tuple):\r\n-                        raise ValueError(\"Transformer models expect a tuple of (src, tgt) sequences\")\r\n-                    src, tgt = batch_input\r\n-                    loss = self.model_wrapper.validation_step((src, tgt), batch_target)\r\n-                else:\r\n-                    if isinstance(batch_input, tuple):\r\n-                        batch_input = batch_input[0]\r\n-                    loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n-                total_loss += loss\r\n-                num_batches += 1\r\n-        \r\n-        return total_loss / num_batches if num_batches > 0 else float('inf')\r\n-\r\n-\r\n-    def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n-        \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n-        X_list, y_list = [], []\r\n-        for batch_input, batch_target in val_loader:\r\n-            X_list.append(batch_input.numpy())\r\n-            y_list.append(batch_target.numpy())\r\n-        return np.vstack(X_list), np.vstack(y_list)\r\n-\r\n-    def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n-        \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n-        if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n-            return False\r\n-            \r\n-        patience = self.config.training_config.early_stopping_patience\r\n-        recent_losses = training_report.val_losses[-patience:]\r\n-        min_loss_idx = recent_losses.index(min(recent_losses))\r\n-        \r\n-        return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n-\r\n-    def training_step(\r\n-        self, \r\n-        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n-        batch_target: torch.Tensor\r\n-    ) -> float:\r\n-        \"\"\"Modified training step to handle transformer input format\"\"\"\r\n-        if self.model_wrapper is None:\r\n-            raise ValueError(\"Model wrapper not initialized\")\r\n-        \r\n-        if isinstance(batch_input, tuple):\r\n-            if self.config.model_config.model_type.is_transformer:\r\n-                src = batch_input[0]\r\n-                tgt = batch_target[:, :-1]\r\n-                tgt_y = batch_target[:, 1:]\r\n-                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n-            batch_input = batch_input[0]\r\n-            \r\n         return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733251477553,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,524 @@\n+# pipeline/implementations/time_series_pipeline.py\r\n+from __future__ import annotations\r\n+import time\r\n+from typing import Tuple, Optional, Dict, Any, cast, Union\r\n+from pathlib import Path\r\n+import torch\r\n+import dataclasses\r\n+from argparse import Namespace\r\n+import numpy as np\r\n+from numpy.typing import NDArray\r\n+import pandas as pd  # Add this import statement\r\n+from datetime import date  # Add this import statement\r\n+\r\n+from data_loading.datasets.standard_dataset import StandardDataset\r\n+from data_loading.datasets.transformer_dataset import TransformerDataset\r\n+from data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\n+from evaluation.evaluator import Evaluator\r\n+from evaluation.metrics import MetricConfig\r\n+from experiments.experiment import Experiment\r\n+from models.base.base_wrapper import BaseWrapper\r\n+from models.registry.factory import ModelFactory\r\n+from models.registry.model_types import ModelType\r\n+from models.wrappers.pytorch_wrapper import PyTorchWrapper\r\n+from models.wrappers.sklearn_wrapper import SklearnWrapper\r\n+from pipeline.base.base_pipeline import BasePipeline\r\n+from training.reports.training_report import TrainingReport\r\n+from torch.utils.data import DataLoader\r\n+from utils.logging.logger import Logger\r\n+from utils.logging.config import LoggerConfig, LogLevel\r\n+\r\n+class TimeSeriesPipeline(BasePipeline):\r\n+    \"\"\"Pipeline implementation for time series forecasting with resolution awareness.\"\"\"\r\n+\r\n+    def __init__(self, *args, **kwargs):\r\n+        super().__init__(*args, **kwargs)\r\n+        self.data_loader: Optional[TimeSeriesLoader] = None\r\n+        self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n+        \r\n+        # Initialize logger\r\n+        logger_config = LoggerConfig(\r\n+            level=LogLevel.INFO,\r\n+            component_name=\"TimeSeriesPipeline\",\r\n+            include_timestamp=True\r\n+        )\r\n+        self.logger = Logger.get_logger(__name__, logger_config)\r\n+        \r\n+        # Validate resolutions during initialization\r\n+        self._validate_resolutions()\r\n+        self._progress_callback = None\r\n+\r\n+    def add_progress_callback(self, callback):\r\n+        \"\"\"Add a callback function to monitor training progress.\r\n+        \r\n+        Args:\r\n+            callback: Function that takes (epoch, train_loss, val_loss) as parameters\r\n+        \"\"\"\r\n+        self._progress_callback = callback\r\n+\r\n+    def _validate_resolutions(self) -> None:\r\n+        \"\"\"Validate that resolutions are consistent across configurations.\"\"\"\r\n+        input_res = self.config.dataset_config.input_resolution_minutes\r\n+        forecast_res = self.config.dataset_config.forecast_resolution_minutes\r\n+        \r\n+        # Check model type matches resolution\r\n+        if self.config.model_config.model_type.is_resolution_specific:\r\n+            expected_type = ModelType.get_for_resolution(forecast_res)\r\n+            if self.config.model_config.model_type != expected_type:\r\n+                raise ValueError(\r\n+                    f\"Model type {self.config.model_config.model_type} does not match \"\r\n+                    f\"forecast resolution {forecast_res} minutes. Expected {expected_type}\"\r\n+                )\r\n+\r\n+    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n+        \"\"\"Initialize and configure the time series data loader with resolution awareness.\"\"\"\r\n+        self.data_loader = TimeSeriesLoader(\r\n+            time_variable=self.config.dataset_config.time_variable,\r\n+            target_variable=self.config.dataset_config.target_variable\r\n+        )\r\n+        return self.data_loader\r\n+\r\n+    def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n+        \"\"\"Prepare train, validation and test datasets with resolution handling.\"\"\"\r\n+        if not self.data_loader:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n+\r\n+        # Load and potentially resample data\r\n+        df = self.data_loader.load(self.config.data_path)\r\n+        \r\n+        # Resample if input and forecast resolutions differ\r\n+        if self.config.dataset_config.needs_resampling():\r\n+            df = self._resample_data(\r\n+                df,\r\n+                target_resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+            )\r\n+\r\n+        # Get time intervals adjusted for resolution\r\n+        train_interval, val_interval, test_interval = self._create_resolution_aware_intervals()\r\n+\r\n+        # Split data\r\n+        train_df, val_df, test_df = self.data_loader.split(\r\n+            df,\r\n+            train_interval,\r\n+            val_interval,\r\n+            test_interval\r\n+        )\r\n+\r\n+        # Select appropriate dataset class\r\n+        dataset_cls = self._get_dataset_class()\r\n+\r\n+        # Create datasets\r\n+        train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n+        val_dataset = dataset_cls(\r\n+            val_df,\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n+        )\r\n+        test_dataset = dataset_cls(\r\n+            test_df,\r\n+            dataclasses.replace(self.config.dataset_config, is_training_set=False)\r\n+        )\r\n+\r\n+        return train_dataset, val_dataset, test_dataset\r\n+\r\n+    def _resample_data(self, df: pd.DataFrame, target_resolution_minutes: int) -> pd.DataFrame:\r\n+        \"\"\"Resample data to match the target resolution.\"\"\"\r\n+        if self.data_loader is None:\r\n+            raise ValueError(\"Data loader not initialized\")\r\n+        rule = f'{target_resolution_minutes}T'\r\n+        resampled = df.resample(rule, on=self.data_loader.time_variable).mean()\r\n+        return resampled.reset_index()\r\n+\r\n+    def _create_resolution_aware_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        if self.config.dataset_config.forecast_resolution_minutes >= 43200:  # Monthly\r\n+            return self._create_monthly_aligned_intervals()\r\n+        \r\n+        # For daily predictions, ensure intervals align with day boundaries\r\n+        elif self.config.dataset_config.forecast_resolution_minutes >= 1440:  # Daily\r\n+            return self._create_daily_aligned_intervals()\r\n+            \r\n+        # For hourly and sub-hourly, use standard intervals\r\n+        return self._create_time_intervals()\r\n+\r\n+    def _create_monthly_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create monthly aligned intervals.\"\"\"\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_daily_aligned_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create daily aligned intervals.\"\"\"\r\n+        train_start, train_end = self.config.train_dates\r\n+        val_start, val_end = self.config.val_dates\r\n+        test_start, test_end = self.config.test_dates\r\n+        \r\n+        return (\r\n+            TimeInterval(min_date=train_start, max_date=train_end),\r\n+            TimeInterval(min_date=val_start, max_date=val_end),\r\n+            TimeInterval(min_date=test_start, max_date=test_end)\r\n+        )\r\n+\r\n+    def _create_time_intervals(self) -> Tuple[TimeInterval, TimeInterval, TimeInterval]:\r\n+        \"\"\"Create standard time intervals.\"\"\"\r\n+        return self._create_monthly_aligned_intervals()\r\n+\r\n+    def _get_dataset_class(self):\r\n+        \"\"\"Get appropriate dataset class based on model type and resolution.\"\"\"\r\n+        if self.config.model_config.model_type.is_transformer:\r\n+            if self.config.model_config.model_type.is_resolution_specific:\r\n+                # Use TransformerDataset with resolution-specific optimizations\r\n+                return TransformerDataset\r\n+            return TransformerDataset\r\n+        return StandardDataset\r\n+\r\n+    def setup_model(self) -> None:\r\n+        \"\"\"Setup model and associated wrapper with resolution awareness.\"\"\"\r\n+        # Convert config to dict for factory\r\n+        model_config_dict = dataclasses.asdict(self.config.model_config)\r\n+\r\n+        # Create model using factory\r\n+        model = ModelFactory.create_base_model(\r\n+            self.config.model_config.model_type,\r\n+            model_config_dict\r\n+        )\r\n+\r\n+        # Convert training config to dict\r\n+        training_config_dict = dataclasses.asdict(self.config.training_config)\r\n+\r\n+        # Create appropriate wrapper\r\n+        if self.config.model_config.model_type == ModelType.LINEAR_REGRESSION:\r\n+            wrapper: Union[SklearnWrapper, PyTorchWrapper] = SklearnWrapper(\r\n+                cast(Any, model),\r\n+                self.config.model_config.model_type,\r\n+                training_config_dict\r\n+            )\r\n+        else:\r\n+            wrapper = PyTorchWrapper(\r\n+                model,\r\n+                self.config.model_config.model_type,\r\n+                training_config_dict\r\n+            )\r\n+        \r\n+        self.model_wrapper = wrapper\r\n+\r\n+        self.logger.debug(\"Model setup complete\", {\r\n+            \"device_config\": training_config_dict.get('device', 'Not specified'),\r\n+            \"cuda_available\": torch.cuda.is_available(),\r\n+            \"cuda_device\": torch.cuda.current_device() if torch.cuda.is_available() else None,\r\n+            \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0\r\n+        })\r\n+\r\n+    def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n+        \"\"\"Evaluate model with resolution-aware metrics.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+\r\n+        # Make predictions\r\n+        test_start = time.time()\r\n+        predictions, targets = self.model_wrapper.predict(test_dataset)\r\n+        self.test_time = time.time() - test_start\r\n+\r\n+        # Get timestamps\r\n+        timestamps = test_dataset.time_labels\r\n+        if timestamps is None:\r\n+            timestamps = np.arange(len(predictions))\r\n+\r\n+        # Create evaluator with resolution-aware configuration\r\n+        evaluator = Evaluator(\r\n+            scaler=self.config.dataset_config.time_series_scaler,\r\n+            metric_config=MetricConfig(),  # Add this if needed\r\n+            resolution_minutes=self.config.dataset_config.forecast_resolution_minutes\r\n+        )\r\n+\r\n+        # Evaluate predictions\r\n+        evaluation = evaluator.evaluate(\r\n+            predictions=predictions,\r\n+            targets=targets,\r\n+            timestamps=cast(NDArray, timestamps),\r\n+            num_variables=1\r\n+        )\r\n+\r\n+        # Create experiment\r\n+        self.experiment = Experiment(\r\n+            model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n+            evaluation=evaluation,\r\n+            training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n+            training_report=self.training_report,\r\n+            training_time=self.training_time,\r\n+            test_time=self.test_time\r\n+        )\r\n+\r\n+        # Save model if path provided\r\n+        if self.config.model_save_path:\r\n+            self._save_model(self.config.model_save_path)\r\n+\r\n+    @property\r\n+    def training_report(self) -> TrainingReport:  # Change return type\r\n+        \"\"\"Get training report.\"\"\"\r\n+        if not hasattr(self, '_training_report'):\r\n+            self._training_report = TrainingReport(train_losses=[])  # Initialize with empty train_losses list\r\n+        return self._training_report\r\n+\r\n+    def train_model(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None) -> Tuple[PyTorchWrapper, TrainingReport]:\r\n+        \"\"\"Train model implementation.\"\"\"\r\n+        if not self.model_wrapper:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        if not isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            raise TypeError(\"Model wrapper must be PyTorchWrapper for time series training\")\r\n+\r\n+        # Get device from model wrapper\r\n+        device = self.model_wrapper.device\r\n+        self.logger.info(f\"Starting model training\", {\"device\": str(device)})\r\n+\r\n+        # Initialize training report with metrics\r\n+        training_report = TrainingReport(\r\n+            train_losses=[],\r\n+            val_losses=[],\r\n+            learning_rates=[],\r\n+            epochs=0,\r\n+            additional_metrics={}\r\n+        )\r\n+        \r\n+        self.logger.debug(\"CUDA memory status\", {\r\n+            \"memory_allocated_gb\": torch.cuda.memory_allocated() / 1e9,\r\n+            \"memory_cached_gb\": torch.cuda.memory_reserved() / 1e9\r\n+        })\r\n+        \r\n+        # Monitor device placement during training\r\n+        for epoch in range(self.config.training_config.max_epochs):\r\n+            if epoch == 0 or epoch % 10 == 0:  # Print every 10 epochs\r\n+                self.logger.debug(f\"Epoch status\", {\r\n+                    \"epoch\": epoch,\r\n+                    \"model_device\": str(next(self.model_wrapper.model.parameters()).device),\r\n+                    \"cuda_memory_gb\": torch.cuda.memory_allocated() / 1e9\r\n+                })\r\n+\r\n+            epoch_loss = 0.0\r\n+            num_batches = 0\r\n+            \r\n+            # Set model to training mode\r\n+            if hasattr(self.model_wrapper.model, 'train'):\r\n+                self.model_wrapper.model.train()\r\n+                \r\n+            # Process each batch\r\n+            for batch_input, batch_target in train_loader:\r\n+                # Move batch to correct device\r\n+                if isinstance(batch_input, tuple):\r\n+                    batch_input = tuple(b.to(device) for b in batch_input)\r\n+                else:\r\n+                    batch_input = batch_input.to(device)\r\n+                batch_target = batch_target.to(device)\r\n+                \r\n+                loss = self._process_training_batch(batch_input, batch_target)\r\n+                epoch_loss += loss\r\n+                num_batches += 1\r\n+\r\n+                \r\n+            # Process epoch results\r\n+            avg_epoch_loss = epoch_loss / num_batches\r\n+            training_report.train_losses.append(avg_epoch_loss)\r\n+            \r\n+            # Validation phase\r\n+            val_loss = None\r\n+            if val_loader:\r\n+                val_loss = self._handle_validation(val_loader, training_report)\r\n+                if self.config.training_config.use_early_stopping and self._should_stop_early(training_report):\r\n+                    break\r\n+            \r\n+            # Call progress callback if set\r\n+            if self._progress_callback:\r\n+                self._progress_callback(epoch, avg_epoch_loss, val_loss)\r\n+            \r\n+            # Log progress\r\n+            if (epoch + 1) % 10 == 0:\r\n+                self.logger.info(\"Training progress\", {\r\n+                    \"epoch\": epoch + 1,\r\n+                    \"total_epochs\": self.config.training_config.max_epochs,\r\n+                    \"loss\": float(avg_epoch_loss)\r\n+                })\r\n+        \r\n+        return self.model_wrapper, training_report\r\n+\r\n+    def _process_training_batch(self, batch_input: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], \r\n+                            batch_target: torch.Tensor) -> float:\r\n+        \"\"\"Process a single training batch with device tracking.\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        # Debug logging for device placement\r\n+        debug_info = {\r\n+            \"target_device\": str(batch_target.device),\r\n+            \"model_device\": str(next(self.model_wrapper.model.parameters()).device) if isinstance(self.model_wrapper, PyTorchWrapper) else \"N/A\"\r\n+        }\r\n+        \r\n+        if isinstance(batch_input, tuple):\r\n+            debug_info.update({\r\n+                \"source_input_device\": str(batch_input[0].device),\r\n+                \"target_input_device\": str(batch_input[1].device)\r\n+            })\r\n+        else:\r\n+            debug_info[\"input_device\"] = str(batch_input.device)\r\n+            \r\n+        self.logger.debug(\"Batch processing details\", debug_info)\r\n+        \r\n+        try:\r\n+            # Handle PyTorch models\r\n+            if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                if isinstance(batch_input, tuple):\r\n+                    src, tgt = batch_input\r\n+                    if self.config.model_config.model_type.is_transformer:\r\n+                        # Generate masks on correct device\r\n+                        device = next(self.model_wrapper.model.parameters()).device\r\n+                        src_mask = self.model_wrapper.model.generate_square_subsequent_mask(src.size(1)).to(device)\r\n+                        tgt_mask = self.model_wrapper.model.generate_square_subsequent_mask(tgt.size(1)).to(device)\r\n+                        return self.model_wrapper.training_step((src, tgt), batch_target, src_mask=src_mask, tgt_mask=tgt_mask)\r\n+                    batch_input = batch_input[0]\r\n+                return self.model_wrapper.training_step(batch_input, batch_target)\r\n+            \r\n+            # Handle sklearn models\r\n+            else:\r\n+                if isinstance(batch_input, tuple):\r\n+                    batch_input = batch_input[0]\r\n+                return self.model_wrapper.training_step(batch_input, batch_target)\r\n+                \r\n+        except RuntimeError as e:\r\n+            error_info = {\r\n+                \"error\": str(e),\r\n+                \"target_shape\": batch_target.shape,\r\n+                \"target_device\": str(batch_target.device)\r\n+            }\r\n+            \r\n+            if isinstance(batch_input, tuple):\r\n+                error_info.update({\r\n+                    \"source_shape\": batch_input[0].shape,\r\n+                    \"source_device\": str(batch_input[0].device),\r\n+                    \"target_input_shape\": batch_input[1].shape,\r\n+                    \"target_input_device\": str(batch_input[1].device)\r\n+                })\r\n+            else:\r\n+                error_info.update({\r\n+                    \"input_shape\": batch_input.shape,\r\n+                    \"input_device\": str(batch_input.device)\r\n+                })\r\n+                \r\n+            if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                error_info[\"model_device\"] = str(next(self.model_wrapper.model.parameters()).device)\r\n+                \r\n+            self.logger.error(\"Training batch processing failed\", error_info)\r\n+            raise\r\n+    \r\n+    def _handle_validation(self, val_loader: DataLoader, training_report: TrainingReport) -> None:\r\n+        \"\"\"Handle validation phase of training.\"\"\"\r\n+        val_loss = self._validate_model(val_loader)\r\n+        if not hasattr(training_report, 'val_losses'):\r\n+            training_report.val_losses = []\r\n+        training_report.val_losses.append(val_loss)\r\n+        \r\n+        self.logger.debug(\"Validation completed\", {\r\n+            \"validation_loss\": float(val_loss),\r\n+            \"epoch\": len(training_report.val_losses)\r\n+        })\r\n+\r\n+    def _validate_model(self, val_loader: DataLoader) -> float:\r\n+        \"\"\"Perform validation.\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        total_loss = 0.0\r\n+        num_batches = 0\r\n+        is_transformer = self.config.model_config.model_type.is_transformer\r\n+\r\n+        # Add validation device debugging\r\n+        debug_info = {}\r\n+        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+            debug_info[\"model_device\"] = str(next(self.model_wrapper.model.parameters()).device)\r\n+        \r\n+        self.logger.debug(\"Starting validation\", debug_info)\r\n+        \r\n+        for batch_input, batch_target in val_loader:\r\n+            if isinstance(batch_input, tuple):\r\n+                debug_info.update({\r\n+                    \"source_validation_device\": str(batch_input[0].device),\r\n+                    \"target_validation_device\": str(batch_input[1].device)\r\n+                })\r\n+            else:\r\n+                debug_info[\"input_validation_device\"] = str(batch_input.device)\r\n+            debug_info[\"target_validation_device\"] = str(batch_target.device)\r\n+            break  # Only print first batch\r\n+\r\n+        self.logger.debug(\"Validation batch devices\", debug_info)\r\n+\r\n+        with torch.no_grad():\r\n+            for batch_input, batch_target in val_loader:\r\n+                # Handle PyTorch models\r\n+                if isinstance(self.model_wrapper, PyTorchWrapper):\r\n+                    if is_transformer:\r\n+                        if not isinstance(batch_input, tuple):\r\n+                            raise ValueError(\"Transformer models expect a tuple of (src, tgt) sequences\")\r\n+                        src, tgt = batch_input\r\n+                        device = next(self.model_wrapper.model.parameters()).device\r\n+                        src = src.to(device)\r\n+                        tgt = tgt.to(device)\r\n+                        batch_target = batch_target.to(device)\r\n+                        loss = self.model_wrapper.validation_step((src, tgt), batch_target)\r\n+                    else:\r\n+                        if isinstance(batch_input, tuple):\r\n+                            batch_input = batch_input[0]\r\n+                        device = next(self.model_wrapper.model.parameters()).device\r\n+                        batch_input = batch_input.to(device)\r\n+                        batch_target = batch_target.to(device)\r\n+                        loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                # Handle sklearn models\r\n+                else:\r\n+                    if isinstance(batch_input, tuple):\r\n+                        batch_input = batch_input[0]\r\n+                    loss = self.model_wrapper.validation_step(batch_input, batch_target)\r\n+                    \r\n+                total_loss += loss\r\n+                num_batches += 1\r\n+        \r\n+        return total_loss / num_batches if num_batches > 0 else float('inf')\r\n+\r\n+\r\n+    def _prepare_validation_data(self, val_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:\r\n+        \"\"\"Prepare validation data for sklearn models.\"\"\"\r\n+        X_list, y_list = [], []\r\n+        for batch_input, batch_target in val_loader:\r\n+            X_list.append(batch_input.numpy())\r\n+            y_list.append(batch_target.numpy())\r\n+        return np.vstack(X_list), np.vstack(y_list)\r\n+\r\n+    def _should_stop_early(self, training_report: TrainingReport) -> bool:\r\n+        \"\"\"Check if training should stop based on validation performance.\"\"\"\r\n+        if len(training_report.val_losses) < self.config.training_config.early_stopping_patience:\r\n+            return False\r\n+            \r\n+        patience = self.config.training_config.early_stopping_patience\r\n+        recent_losses = training_report.val_losses[-patience:]\r\n+        min_loss_idx = recent_losses.index(min(recent_losses))\r\n+        \r\n+        return min_loss_idx == 0  # Stop if best loss was patience steps ago\r\n+\r\n+    def training_step(\r\n+        self, \r\n+        batch_input: Union[torch.Tensor, Tuple[torch.Tensor, ...]], \r\n+        batch_target: torch.Tensor\r\n+    ) -> float:\r\n+        \"\"\"Modified training step to handle transformer input format\"\"\"\r\n+        if self.model_wrapper is None:\r\n+            raise ValueError(\"Model wrapper not initialized\")\r\n+        \r\n+        if isinstance(batch_input, tuple):\r\n+            if self.config.model_config.model_type.is_transformer:\r\n+                src = batch_input[0]\r\n+                tgt = batch_target[:, :-1]\r\n+                tgt_y = batch_target[:, 1:]\r\n+                return cast(float, self.model_wrapper.training_step((src, tgt), tgt_y))\r\n+            batch_input = batch_input[0]\r\n+            \r\n+        return cast(float, self.model_wrapper.training_step(batch_input, batch_target))\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733091623393,
            "name": "Commit-0",
            "content": "from __future__ import annotations\r\n\r\nimport time\r\nfrom typing import Tuple, Optional, Dict, Any, cast, Union\r\nfrom pathlib import Path\r\nimport torch\r\nimport dataclasses\r\nfrom argparse import Namespace\r\nimport numpy as np\r\nfrom numpy.typing import NDArray\r\n\r\nfrom data_loading.datasets.standard_dataset import StandardDataset\r\nfrom data_loading.datasets.transformer_dataset import TransformerDataset\r\nfrom data_loading.loaders.time_series_loader import TimeSeriesLoader, TimeInterval\r\nfrom evaluation.evaluator import Evaluator\r\nfrom experiments.experiment import Experiment\r\nfrom models.base.base_wrapper import BaseWrapper\r\nfrom models.registry.factory import ModelFactory\r\nfrom models.registry.model_types import ModelType\r\nfrom models.wrappers.pytorch_wrapper import PyTorchWrapper\r\nfrom models.wrappers.sklearn_wrapper import SklearnWrapper\r\nfrom pipeline.base.base_pipeline import BasePipeline\r\n\r\nclass TimeSeriesPipeline(BasePipeline):\r\n    def __init__(self, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.data_loader: Optional[TimeSeriesLoader] = None\r\n        self.model_wrapper: Optional[Union[SklearnWrapper, PyTorchWrapper]] = None\r\n\r\n    def prepare_data_loader(self) -> TimeSeriesLoader:\r\n        \"\"\"Initialize and configure the time series data loader.\"\"\"\r\n        self.data_loader = TimeSeriesLoader(\r\n            time_variable=self.config.dataset_config.time_variable,\r\n            target_variable=self.config.dataset_config.target_variable\r\n        )\r\n        return self.data_loader\r\n\r\n    def prepare_datasets(self) -> Tuple[StandardDataset | TransformerDataset, ...]:\r\n        \"\"\"Prepare train, validation and test datasets.\"\"\"\r\n        if not self.data_loader:\r\n            raise ValueError(\"Data loader not initialized\")\r\n\r\n        # Load data\r\n        df = self.data_loader.load(self.config.data_path)\r\n\r\n        # Get time intervals\r\n        train_interval, val_interval, test_interval = self._create_time_intervals()\r\n\r\n        # Split data\r\n        train_df, val_df, test_df = self.data_loader.split(\r\n            df,  # Add df as first argument\r\n            train_interval,\r\n            val_interval,\r\n            test_interval\r\n        )\r\n\r\n        # Create appropriate dataset types based on model\r\n        dataset_cls = (\r\n            TransformerDataset\r\n            if self.config.model_config.model_type.is_transformer\r\n            else StandardDataset\r\n        )\r\n\r\n        # Create datasets\r\n        train_dataset = dataset_cls(train_df, self.config.dataset_config)\r\n        val_dataset = dataset_cls(\r\n            val_df,\r\n            dataclasses.replace(\r\n                self.config.dataset_config,\r\n                is_training_set=False\r\n            )\r\n        )\r\n        test_dataset = dataset_cls(\r\n            test_df,\r\n            dataclasses.replace(\r\n                self.config.dataset_config,\r\n                is_training_set=False\r\n            )\r\n        )\r\n\r\n        return train_dataset, val_dataset, test_dataset\r\n\r\n    def setup_model(self) -> None:\r\n        \"\"\"Setup model and associated wrapper.\"\"\"\r\n        # Convert config to dict for factory\r\n        model_config_dict = dataclasses.asdict(self.config.model_config)\r\n\r\n        # Create model using factory\r\n        model = ModelFactory.create_base_model(\r\n            self.config.model_config.model_type,\r\n            model_config_dict\r\n        )\r\n\r\n        # Convert training config to dict\r\n        training_config_dict = dataclasses.asdict(self.config.training_config)\r\n\r\n        # Create appropriate wrapper\r\n        if self.config.model_config.model_type == ModelType.LINEAR_REGRESSION:\r\n            wrapper: Union[SklearnWrapper, PyTorchWrapper] = SklearnWrapper(\r\n                cast(Any, model),\r\n                self.config.model_config.model_type,\r\n                training_config_dict\r\n            )\r\n        else:\r\n            wrapper = PyTorchWrapper(\r\n                model,\r\n                self.config.model_config.model_type,\r\n                training_config_dict\r\n            )\r\n        \r\n        # Assign wrapper with proper type annotation\r\n        self.model_wrapper = wrapper\r\n\r\n    def train_model(self, train_dataset: StandardDataset | TransformerDataset,\r\n                    val_dataset: StandardDataset | TransformerDataset) -> None:\r\n        \"\"\"Train the model and record metrics.\"\"\"\r\n        if not self.model_wrapper:\r\n            raise ValueError(\"Model wrapper not initialized\")\r\n\r\n        training_start = time.time()\r\n\r\n        self.training_report = self.model_wrapper.train(\r\n            train_dataset,\r\n            val_dataset\r\n        )\r\n\r\n        self.training_time = time.time() - training_start\r\n\r\n    def evaluate_model(self, test_dataset: StandardDataset | TransformerDataset) -> None:\r\n        \"\"\"Evaluate model and create experiment.\"\"\"\r\n        if not self.model_wrapper:\r\n            raise ValueError(\"Model wrapper not initialized\")\r\n\r\n        # Make predictions\r\n        test_start = time.time()\r\n        predictions, targets = self.model_wrapper.predict(test_dataset)\r\n        self.test_time = time.time() - test_start\r\n\r\n        # Get timestamps with null check\r\n        timestamps = test_dataset.time_labels\r\n        if timestamps is None:\r\n            timestamps = np.arange(len(predictions))  # Fallback to indices if no timestamps\r\n\r\n        # Create evaluator with the dataset's scaler\r\n        evaluator = Evaluator(\r\n            scaler=self.config.dataset_config.time_series_scaler\r\n        )\r\n\r\n        # Evaluate predictions\r\n        evaluation = evaluator.evaluate(\r\n            predictions=predictions,\r\n            targets=targets,\r\n            timestamps=cast(NDArray, timestamps),  # Cast to satisfy type checker\r\n            num_variables=1  # Assuming single variable prediction\r\n        )\r\n\r\n        # Create experiment with type-safe arguments\r\n        self.experiment = Experiment(\r\n            model_wrapper=cast(BaseWrapper, self.model_wrapper),\r\n            evaluation=evaluation,\r\n            training_config=Namespace(**dataclasses.asdict(self.config.training_config)),\r\n            training_report=self.training_report,\r\n            training_time=self.training_time,\r\n            test_time=self.test_time\r\n        )\r\n\r\n        # Save model if path provided\r\n        if self.config.model_save_path:\r\n            self._save_model(str(self.config.model_save_path))\r\n\r\n    def _save_model(self, save_path: str) -> None:\r\n        \"\"\"Save model artifacts.\"\"\"\r\n        if not self.model_wrapper:\r\n            raise ValueError(\"Model wrapper not initialized\")\r\n\r\n        # Save PyTorch model\r\n        if isinstance(self.model_wrapper, PyTorchWrapper):\r\n            if hasattr(self.model_wrapper.model, 'state_dict'):\r\n                torch.save(\r\n                    self.model_wrapper.model.state_dict(),\r\n                    f\"{save_path}/model.pt\"\r\n                )\r\n\r\n            # Save scaler if exists\r\n            if self.config.dataset_config.time_series_scaler:\r\n                import pickle\r\n                with open(f\"{save_path}/scaler.pkl\", \"wb\") as f:\r\n                    pickle.dump(\r\n                        self.config.dataset_config.time_series_scaler,\r\n                        f\r\n                    )"
        }
    ]
}