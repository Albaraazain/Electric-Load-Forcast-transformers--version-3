{
    "sourceFile": "pipeline/utils/config_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 8,
            "patches": [
                {
                    "date": 1733004655385,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733091500366,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,8 +10,27 @@\n \r\n from ..config.pipeline_config import PipelineConfig\r\n from training.config import TrainingConfig, TransformerTrainingConfig  # Add import\r\n \r\n+from __future__ import annotations\r\n+\r\n+from pathlib import Path\r\n+from typing import Dict, Any\r\n+\r\n+from data_loading.base.base_dataset import DatasetConfig\r\n+from models.config.model_config import ModelConfig\r\n+from models.registry.model_types import ModelType\r\n+from ..config.pipeline_config import PipelineConfig\r\n+from training.config import TrainingConfig, TransformerTrainingConfig\r\n+\r\n+\r\n+def filter_config_params(params: Dict[str, Any], config_class: Any) -> Dict[str, Any]:\r\n+    \"\"\"Filter configuration parameters to match the target class fields.\"\"\"\r\n+    return {\r\n+        k: v for k, v in params.items() \r\n+        if k in config_class.__dataclass_fields__  # type: ignore\r\n+    }\r\n+\r\n def create_pipeline_config(\r\n         data_path: str | Path,\r\n         model_type: ModelType,\r\n         model_params: Dict[str, Any],\r\n@@ -20,24 +39,28 @@\n ) -> PipelineConfig:\r\n     \"\"\"Create a pipeline configuration from parameters.\"\"\"\r\n \r\n     # Create dataset config\r\n-    dataset_config = DatasetConfig(**dataset_params)\r\n+    filtered_dataset_params = filter_config_params(dataset_params, DatasetConfig)\r\n+    dataset_config = DatasetConfig(**filtered_dataset_params)\r\n \r\n     # Create model config\r\n+    filtered_model_params = filter_config_params(model_params, ModelConfig)\r\n     model_config = ModelConfig(\r\n-        model_type=model_type,  # Pass ModelType directly\r\n-        **model_params\r\n+        model_type=model_type,\r\n+        **filtered_model_params\r\n     )\r\n \r\n     # Choose appropriate training config based on model type\r\n     if model_type.is_transformer:\r\n-        training_config = TransformerTrainingConfig(**training_params)\r\n+        filtered_training_params = filter_config_params(training_params, TransformerTrainingConfig)\r\n+        training_config = TransformerTrainingConfig(**filtered_training_params)\r\n     else:\r\n         # Remove transformer-specific parameters for non-transformer models\r\n         base_params = {k: v for k, v in training_params.items() \r\n                     if not k.startswith('transformer_')}\r\n-        training_config = TrainingConfig(**base_params)\r\n+        filtered_base_params = filter_config_params(base_params, TrainingConfig)\r\n+        training_config = TrainingConfig(**filtered_base_params)\r\n \r\n     # Create pipeline config\r\n     return PipelineConfig(\r\n         dataset_config=dataset_config,\r\n"
                },
                {
                    "date": 1733091517101,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,20 +10,9 @@\n \r\n from ..config.pipeline_config import PipelineConfig\r\n from training.config import TrainingConfig, TransformerTrainingConfig  # Add import\r\n \r\n-from __future__ import annotations\r\n \r\n-from pathlib import Path\r\n-from typing import Dict, Any\r\n-\r\n-from data_loading.base.base_dataset import DatasetConfig\r\n-from models.config.model_config import ModelConfig\r\n-from models.registry.model_types import ModelType\r\n-from ..config.pipeline_config import PipelineConfig\r\n-from training.config import TrainingConfig, TransformerTrainingConfig\r\n-\r\n-\r\n def filter_config_params(params: Dict[str, Any], config_class: Any) -> Dict[str, Any]:\r\n     \"\"\"Filter configuration parameters to match the target class fields.\"\"\"\r\n     return {\r\n         k: v for k, v in params.items() \r\n"
                },
                {
                    "date": 1733176878011,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,161 @@\n+# pipeline/utils/config_utils.py\r\n+from __future__ import annotations\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Optional, Tuple\r\n+\r\n+from data_loading.base.base_dataset import DatasetConfig\r\n+from models.config.model_config import ModelConfig\r\n+from models.registry.model_types import ModelType\r\n+from ..config.pipeline_config import PipelineConfig\r\n+from training.config import (\r\n+    TrainingConfig,\r\n+    TransformerTrainingConfig,\r\n+    ResolutionBasedTrainingConfig\r\n+)\r\n+\r\n+def get_resolution_parameters(\r\n+    input_resolution: int,\r\n+    forecast_resolution: int\r\n+) -> Dict[str, Any]:\r\n+    \"\"\"\r\n+    Calculate appropriate parameters based on input and forecast resolutions.\r\n+    This helps ensure consistent configuration across different components.\r\n+    \"\"\"\r\n+    # Calculate points per period for different time scales\r\n+    points_per_hour = 60 // min(input_resolution, 60)\r\n+    points_per_day = 24 * points_per_hour\r\n+    \r\n+    # Determine appropriate window sizes based on resolution\r\n+    if forecast_resolution <= 60:  # Hourly or sub-hourly\r\n+        lookback_hours = 24  # One day of history\r\n+        forecast_hours = 12  # 12 hours ahead\r\n+    elif forecast_resolution <= 1440:  # Daily\r\n+        lookback_hours = 168  # One week of history\r\n+        forecast_hours = 72  # 3 days ahead\r\n+    else:  # Monthly\r\n+        lookback_hours = 720  # 30 days of history\r\n+        forecast_hours = 168  # 1 week ahead\r\n+\r\n+    return {\r\n+        'input_resolution_minutes': input_resolution,\r\n+        'forecast_resolution_minutes': forecast_resolution,\r\n+        'lookback_periods': lookback_hours * (60 // input_resolution),\r\n+        'forecast_periods': forecast_hours * (60 // forecast_resolution),\r\n+        'points_per_hour': points_per_hour,\r\n+    }\r\n+\r\n+def create_pipeline_config(\r\n+    data_path: str | Path,\r\n+    model_type: ModelType,\r\n+    model_params: Dict[str, Any],\r\n+    training_params: Dict[str, Any],\r\n+    dataset_params: Dict[str, Any]\r\n+) -> PipelineConfig:\r\n+    \"\"\"\r\n+    Create a pipeline configuration with enhanced support for different time resolutions.\r\n+    This function ensures consistent resolution handling across all components.\r\n+    \"\"\"\r\n+    # Extract resolution parameters\r\n+    input_resolution = dataset_params.get('input_resolution_minutes', 15)\r\n+    forecast_resolution = dataset_params.get('forecast_resolution_minutes', 15)\r\n+    \r\n+    # Get resolution-specific parameters\r\n+    resolution_params = get_resolution_parameters(input_resolution, forecast_resolution)\r\n+    \r\n+    # Update all parameter dictionaries with resolution information\r\n+    dataset_params.update(resolution_params)\r\n+    model_params.update(resolution_params)\r\n+    training_params.update(resolution_params)\r\n+\r\n+    # Create dataset configuration\r\n+    filtered_dataset_params = filter_config_params(dataset_params, DatasetConfig)\r\n+    dataset_config = DatasetConfig(**filtered_dataset_params)\r\n+\r\n+    # Create model configuration with resolution awareness\r\n+    filtered_model_params = filter_config_params(model_params, ModelConfig)\r\n+    if model_type.is_resolution_specific:\r\n+        # Let the model type adjust itself based on resolution\r\n+        model_type = ModelType.get_for_resolution(forecast_resolution)\r\n+    \r\n+    model_config = ModelConfig(\r\n+        model_type=model_type,\r\n+        **filtered_model_params\r\n+    )\r\n+\r\n+    # Create appropriate training configuration\r\n+    training_config = create_training_config(\r\n+        model_type=model_type,\r\n+        params=training_params,\r\n+        input_resolution=input_resolution,\r\n+        forecast_resolution=forecast_resolution\r\n+    )\r\n+\r\n+    # Create and return the complete pipeline configuration\r\n+    return PipelineConfig(\r\n+        dataset_config=dataset_config,\r\n+        model_config=model_config,\r\n+        training_config=training_config,\r\n+        data_path=Path(data_path),\r\n+        input_resolution_minutes=input_resolution,\r\n+        forecast_resolution_minutes=forecast_resolution\r\n+    )\r\n+\r\n+def create_training_config(\r\n+    model_type: ModelType,\r\n+    params: Dict[str, Any],\r\n+    input_resolution: int,\r\n+    forecast_resolution: int\r\n+) -> TrainingConfig:\r\n+    \"\"\"\r\n+    Create an appropriate training configuration based on model type and resolution.\r\n+    This function handles the complexity of choosing and configuring the right training setup.\r\n+    \"\"\"\r\n+    # Adjust batch size based on resolution\r\n+    if 'batch_size' not in params:\r\n+        params['batch_size'] = _get_default_batch_size(forecast_resolution)\r\n+\r\n+    # Adjust learning rate based on resolution\r\n+    if 'learning_rate' not in params:\r\n+        params['learning_rate'] = _get_default_learning_rate(forecast_resolution)\r\n+\r\n+    if model_type.is_transformer:\r\n+        # For transformer models, use resolution-aware transformer config\r\n+        filtered_params = filter_config_params(params, TransformerTrainingConfig)\r\n+        return TransformerTrainingConfig(\r\n+            input_resolution_minutes=input_resolution,\r\n+            forecast_resolution_minutes=forecast_resolution,\r\n+            **filtered_params\r\n+        )\r\n+    else:\r\n+        # For other models, use base training config\r\n+        filtered_params = filter_config_params(params, TrainingConfig)\r\n+        return TrainingConfig(**filtered_params)\r\n+\r\n+def _get_default_batch_size(resolution: int) -> int:\r\n+    \"\"\"Calculate appropriate batch size based on resolution.\"\"\"\r\n+    if resolution <= 15:\r\n+        return 64  # Larger batches for high-frequency data\r\n+    elif resolution <= 60:\r\n+        return 32  # Medium batches for hourly data\r\n+    elif resolution <= 1440:\r\n+        return 16  # Smaller batches for daily data\r\n+    else:\r\n+        return 8   # Smallest batches for monthly data\r\n+\r\n+def _get_default_learning_rate(resolution: int) -> float:\r\n+    \"\"\"Calculate appropriate learning rate based on resolution.\"\"\"\r\n+    if resolution <= 15:\r\n+        return 0.001  # Smaller learning rate for high-frequency data\r\n+    elif resolution <= 60:\r\n+        return 0.002  # Medium learning rate for hourly data\r\n+    elif resolution <= 1440:\r\n+        return 0.005  # Larger learning rate for daily data\r\n+    else:\r\n+        return 0.01  # Largest learning rate for monthly data\r\n+\r\n+def filter_config_params(params: Dict[str, Any], config_class: Any) -> Dict[str, Any]:\r\n+    \"\"\"Filter configuration parameters to match the target class fields.\"\"\"\r\n+    return {\r\n+        k: v for k, v in params.items() \r\n+        if k in config_class.__dataclass_fields__  # type: ignore\r\n+    }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733177657483,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,10 +5,10 @@\n \r\n from data_loading.base.base_dataset import DatasetConfig\r\n from models.config.model_config import ModelConfig\r\n from models.registry.model_types import ModelType\r\n-from ..config.pipeline_config import PipelineConfig\r\n-from training.config import (\r\n+from pipeline.config.pipeline_config import PipelineConfig  # Fix import path\r\n+from training.config.training_config import (  # Fix import path\r\n     TrainingConfig,\r\n     TransformerTrainingConfig,\r\n     ResolutionBasedTrainingConfig\r\n )\r\n@@ -157,64 +157,5 @@\n     \"\"\"Filter configuration parameters to match the target class fields.\"\"\"\r\n     return {\r\n         k: v for k, v in params.items() \r\n         if k in config_class.__dataclass_fields__  # type: ignore\r\n-    }\n-# pipeline/utils/config_utils.py\r\n-from __future__ import annotations\r\n-\r\n-from pathlib import Path\r\n-from typing import Dict, Any\r\n-\r\n-from data_loading.base.base_dataset import DatasetConfig\r\n-from models.config.model_config import ModelConfig\r\n-from models.registry.model_types import ModelType\r\n-\r\n-from ..config.pipeline_config import PipelineConfig\r\n-from training.config import TrainingConfig, TransformerTrainingConfig  # Add import\r\n-\r\n-\r\n-def filter_config_params(params: Dict[str, Any], config_class: Any) -> Dict[str, Any]:\r\n-    \"\"\"Filter configuration parameters to match the target class fields.\"\"\"\r\n-    return {\r\n-        k: v for k, v in params.items() \r\n-        if k in config_class.__dataclass_fields__  # type: ignore\r\n-    }\r\n-\r\n-def create_pipeline_config(\r\n-        data_path: str | Path,\r\n-        model_type: ModelType,\r\n-        model_params: Dict[str, Any],\r\n-        training_params: Dict[str, Any],\r\n-        dataset_params: Dict[str, Any]\r\n-) -> PipelineConfig:\r\n-    \"\"\"Create a pipeline configuration from parameters.\"\"\"\r\n-\r\n-    # Create dataset config\r\n-    filtered_dataset_params = filter_config_params(dataset_params, DatasetConfig)\r\n-    dataset_config = DatasetConfig(**filtered_dataset_params)\r\n-\r\n-    # Create model config\r\n-    filtered_model_params = filter_config_params(model_params, ModelConfig)\r\n-    model_config = ModelConfig(\r\n-        model_type=model_type,\r\n-        **filtered_model_params\r\n-    )\r\n-\r\n-    # Choose appropriate training config based on model type\r\n-    if model_type.is_transformer:\r\n-        filtered_training_params = filter_config_params(training_params, TransformerTrainingConfig)\r\n-        training_config = TransformerTrainingConfig(**filtered_training_params)\r\n-    else:\r\n-        # Remove transformer-specific parameters for non-transformer models\r\n-        base_params = {k: v for k, v in training_params.items() \r\n-                    if not k.startswith('transformer_')}\r\n-        filtered_base_params = filter_config_params(base_params, TrainingConfig)\r\n-        training_config = TrainingConfig(**filtered_base_params)\r\n-\r\n-    # Create pipeline config\r\n-    return PipelineConfig(\r\n-        dataset_config=dataset_config,\r\n-        model_config=model_config,\r\n-        training_config=training_config,\r\n-        data_path=Path(data_path)\r\n-    )\n\\ No newline at end of file\n+    }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733179305193,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n from models.config.model_config import ModelConfig\r\n from models.registry.model_types import ModelType\r\n from pipeline.config.pipeline_config import PipelineConfig  # Fix import path\r\n from training.config.training_config import (  # Fix import path\r\n-    TrainingConfig,\r\n+    BaseConfig,\r\n     TransformerTrainingConfig,\r\n     ResolutionBasedTrainingConfig\r\n )\r\n \r\n@@ -104,9 +104,9 @@\n     model_type: ModelType,\r\n     params: Dict[str, Any],\r\n     input_resolution: int,\r\n     forecast_resolution: int\r\n-) -> TrainingConfig:\r\n+) -> BaseConfig:  # Changed return type to BaseConfig\r\n     \"\"\"\r\n     Create an appropriate training configuration based on model type and resolution.\r\n     This function handles the complexity of choosing and configuring the right training setup.\r\n     \"\"\"\r\n@@ -126,11 +126,11 @@\n             forecast_resolution_minutes=forecast_resolution,\r\n             **filtered_params\r\n         )\r\n     else:\r\n-        # For other models, use base training config\r\n-        filtered_params = filter_config_params(params, TrainingConfig)\r\n-        return TrainingConfig(**filtered_params)\r\n+        # For other models, use base config\r\n+        filtered_params = filter_config_params(params, BaseConfig)  # Changed to BaseConfig\r\n+        return BaseConfig(**filtered_params)  # Changed to BaseConfig\r\n \r\n def _get_default_batch_size(resolution: int) -> int:\r\n     \"\"\"Calculate appropriate batch size based on resolution.\"\"\"\r\n     if resolution <= 15:\r\n"
                },
                {
                    "date": 1733180196489,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,141 @@\n+# pipeline/utils/config_utils.py\r\n+from __future__ import annotations\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Optional, Tuple\r\n+\r\n+from data_loading.base.base_dataset import DatasetConfig\r\n+from models.config.model_config import ModelConfig\r\n+from models.registry.model_types import ModelType\r\n+from pipeline.config.pipeline_config import PipelineConfig  # Fix import path\r\n+from training.config.training_config import (  # Fix import path\r\n+    BaseConfig,\r\n+    TransformerTrainingConfig,\r\n+    ResolutionBasedTrainingConfig\r\n+)\r\n+\r\n+def get_resolution_parameters(\r\n+    input_resolution: int,\r\n+    forecast_resolution: int\r\n+) -> Dict[str, Any]:\r\n+    \"\"\"\r\n+    Calculate appropriate parameters based on input and forecast resolutions.\r\n+    This helps ensure consistent configuration across different components.\r\n+    \"\"\"\r\n+    # Calculate points per period for different time scales\r\n+    points_per_hour = 60 // min(input_resolution, 60)\r\n+    points_per_day = 24 * points_per_hour\r\n+    \r\n+    # Determine appropriate window sizes based on resolution\r\n+    if forecast_resolution <= 60:  # Hourly or sub-hourly\r\n+        lookback_hours = 24  # One day of history\r\n+        forecast_hours = 12  # 12 hours ahead\r\n+    elif forecast_resolution <= 1440:  # Daily\r\n+        lookback_hours = 168  # One week of history\r\n+        forecast_hours = 72  # 3 days ahead\r\n+    else:  # Monthly\r\n+        lookback_hours = 720  # 30 days of history\r\n+        forecast_hours = 168  # 1 week ahead\r\n+\r\n+    return {\r\n+        'input_resolution_minutes': input_resolution,\r\n+        'forecast_resolution_minutes': forecast_resolution,\r\n+        'lookback_periods': lookback_hours * (60 // input_resolution),\r\n+        'forecast_periods': forecast_hours * (60 // forecast_resolution),\r\n+        'points_per_hour': points_per_hour,\r\n+    }\r\n+\r\n+def create_pipeline_config(\r\n+    data_path: str | Path,\r\n+    model_type: ModelType,\r\n+    model_params: Dict[str, Any],\r\n+    training_params: Dict[str, Any],\r\n+    dataset_params: Dict[str, Any]\r\n+) -> PipelineConfig:\r\n+    \"\"\"Create pipeline configuration.\"\"\"\r\n+    # Extract resolution parameters from dataset_params\r\n+    input_resolution = dataset_params['input_resolution_minutes']\r\n+    forecast_resolution = dataset_params['forecast_resolution_minutes']\r\n+    \r\n+    # Create configs\r\n+    dataset_config = DatasetConfig(**dataset_params)\r\n+    model_config = ModelConfig(**model_params)\r\n+    training_config = create_training_config(\r\n+        model_type=model_type,\r\n+        params=training_params,\r\n+        input_resolution=input_resolution,\r\n+        forecast_resolution=forecast_resolution\r\n+    )\r\n+    \r\n+    return PipelineConfig(\r\n+        dataset_config=dataset_config,\r\n+        model_config=model_config,\r\n+        training_config=training_config,\r\n+        data_path=Path(data_path),\r\n+        input_resolution_minutes=input_resolution,\r\n+        forecast_resolution_minutes=forecast_resolution\r\n+    )\r\n+\r\n+def create_training_config(\r\n+    model_type: ModelType,\r\n+    params: Dict[str, Any],\r\n+    input_resolution: int,\r\n+    forecast_resolution: int\r\n+) -> BaseConfig:  # Changed return type to BaseConfig\r\n+    \"\"\"\r\n+    Create an appropriate training configuration based on model type and resolution.\r\n+    This function handles the complexity of choosing and configuring the right training setup.\r\n+    \"\"\"\r\n+    # Remove resolution parameters from params if they exist\r\n+    filtered_params = params.copy()\r\n+    filtered_params.pop('input_resolution_minutes', None)\r\n+    filtered_params.pop('forecast_resolution_minutes', None)\r\n+    \r\n+    # Adjust batch size based on resolution\r\n+    if 'batch_size' not in filtered_params:\r\n+        filtered_params['batch_size'] = _get_default_batch_size(forecast_resolution)\r\n+\r\n+    # Adjust learning rate based on resolution\r\n+    if 'learning_rate' not in filtered_params:\r\n+        filtered_params['learning_rate'] = _get_default_learning_rate(forecast_resolution)\r\n+\r\n+    if model_type.is_transformer:\r\n+        # For transformer models, use resolution-aware transformer config\r\n+        filtered_params = filter_config_params(filtered_params, TransformerTrainingConfig)\r\n+        return TransformerTrainingConfig(\r\n+            input_resolution_minutes=input_resolution,\r\n+            forecast_resolution_minutes=forecast_resolution,\r\n+            **filtered_params\r\n+        )\r\n+    else:\r\n+        # For other models, use base config\r\n+        filtered_params = filter_config_params(filtered_params, BaseConfig)  # Changed to BaseConfig\r\n+        return BaseConfig(**filtered_params)  # Changed to BaseConfig\r\n+\r\n+def _get_default_batch_size(resolution: int) -> int:\r\n+    \"\"\"Calculate appropriate batch size based on resolution.\"\"\"\r\n+    if resolution <= 15:\r\n+        return 64  # Larger batches for high-frequency data\r\n+    elif resolution <= 60:\r\n+        return 32  # Medium batches for hourly data\r\n+    elif resolution <= 1440:\r\n+        return 16  # Smaller batches for daily data\r\n+    else:\r\n+        return 8   # Smallest batches for monthly data\r\n+\r\n+def _get_default_learning_rate(resolution: int) -> float:\r\n+    \"\"\"Calculate appropriate learning rate based on resolution.\"\"\"\r\n+    if resolution <= 15:\r\n+        return 0.001  # Smaller learning rate for high-frequency data\r\n+    elif resolution <= 60:\r\n+        return 0.002  # Medium learning rate for hourly data\r\n+    elif resolution <= 1440:\r\n+        return 0.005  # Larger learning rate for daily data\r\n+    else:\r\n+        return 0.01  # Largest learning rate for monthly data\r\n+\r\n+def filter_config_params(params: Dict[str, Any], config_class: Any) -> Dict[str, Any]:\r\n+    \"\"\"Filter configuration parameters to match the target class fields.\"\"\"\r\n+    return {\r\n+        k: v for k, v in params.items() \r\n+        if k in config_class.__dataclass_fields__  # type: ignore\r\n+    }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733238135931,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,32 +50,57 @@\n     model_params: Dict[str, Any],\r\n     training_params: Dict[str, Any],\r\n     dataset_params: Dict[str, Any]\r\n ) -> PipelineConfig:\r\n-    \"\"\"Create pipeline configuration.\"\"\"\r\n-    # Extract resolution parameters from dataset_params\r\n-    input_resolution = dataset_params['input_resolution_minutes']\r\n-    forecast_resolution = dataset_params['forecast_resolution_minutes']\r\n+    \"\"\"Create pipeline configuration with proper feature dimension handling.\"\"\"\r\n+    print(\"\\nDEBUG: Creating Pipeline Configuration\")\r\n+    print(f\"Initial model params - input_features: {model_params.get('input_features')}\")\r\n     \r\n-    # Create configs\r\n-    dataset_config = DatasetConfig(**dataset_params)\r\n-    model_config = ModelConfig(**model_params)\r\n+    # Extract and validate feature dimensions\r\n+    n_features = model_params['input_features']\r\n+    value_features = model_params['value_features']\r\n+    time_features = model_params['time_features']\r\n+    \r\n+    # Validate feature dimensions match\r\n+    assert n_features == value_features + time_features, (\r\n+        f\"Feature dimension mismatch! Total ({n_features}) must equal \"\r\n+        f\"value ({value_features}) + time ({time_features})\"\r\n+    )\r\n+    \r\n+    # Create configs with explicit feature dimensions\r\n+    dataset_config = DatasetConfig(**{\r\n+        **dataset_params,\r\n+        'input_features': n_features,\r\n+        'value_features': value_features,\r\n+        'time_features': time_features\r\n+    })\r\n+    \r\n+    # Ensure model config preserves feature dimensions\r\n+    model_config = ModelConfig(**{\r\n+        **model_params,\r\n+        'input_features': n_features,\r\n+        'value_features': value_features,\r\n+        'time_features': time_features\r\n+    })\r\n+    \r\n+    # Create training config\r\n     training_config = create_training_config(\r\n         model_type=model_type,\r\n         params=training_params,\r\n-        input_resolution=input_resolution,\r\n-        forecast_resolution=forecast_resolution\r\n+        input_resolution=dataset_params['input_resolution_minutes'],\r\n+        forecast_resolution=dataset_params['forecast_resolution_minutes']\r\n     )\r\n     \r\n+    print(f\"Final model config - input_features: {model_config.input_features}\")\r\n+    \r\n     return PipelineConfig(\r\n         dataset_config=dataset_config,\r\n         model_config=model_config,\r\n         training_config=training_config,\r\n         data_path=Path(data_path),\r\n-        input_resolution_minutes=input_resolution,\r\n-        forecast_resolution_minutes=forecast_resolution\r\n+        input_resolution_minutes=dataset_params['input_resolution_minutes'],\r\n+        forecast_resolution_minutes=dataset_params['forecast_resolution_minutes']\r\n     )\r\n-\r\n def create_training_config(\r\n     model_type: ModelType,\r\n     params: Dict[str, Any],\r\n     input_resolution: int,\r\n@@ -137,166 +162,5 @@\n     \"\"\"Filter configuration parameters to match the target class fields.\"\"\"\r\n     return {\r\n         k: v for k, v in params.items() \r\n         if k in config_class.__dataclass_fields__  # type: ignore\r\n-    }\n-# pipeline/utils/config_utils.py\r\n-from __future__ import annotations\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Optional, Tuple\r\n-\r\n-from data_loading.base.base_dataset import DatasetConfig\r\n-from models.config.model_config import ModelConfig\r\n-from models.registry.model_types import ModelType\r\n-from pipeline.config.pipeline_config import PipelineConfig  # Fix import path\r\n-from training.config.training_config import (  # Fix import path\r\n-    BaseConfig,\r\n-    TransformerTrainingConfig,\r\n-    ResolutionBasedTrainingConfig\r\n-)\r\n-\r\n-def get_resolution_parameters(\r\n-    input_resolution: int,\r\n-    forecast_resolution: int\r\n-) -> Dict[str, Any]:\r\n-    \"\"\"\r\n-    Calculate appropriate parameters based on input and forecast resolutions.\r\n-    This helps ensure consistent configuration across different components.\r\n-    \"\"\"\r\n-    # Calculate points per period for different time scales\r\n-    points_per_hour = 60 // min(input_resolution, 60)\r\n-    points_per_day = 24 * points_per_hour\r\n-    \r\n-    # Determine appropriate window sizes based on resolution\r\n-    if forecast_resolution <= 60:  # Hourly or sub-hourly\r\n-        lookback_hours = 24  # One day of history\r\n-        forecast_hours = 12  # 12 hours ahead\r\n-    elif forecast_resolution <= 1440:  # Daily\r\n-        lookback_hours = 168  # One week of history\r\n-        forecast_hours = 72  # 3 days ahead\r\n-    else:  # Monthly\r\n-        lookback_hours = 720  # 30 days of history\r\n-        forecast_hours = 168  # 1 week ahead\r\n-\r\n-    return {\r\n-        'input_resolution_minutes': input_resolution,\r\n-        'forecast_resolution_minutes': forecast_resolution,\r\n-        'lookback_periods': lookback_hours * (60 // input_resolution),\r\n-        'forecast_periods': forecast_hours * (60 // forecast_resolution),\r\n-        'points_per_hour': points_per_hour,\r\n-    }\r\n-\r\n-def create_pipeline_config(\r\n-    data_path: str | Path,\r\n-    model_type: ModelType,\r\n-    model_params: Dict[str, Any],\r\n-    training_params: Dict[str, Any],\r\n-    dataset_params: Dict[str, Any]\r\n-) -> PipelineConfig:\r\n-    \"\"\"\r\n-    Create a pipeline configuration with enhanced support for different time resolutions.\r\n-    This function ensures consistent resolution handling across all components.\r\n-    \"\"\"\r\n-    # Extract resolution parameters\r\n-    input_resolution = dataset_params.get('input_resolution_minutes', 15)\r\n-    forecast_resolution = dataset_params.get('forecast_resolution_minutes', 15)\r\n-    \r\n-    # Get resolution-specific parameters\r\n-    resolution_params = get_resolution_parameters(input_resolution, forecast_resolution)\r\n-    \r\n-    # Update all parameter dictionaries with resolution information\r\n-    dataset_params.update(resolution_params)\r\n-    model_params.update(resolution_params)\r\n-    training_params.update(resolution_params)\r\n-\r\n-    # Create dataset configuration\r\n-    filtered_dataset_params = filter_config_params(dataset_params, DatasetConfig)\r\n-    dataset_config = DatasetConfig(**filtered_dataset_params)\r\n-\r\n-    # Create model configuration with resolution awareness\r\n-    filtered_model_params = filter_config_params(model_params, ModelConfig)\r\n-    if model_type.is_resolution_specific:\r\n-        # Let the model type adjust itself based on resolution\r\n-        model_type = ModelType.get_for_resolution(forecast_resolution)\r\n-    \r\n-    model_config = ModelConfig(\r\n-        model_type=model_type,\r\n-        **filtered_model_params\r\n-    )\r\n-\r\n-    # Create appropriate training configuration\r\n-    training_config = create_training_config(\r\n-        model_type=model_type,\r\n-        params=training_params,\r\n-        input_resolution=input_resolution,\r\n-        forecast_resolution=forecast_resolution\r\n-    )\r\n-\r\n-    # Create and return the complete pipeline configuration\r\n-    return PipelineConfig(\r\n-        dataset_config=dataset_config,\r\n-        model_config=model_config,\r\n-        training_config=training_config,\r\n-        data_path=Path(data_path),\r\n-        input_resolution_minutes=input_resolution,\r\n-        forecast_resolution_minutes=forecast_resolution\r\n-    )\r\n-\r\n-def create_training_config(\r\n-    model_type: ModelType,\r\n-    params: Dict[str, Any],\r\n-    input_resolution: int,\r\n-    forecast_resolution: int\r\n-) -> BaseConfig:  # Changed return type to BaseConfig\r\n-    \"\"\"\r\n-    Create an appropriate training configuration based on model type and resolution.\r\n-    This function handles the complexity of choosing and configuring the right training setup.\r\n-    \"\"\"\r\n-    # Adjust batch size based on resolution\r\n-    if 'batch_size' not in params:\r\n-        params['batch_size'] = _get_default_batch_size(forecast_resolution)\r\n-\r\n-    # Adjust learning rate based on resolution\r\n-    if 'learning_rate' not in params:\r\n-        params['learning_rate'] = _get_default_learning_rate(forecast_resolution)\r\n-\r\n-    if model_type.is_transformer:\r\n-        # For transformer models, use resolution-aware transformer config\r\n-        filtered_params = filter_config_params(params, TransformerTrainingConfig)\r\n-        return TransformerTrainingConfig(\r\n-            input_resolution_minutes=input_resolution,\r\n-            forecast_resolution_minutes=forecast_resolution,\r\n-            **filtered_params\r\n-        )\r\n-    else:\r\n-        # For other models, use base config\r\n-        filtered_params = filter_config_params(params, BaseConfig)  # Changed to BaseConfig\r\n-        return BaseConfig(**filtered_params)  # Changed to BaseConfig\r\n-\r\n-def _get_default_batch_size(resolution: int) -> int:\r\n-    \"\"\"Calculate appropriate batch size based on resolution.\"\"\"\r\n-    if resolution <= 15:\r\n-        return 64  # Larger batches for high-frequency data\r\n-    elif resolution <= 60:\r\n-        return 32  # Medium batches for hourly data\r\n-    elif resolution <= 1440:\r\n-        return 16  # Smaller batches for daily data\r\n-    else:\r\n-        return 8   # Smallest batches for monthly data\r\n-\r\n-def _get_default_learning_rate(resolution: int) -> float:\r\n-    \"\"\"Calculate appropriate learning rate based on resolution.\"\"\"\r\n-    if resolution <= 15:\r\n-        return 0.001  # Smaller learning rate for high-frequency data\r\n-    elif resolution <= 60:\r\n-        return 0.002  # Medium learning rate for hourly data\r\n-    elif resolution <= 1440:\r\n-        return 0.005  # Larger learning rate for daily data\r\n-    else:\r\n-        return 0.01  # Largest learning rate for monthly data\r\n-\r\n-def filter_config_params(params: Dict[str, Any], config_class: Any) -> Dict[str, Any]:\r\n-    \"\"\"Filter configuration parameters to match the target class fields.\"\"\"\r\n-    return {\r\n-        k: v for k, v in params.items() \r\n-        if k in config_class.__dataclass_fields__  # type: ignore\r\n     }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733238344875,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,14 +66,9 @@\n         f\"value ({value_features}) + time ({time_features})\"\r\n     )\r\n     \r\n     # Create configs with explicit feature dimensions\r\n-    dataset_config = DatasetConfig(**{\r\n-        **dataset_params,\r\n-        'input_features': n_features,\r\n-        'value_features': value_features,\r\n-        'time_features': time_features\r\n-    })\r\n+    dataset_config = DatasetConfig(**dataset_params)\r\n     \r\n     # Ensure model config preserves feature dimensions\r\n     model_config = ModelConfig(**{\r\n         **model_params,\r\n"
                }
            ],
            "date": 1733004655385,
            "name": "Commit-0",
            "content": "# pipeline/utils/config_utils.py\r\nfrom __future__ import annotations\r\n\r\nfrom pathlib import Path\r\nfrom typing import Dict, Any\r\n\r\nfrom data_loading.base.base_dataset import DatasetConfig\r\nfrom models.config.model_config import ModelConfig\r\nfrom models.registry.model_types import ModelType\r\n\r\nfrom ..config.pipeline_config import PipelineConfig\r\nfrom training.config import TrainingConfig, TransformerTrainingConfig  # Add import\r\n\r\ndef create_pipeline_config(\r\n        data_path: str | Path,\r\n        model_type: ModelType,\r\n        model_params: Dict[str, Any],\r\n        training_params: Dict[str, Any],\r\n        dataset_params: Dict[str, Any]\r\n) -> PipelineConfig:\r\n    \"\"\"Create a pipeline configuration from parameters.\"\"\"\r\n\r\n    # Create dataset config\r\n    dataset_config = DatasetConfig(**dataset_params)\r\n\r\n    # Create model config\r\n    model_config = ModelConfig(\r\n        model_type=model_type,  # Pass ModelType directly\r\n        **model_params\r\n    )\r\n\r\n    # Choose appropriate training config based on model type\r\n    if model_type.is_transformer:\r\n        training_config = TransformerTrainingConfig(**training_params)\r\n    else:\r\n        # Remove transformer-specific parameters for non-transformer models\r\n        base_params = {k: v for k, v in training_params.items() \r\n                    if not k.startswith('transformer_')}\r\n        training_config = TrainingConfig(**base_params)\r\n\r\n    # Create pipeline config\r\n    return PipelineConfig(\r\n        dataset_config=dataset_config,\r\n        model_config=model_config,\r\n        training_config=training_config,\r\n        data_path=Path(data_path)\r\n    )"
        }
    ]
}