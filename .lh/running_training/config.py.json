{
    "sourceFile": "running_training/config.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1733264549196,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1733264549196,
            "name": "Commit-0",
            "content": "import torch\r\nfrom typing import Dict, Any, Tuple\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom models.registry.model_types import ModelType\r\n\r\ndef get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n    \"\"\"Get optimized configuration parameters for transformer-based energy forecasting.\"\"\"\r\n# Core architecture dimensions\r\n    sequence_length = 24   # Hours in a day for hourly predictions\r\n    n_features = 7        # Energy value (1) + cyclical time encodings (6)\r\n    d_model = 512        # Transformer embedding dimension\r\n    n_heads = 8          # Number of attention heads\r\n    \r\n    # Compute derived parameters\r\n    d_ff = d_model * 4   # Feed-forward dimension\r\n    n_layers = 6         # Number of transformer layers\r\n    \r\n    model_params = {\r\n        # Core model identification\r\n        \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n        \r\n        # Time resolution and sequence configuration\r\n        \"input_resolution_minutes\": 60,\r\n        \"forecast_resolution_minutes\": 60,\r\n        \"lookback_periods\": sequence_length,\r\n        \"forecast_periods\": sequence_length,\r\n        \r\n        # Model architecture parameters\r\n        \"input_features\": n_features,\r\n        \"output_features\": 1,\r\n        \"d_model\": d_model,\r\n        \"n_heads\": n_heads,\r\n        \"n_encoder_layers\": n_layers,\r\n        \"n_decoder_layers\": n_layers,\r\n        \"d_ff\": d_ff,\r\n        \"dropout\": 0.1,\r\n        \"max_sequence_length\": 5000,\r\n        \r\n        # Time series specific parameters\r\n        \"value_features\": 1,\r\n        \"time_features\": n_features - 1,\r\n        \"kernel_size\": 3,\r\n        \"batch_first\": True,\r\n        \r\n        # Neural network specific\r\n        \"hidden_dims\": [64, 32],\r\n        \"activation\": \"relu\",\r\n        \r\n        # Training\r\n        \"batch_size\": 512,\r\n        \"learning_rate\": 0.00001,\r\n        \"max_epochs\": 100,\r\n        \"optimizer\": \"adamw\",\r\n        \"optimizer_config\": {\r\n            \"weight_decay\": 0.01,\r\n            \"betas\": (0.9, 0.98)\r\n        },\r\n        \"scheduler\": \"one_cycle\",\r\n        \"scheduler_config\": {\r\n            \"pct_start\": 0.3,\r\n            \"div_factor\": 25.0,\r\n            \"final_div_factor\": 1000.0\r\n        },\r\n        \"criterion\": \"mse\",\r\n        \"criterion_config\": {},\r\n        \r\n        # Device\r\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n    }\r\n\r\n    training_params = {\r\n        # Core training settings\r\n        \"learning_rate\": 0.0001,\r\n        \"max_epochs\": 100,\r\n        \"batch_size\": 32,\r\n        \"device\": model_params[\"device\"],\r\n        \r\n        # Early stopping\r\n        \"use_early_stopping\": True,\r\n        \"early_stopping_patience\": 15,\r\n        \"early_stopping_min_delta\": 1e-4,\r\n        \"save_best_model\": True,\r\n        \r\n        # Sequence handling\r\n        \"input_resolution_minutes\": 60,\r\n        \"forecast_resolution_minutes\": 60,\r\n        \"transformer_labels_count\": sequence_length,\r\n        \"forecasting_horizon\": sequence_length,\r\n        \r\n        # Teacher forcing\r\n        \"transformer_use_teacher_forcing\": True,\r\n        \"teacher_forcing_ratio\": 0.5,\r\n        \"teacher_forcing_schedule\": \"linear_decay\",\r\n        \r\n        # Optimization\r\n        \"gradient_clip_val\": 1.0,\r\n        \"optimizer\": \"adamw\",\r\n        \"optimizer_config\": {\r\n            \"weight_decay\": 0.01,\r\n            \"betas\": (0.9, 0.98),\r\n            \"eps\": 1e-8\r\n        },\r\n        \r\n        # Learning rate scheduling\r\n        \"scheduler\": \"one_cycle\",\r\n        \"scheduler_config\": {\r\n            \"pct_start\": 0.3,\r\n            \"div_factor\": 25.0,\r\n            \"final_div_factor\": 1000.0,\r\n            \"anneal_strategy\": \"cos\",\r\n            \"cycle_momentum\": True\r\n        },\r\n        \r\n        # Validation settings\r\n        \"validation_frequency\": 1,         # Validate every N epochs\r\n        \"validation_metric\": \"mae\",        # Primary metric for model selection\r\n        \r\n        # Logging and checkpointing\r\n        \"log_frequency\": 10,               # Log every N batches\r\n        \"checkpoint_frequency\": 5,         # Save checkpoint every N epochs\r\n        \"keep_last_n_checkpoints\": 3       # Number of checkpoints to keep\r\n    }\r\n\r\n    dataset_params = {\r\n        # Core parameters\r\n        \"time_variable\": \"utc_timestamp\",\r\n        \"target_variable\": \"energy_consumption\",\r\n        \r\n        # Resolution and sequence settings\r\n        \"input_resolution_minutes\": 60,\r\n        \"forecast_resolution_minutes\": 60,\r\n        \"lookback_periods\": sequence_length,\r\n        \"forecast_periods\": sequence_length,\r\n        \r\n        # Core feature configuration\r\n        \"is_single_time_point_prediction\": False,\r\n        \"include_time_information\": True,\r\n        \"is_training_set\": True,\r\n        \"labels_count\": sequence_length,\r\n        \"one_hot_time_variables\": False,\r\n        \r\n        # Data preprocessing\r\n        \"normalize_data\": True,\r\n        \"scaling_method\": \"standard\",\r\n        \"time_series_scaler\": StandardScaler(),\r\n        \"handle_missing_values\": \"interpolate\",\r\n        \"remove_outliers\": True,\r\n        \"outlier_std_threshold\": 3.0,\r\n        \r\n        # Basic feature flags\r\n        \"add_time_features\": True,\r\n        \"add_holiday_features\": True,\r\n        \"add_weather_features\": False,\r\n        \r\n        # Detailed time features\r\n        \"add_hour_feature\": True,\r\n        \"add_weekday_feature\": True,\r\n        \"add_month_feature\": True,\r\n        \"add_season_feature\": False,\r\n        \"add_year_feature\": False,\r\n        \r\n        # Data augmentation\r\n        \"use_data_augmentation\": False,\r\n        \"augmentation_methods\": [\"jitter\", \"scaling\"],\r\n        \"augmentation_probability\": 0.3,\r\n        \r\n        # Sequence handling\r\n        \"padding_value\": 0.0,\r\n        \"mask_padding\": True,\r\n        \"max_sequence_gaps\": 3\r\n    }\r\n\r\n    return model_params, training_params, dataset_params\r\n"
        }
    ]
}