{
    "sourceFile": "train.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 17,
            "patches": [
                {
                    "date": 1733003883280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733003949096,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,11 +42,9 @@\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n         \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,  # Add data normalization\r\n-        \"scaling_method\": \"standard\",  # Use standard scaling\r\n+        \"one_hot_time_variables\": False\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733004103522,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,18 +21,18 @@\n         \"transformer_labels_count\": 12\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.0001,  # Reduced from 0.001\r\n+        \"learning_rate\": 0.0001,\r\n         \"max_epochs\": 100,\r\n         \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,  # Increased from 10\r\n-        \"batch_size\": 64,        # Increased from 32\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,\r\n         \"device\": \"cuda\",\r\n         \"transformer_labels_count\": 12,\r\n         \"forecasting_horizon\": 12,\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clip_val\": 1.0  # Add gradient clipping\r\n+        \"gradient_clip_val\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n"
                },
                {
                    "date": 1733004211124,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,96 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"Get default configuration parameters.\"\"\"\r\n+    model_params = {\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n+        \"d_model\": 128,       # Increased from 64\r\n+        \"n_heads\": 8,         # Increased from 4\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 512,         # Increased from 256\r\n+        \"dropout\": 0.2,      # Increased from 0.1\r\n+        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n+        \"transformer_labels_count\": 12\r\n+    }\r\n+\r\n+    training_params = {\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,\r\n+        \"device\": \"cuda\",\r\n+        \"transformer_labels_count\": 12,\r\n+        \"forecasting_horizon\": 12,\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n+        \"time_series_window_in_hours\": 24,\r\n+        \"forecasting_horizon_in_hours\": 12,\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": 12,\r\n+        \"one_hot_time_variables\": False\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    # Initialize model registry first\r\n+    initialize_model_registry()\r\n+    \r\n+    # Debug: Print available models\r\n+    from models.registry.factory import ModelFactory\r\n+    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n+    \r\n+    model_params, training_params, dataset_params = get_default_config_params()\r\n+    \r\n+    # Create pipeline configuration\r\n+    config = create_pipeline_config(\r\n+        data_path=data_path,\r\n+        model_type=ModelType.VANILLA_TRANSFORMER,\r\n+        model_params=model_params,\r\n+        training_params=training_params,\r\n+        dataset_params=dataset_params\r\n+    )\r\n+    \r\n+    # Initialize and run pipeline\r\n+    pipeline = TimeSeriesPipeline(config)\r\n+    experiment = pipeline.run()\r\n+    \r\n+    # Print results\r\n+    if experiment is None:\r\n+        print(\"Error: Training failed to produce results\")\r\n+        return\r\n+        \r\n+    print(\"\\nTraining Complete!\")\r\n+    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n+    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+    \r\n+    print(\"\\nEvaluation Metrics:\")\r\n+    for metric, value in experiment.evaluation.total_metrics.items():\r\n+        print(f\"{metric}: {value:.4f}\")\r\n+\r\n+def main():\r\n+    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n+    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    args = parser.parse_args()\r\n+    \r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733004852064,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,31 +8,31 @@\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get default configuration parameters.\"\"\"\r\n     model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n-        \"d_model\": 128,       # Increased from 64\r\n-        \"n_heads\": 8,         # Increased from 4\r\n+        \"input_features\": 1,     # Changed from 10 to 1 since you have only one input feature\r\n+        \"output_features\": 1,    # Correct, already 1\r\n+        \"d_model\": 128,         \r\n+        \"n_heads\": 8,           \r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,         # Increased from 256\r\n-        \"dropout\": 0.2,      # Increased from 0.1\r\n-        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n-        \"transformer_labels_count\": 12\r\n+        \"d_ff\": 512,           \r\n+        \"dropout\": 0.2,        \r\n+        \"max_seq_length\": 96,    # Changed to 96 (24 hours * 4 readings per hour)\r\n+        \"transformer_labels_count\": 48  # Changed to 48 (12 hours * 4 readings per hour)\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n+        \"learning_rate\": 0.001,  # Increased slightly as 0.0001 might be too small\r\n         \"max_epochs\": 100,\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n+        \"batch_size\": 32,        # Reduced from 64 to start\r\n         \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n+        \"transformer_labels_count\": 48,  # Match with model_params\r\n+        \"forecasting_horizon\": 48,       # Changed to match 15-minute intervals\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+        \"gradient_clipping\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n@@ -41,10 +41,12 @@\n         \"forecasting_horizon_in_hours\": 12,\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False\r\n+        \"labels_count\": 48,               # Changed to match 15-minute intervals\r\n+        \"one_hot_time_variables\": False,\r\n+        \"normalize_data\": True,           # Added normalization\r\n+        \"scaling_method\": \"standard\"      # Added scaling method\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n@@ -92,101 +94,5 @@\n     \r\n     train_model(args.data)\r\n \r\n if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters.\"\"\"\r\n-    model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n-        \"d_model\": 128,       # Increased from 64\r\n-        \"n_heads\": 8,         # Increased from 4\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,         # Increased from 256\r\n-        \"dropout\": 0.2,      # Increased from 0.1\r\n-        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n-        \"transformer_labels_count\": 12\r\n-    }\r\n-\r\n-    training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clip_val\": 1.0\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n-    # Initialize model registry first\r\n-    initialize_model_registry()\r\n-    \r\n-    # Debug: Print available models\r\n-    from models.registry.factory import ModelFactory\r\n-    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n-    \r\n-    model_params, training_params, dataset_params = get_default_config_params()\r\n-    \r\n-    # Create pipeline configuration\r\n-    config = create_pipeline_config(\r\n-        data_path=data_path,\r\n-        model_type=ModelType.VANILLA_TRANSFORMER,\r\n-        model_params=model_params,\r\n-        training_params=training_params,\r\n-        dataset_params=dataset_params\r\n-    )\r\n-    \r\n-    # Initialize and run pipeline\r\n-    pipeline = TimeSeriesPipeline(config)\r\n-    experiment = pipeline.run()\r\n-    \r\n-    # Print results\r\n-    if experiment is None:\r\n-        print(\"Error: Training failed to produce results\")\r\n-        return\r\n-        \r\n-    print(\"\\nTraining Complete!\")\r\n-    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n-    \r\n-    print(\"\\nEvaluation Metrics:\")\r\n-    for metric, value in experiment.evaluation.total_metrics.items():\r\n-        print(f\"{metric}: {value:.4f}\")\r\n-\r\n-def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n-    args = parser.parse_args()\r\n-    \r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733004933464,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,31 +8,31 @@\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get default configuration parameters.\"\"\"\r\n     model_params = {\r\n-        \"input_features\": 1,     # Changed from 10 to 1 since you have only one input feature\r\n-        \"output_features\": 1,    # Correct, already 1\r\n-        \"d_model\": 128,         \r\n-        \"n_heads\": 8,           \r\n+        \"input_features\": 10,\r\n+        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n+        \"d_model\": 128,       # Increased from 64\r\n+        \"n_heads\": 8,         # Increased from 4\r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,           \r\n-        \"dropout\": 0.2,        \r\n-        \"max_seq_length\": 96,    # Changed to 96 (24 hours * 4 readings per hour)\r\n-        \"transformer_labels_count\": 48  # Changed to 48 (12 hours * 4 readings per hour)\r\n+        \"d_ff\": 512,         # Increased from 256\r\n+        \"dropout\": 0.2,      # Increased from 0.1\r\n+        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n+        \"transformer_labels_count\": 12\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.001,  # Increased slightly as 0.0001 might be too small\r\n+        \"learning_rate\": 0.0001,\r\n         \"max_epochs\": 100,\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 32,        # Reduced from 64 to start\r\n+        \"batch_size\": 64,\r\n         \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 48,  # Match with model_params\r\n-        \"forecasting_horizon\": 48,       # Changed to match 15-minute intervals\r\n+        \"transformer_labels_count\": 12,\r\n+        \"forecasting_horizon\": 12,\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0\r\n+        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n@@ -41,12 +41,10 @@\n         \"forecasting_horizon_in_hours\": 12,\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 48,               # Changed to match 15-minute intervals\r\n-        \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,           # Added normalization\r\n-        \"scaling_method\": \"standard\"      # Added scaling method\r\n+        \"labels_count\": 12,\r\n+        \"one_hot_time_variables\": False\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733006957066,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,101 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from sklearn.discriminant_analysis import StandardScaler\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"Get default configuration parameters.\"\"\"\r\n+    model_params = {\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n+        \"d_model\": 128,       # Increased from 64\r\n+        \"n_heads\": 8,         # Increased from 4\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 512,         # Increased from 256\r\n+        \"dropout\": 0.2,      # Increased from 0.1\r\n+        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n+        \"transformer_labels_count\": 12\r\n+    }\r\n+\r\n+    training_params = {\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,\r\n+        \"device\": \"cuda\",\r\n+        \"transformer_labels_count\": 12,\r\n+        \"forecasting_horizon\": 12,\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n+        \"time_series_window_in_hours\": 24,\r\n+        \"forecasting_horizon_in_hours\": 12,\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": 12,\r\n+        \"one_hot_time_variables\": False,\r\n+        \"normalize_data\": True,  # Add this\r\n+        \"scaling_method\": \"standard\",  # Add this\r\n+        \"time_series_scaler\": StandardScaler()  # Add this\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    # Initialize model registry first\r\n+    initialize_model_registry()\r\n+    \r\n+    # Debug: Print available models\r\n+    from models.registry.factory import ModelFactory\r\n+    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n+    \r\n+    model_params, training_params, dataset_params = get_default_config_params()\r\n+    \r\n+    # Create pipeline configuration\r\n+    config = create_pipeline_config(\r\n+        data_path=data_path,\r\n+        model_type=ModelType.VANILLA_TRANSFORMER,\r\n+        model_params=model_params,\r\n+        training_params=training_params,\r\n+        dataset_params=dataset_params\r\n+    )\r\n+    \r\n+    # Initialize and run pipeline\r\n+    pipeline = TimeSeriesPipeline(config)\r\n+    experiment = pipeline.run()\r\n+    \r\n+    # Print results\r\n+    if experiment is None:\r\n+        print(\"Error: Training failed to produce results\")\r\n+        return\r\n+        \r\n+    print(\"\\nTraining Complete!\")\r\n+    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n+    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+    \r\n+    print(\"\\nEvaluation Metrics:\")\r\n+    for metric, value in experiment.evaluation.total_metrics.items():\r\n+        print(f\"{metric}: {value:.4f}\")\r\n+\r\n+def main():\r\n+    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n+    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    args = parser.parse_args()\r\n+    \r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733006972835,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,101 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from sklearn.discriminant_analysis import StandardScaler\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"Get default configuration parameters.\"\"\"\r\n+    model_params = {\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 10,  # Change to 10 to match input dimensions\r\n+        \"d_model\": 128,\r\n+        \"n_heads\": 8,\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 512,\r\n+        \"dropout\": 0.2,\r\n+        \"max_seq_length\": 168,\r\n+        \"transformer_labels_count\": 12\r\n+    }\r\n+\r\n+    training_params = {\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,\r\n+        \"device\": \"cuda\",\r\n+        \"transformer_labels_count\": 12,\r\n+        \"forecasting_horizon\": 12,\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n+        \"time_series_window_in_hours\": 24,\r\n+        \"forecasting_horizon_in_hours\": 12,\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": 12,\r\n+        \"one_hot_time_variables\": False,\r\n+        \"normalize_data\": True,  # Add this\r\n+        \"scaling_method\": \"standard\",  # Add this\r\n+        \"time_series_scaler\": StandardScaler()  # Add this\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    # Initialize model registry first\r\n+    initialize_model_registry()\r\n+    \r\n+    # Debug: Print available models\r\n+    from models.registry.factory import ModelFactory\r\n+    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n+    \r\n+    model_params, training_params, dataset_params = get_default_config_params()\r\n+    \r\n+    # Create pipeline configuration\r\n+    config = create_pipeline_config(\r\n+        data_path=data_path,\r\n+        model_type=ModelType.VANILLA_TRANSFORMER,\r\n+        model_params=model_params,\r\n+        training_params=training_params,\r\n+        dataset_params=dataset_params\r\n+    )\r\n+    \r\n+    # Initialize and run pipeline\r\n+    pipeline = TimeSeriesPipeline(config)\r\n+    experiment = pipeline.run()\r\n+    \r\n+    # Print results\r\n+    if experiment is None:\r\n+        print(\"Error: Training failed to produce results\")\r\n+        return\r\n+        \r\n+    print(\"\\nTraining Complete!\")\r\n+    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n+    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+    \r\n+    print(\"\\nEvaluation Metrics:\")\r\n+    for metric, value in experiment.evaluation.total_metrics.items():\r\n+        print(f\"{metric}: {value:.4f}\")\r\n+\r\n+def main():\r\n+    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n+    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    args = parser.parse_args()\r\n+    \r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733006982786,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,18 +23,18 @@\n         \"transformer_labels_count\": 12\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n+        \"learning_rate\": 0.001,  # Increase learning rate\r\n         \"max_epochs\": 100,\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n+        \"batch_size\": 32,  # Reduce batch size\r\n         \"device\": \"cuda\",\r\n         \"transformer_labels_count\": 12,\r\n         \"forecasting_horizon\": 12,\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+        \"gradient_clipping\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n@@ -97,202 +97,5 @@\n     \r\n     train_model(args.data)\r\n \r\n if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from sklearn.discriminant_analysis import StandardScaler\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters.\"\"\"\r\n-    model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n-        \"d_model\": 128,       # Increased from 64\r\n-        \"n_heads\": 8,         # Increased from 4\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,         # Increased from 256\r\n-        \"dropout\": 0.2,      # Increased from 0.1\r\n-        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n-        \"transformer_labels_count\": 12\r\n-    }\r\n-\r\n-    training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,  # Add this\r\n-        \"scaling_method\": \"standard\",  # Add this\r\n-        \"time_series_scaler\": StandardScaler()  # Add this\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n-    # Initialize model registry first\r\n-    initialize_model_registry()\r\n-    \r\n-    # Debug: Print available models\r\n-    from models.registry.factory import ModelFactory\r\n-    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n-    \r\n-    model_params, training_params, dataset_params = get_default_config_params()\r\n-    \r\n-    # Create pipeline configuration\r\n-    config = create_pipeline_config(\r\n-        data_path=data_path,\r\n-        model_type=ModelType.VANILLA_TRANSFORMER,\r\n-        model_params=model_params,\r\n-        training_params=training_params,\r\n-        dataset_params=dataset_params\r\n-    )\r\n-    \r\n-    # Initialize and run pipeline\r\n-    pipeline = TimeSeriesPipeline(config)\r\n-    experiment = pipeline.run()\r\n-    \r\n-    # Print results\r\n-    if experiment is None:\r\n-        print(\"Error: Training failed to produce results\")\r\n-        return\r\n-        \r\n-    print(\"\\nTraining Complete!\")\r\n-    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n-    \r\n-    print(\"\\nEvaluation Metrics:\")\r\n-    for metric, value in experiment.evaluation.total_metrics.items():\r\n-        print(f\"{metric}: {value:.4f}\")\r\n-\r\n-def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n-    args = parser.parse_args()\r\n-    \r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters.\"\"\"\r\n-    model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n-        \"d_model\": 128,       # Increased from 64\r\n-        \"n_heads\": 8,         # Increased from 4\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,         # Increased from 256\r\n-        \"dropout\": 0.2,      # Increased from 0.1\r\n-        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n-        \"transformer_labels_count\": 12\r\n-    }\r\n-\r\n-    training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n-    # Initialize model registry first\r\n-    initialize_model_registry()\r\n-    \r\n-    # Debug: Print available models\r\n-    from models.registry.factory import ModelFactory\r\n-    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n-    \r\n-    model_params, training_params, dataset_params = get_default_config_params()\r\n-    \r\n-    # Create pipeline configuration\r\n-    config = create_pipeline_config(\r\n-        data_path=data_path,\r\n-        model_type=ModelType.VANILLA_TRANSFORMER,\r\n-        model_params=model_params,\r\n-        training_params=training_params,\r\n-        dataset_params=dataset_params\r\n-    )\r\n-    \r\n-    # Initialize and run pipeline\r\n-    pipeline = TimeSeriesPipeline(config)\r\n-    experiment = pipeline.run()\r\n-    \r\n-    # Print results\r\n-    if experiment is None:\r\n-        print(\"Error: Training failed to produce results\")\r\n-        return\r\n-        \r\n-    print(\"\\nTraining Complete!\")\r\n-    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n-    \r\n-    print(\"\\nEvaluation Metrics:\")\r\n-    for metric, value in experiment.evaluation.total_metrics.items():\r\n-        print(f\"{metric}: {value:.4f}\")\r\n-\r\n-def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n-    args = parser.parse_args()\r\n-    \r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733061971728,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,17 +11,17 @@\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get default configuration parameters.\"\"\"\r\n     model_params = {\r\n         \"input_features\": 10,\r\n-        \"output_features\": 10,  # Change to 10 to match input dimensions\r\n+        \"output_features\": 10,\r\n         \"d_model\": 128,\r\n         \"n_heads\": 8,\r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n         \"d_ff\": 512,\r\n         \"dropout\": 0.2,\r\n-        \"max_seq_length\": 168,\r\n-        \"transformer_labels_count\": 12\r\n+        \"max_seq_length\": 672,  # Updated: 28 days * 24 hours * 4 intervals\r\n+        \"transformer_labels_count\": 48   # Updated: 12 hours * 4 intervals\r\n     }\r\n \r\n     training_params = {\r\n         \"learning_rate\": 0.001,  # Increase learning rate\r\n@@ -29,27 +29,29 @@\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n         \"batch_size\": 32,  # Reduce batch size\r\n         \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n+        \"transformer_labels_count\": 48,  # Updated: 12 hours * 4 intervals\r\n+        \"forecasting_horizon\": 48,       # Updated: 12 hours * 4 intervals\r\n         \"transformer_use_teacher_forcing\": True,\r\n         \"gradient_clipping\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n+        \"target_variable\": \"energy_consumption\",  # Updated to match new column name\r\n+        \"time_series_window_in_hours\": 24,       # Will automatically use 96 points (24*4)\r\n+        \"forecasting_horizon_in_hours\": 12,      # Will automatically use 48 points (12*4)\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n+        \"labels_count\": 48,                      # Updated: 12 hours * 4 intervals\r\n         \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,  # Add this\r\n-        \"scaling_method\": \"standard\",  # Add this\r\n-        \"time_series_scaler\": StandardScaler()  # Add this\r\n+        \"normalize_data\": True,\r\n+        \"scaling_method\": \"standard\",\r\n+        \"time_series_scaler\": StandardScaler(),\r\n+        \"time_resolution_minutes\": 15,           # Added to specify interval\r\n+        \"points_per_hour\": 4                     # Added to specify points per hour\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733062194876,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,10 +18,10 @@\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n         \"d_ff\": 512,\r\n         \"dropout\": 0.2,\r\n-        \"max_seq_length\": 672,  # Updated: 28 days * 24 hours * 4 intervals\r\n-        \"transformer_labels_count\": 48   # Updated: 12 hours * 4 intervals\r\n+        \"max_seq_length\": 96,  # 24 hours * 4 intervals\r\n+        \"transformer_labels_count\": 48  # 12 hours * 4 intervals\r\n     }\r\n \r\n     training_params = {\r\n         \"learning_rate\": 0.001,  # Increase learning rate\r\n@@ -29,23 +29,23 @@\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n         \"batch_size\": 32,  # Reduce batch size\r\n         \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 48,  # Updated: 12 hours * 4 intervals\r\n-        \"forecasting_horizon\": 48,       # Updated: 12 hours * 4 intervals\r\n+        \"transformer_labels_count\": 48,  # 12 hours * 4 intervals\r\n+        \"forecasting_horizon\": 48,       # 12 hours * 4 intervals\r\n         \"transformer_use_teacher_forcing\": True,\r\n         \"gradient_clipping\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",  # Updated to match new column name\r\n-        \"time_series_window_in_hours\": 24,       # Will automatically use 96 points (24*4)\r\n-        \"forecasting_horizon_in_hours\": 12,      # Will automatically use 48 points (12*4)\r\n+        \"time_series_window_in_hours\": 24,       # Will be multiplied by points_per_hour\r\n+        \"forecasting_horizon_in_hours\": 12,      # Will be multiplied by points_per_hour\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 48,                      # Updated: 12 hours * 4 intervals\r\n+        \"labels_count\": 48,                      # 12 hours * 4 intervals\r\n         \"one_hot_time_variables\": False,\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n"
                },
                {
                    "date": 1733065770885,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,129 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from sklearn.preprocessing import StandardScaler\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"Get default configuration parameters optimized for 15-min interval data.\"\"\"\r\n+    model_params = {\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 10,\r\n+        \"d_model\": 256,         # Increased model capacity\r\n+        \"n_heads\": 8,\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 1024,          # Increased feedforward capacity\r\n+        \"dropout\": 0.2,\r\n+        \"max_seq_length\": 96,   # 24 hours * 4 intervals per hour\r\n+        \"transformer_labels_count\": 48,  # 12 hours * 4 intervals\r\n+    }\r\n+\r\n+    training_params = {\r\n+        \"learning_rate\": 0.0005,  # Reduced learning rate for stability\r\n+        \"max_epochs\": 150,       # Increased epochs\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,        # Increased batch size\r\n+        \"device\": \"cuda\",\r\n+        \"transformer_labels_count\": 48,\r\n+        \"forecasting_horizon\": 48,\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"gradient_clipping\": 1.0,\r\n+        \"optimizer\": \"adamw\",    # Changed to AdamW\r\n+        \"optimizer_config\": {\r\n+            \"weight_decay\": 0.01,\r\n+            \"betas\": (0.9, 0.999),\r\n+            \"eps\": 1e-8\r\n+        },\r\n+        \"scheduler\": \"cosine\",   # Changed to cosine annealing\r\n+        \"scheduler_config\": {\r\n+            \"T_max\": 150,        # Match max_epochs\r\n+            \"eta_min\": 1e-6\r\n+        }\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"energy_consumption\",\r\n+        \"time_series_window_in_hours\": 24,\r\n+        \"forecasting_horizon_in_hours\": 12,\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": 48,\r\n+        \"one_hot_time_variables\": False,\r\n+        \"normalize_data\": True,\r\n+        \"scaling_method\": \"standard\",\r\n+        \"time_series_scaler\": StandardScaler(),\r\n+        \"time_resolution_minutes\": 15,\r\n+        \"points_per_hour\": 4,\r\n+        \"add_time_features\": True,    # Add cyclical time features\r\n+        \"add_holiday_features\": True,  # Add holiday indicators\r\n+        \"add_weather_features\": False  # Optional weather features if available\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    try:\r\n+        # Initialize model registry\r\n+        initialize_model_registry()\r\n+        \r\n+        # Get configuration parameters\r\n+        model_params, training_params, dataset_params = get_default_config_params()\r\n+        \r\n+        # Create pipeline configuration\r\n+        config = create_pipeline_config(\r\n+            data_path=data_path,\r\n+            model_type=ModelType.VANILLA_TRANSFORMER,\r\n+            model_params=model_params,\r\n+            training_params=training_params,\r\n+            dataset_params=dataset_params\r\n+        )\r\n+        \r\n+        # Initialize and run pipeline\r\n+        pipeline = TimeSeriesPipeline(config)\r\n+        experiment = pipeline.run()\r\n+        \r\n+        if experiment is None:\r\n+            raise RuntimeError(\"Training failed to produce results\")\r\n+        \r\n+        # Print results\r\n+        print(\"\\nTraining Complete!\")\r\n+        print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n+        print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+        \r\n+        print(\"\\nEvaluation Metrics:\")\r\n+        for metric, value in experiment.evaluation.total_metrics.items():\r\n+            print(f\"{metric}: {value:.4f}\")\r\n+        \r\n+        # Print training history\r\n+        print(\"\\nTraining History:\")\r\n+        final_train_loss = experiment.training_report.train_losses[-1]\r\n+        final_val_loss = experiment.training_report.val_losses[-1]\r\n+        print(f\"Final Training Loss: {final_train_loss:.4f}\")\r\n+        print(f\"Final Validation Loss: {final_val_loss:.4f}\")\r\n+        \r\n+        # Save the experiment\r\n+        experiment.save_to_json_file()\r\n+        print(f\"\\nExperiment saved successfully\")\r\n+        \r\n+    except Exception as e:\r\n+        print(f\"Error during training: {str(e)}\")\r\n+        raise\r\n+\r\n+def main():\r\n+    parser = argparse.ArgumentParser(description='Train transformer model for 15-min interval energy forecasting')\r\n+    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    args = parser.parse_args()\r\n+    \r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733065822550,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -104,12 +104,19 @@\n             print(f\"{metric}: {value:.4f}\")\r\n         \r\n         # Print training history\r\n         print(\"\\nTraining History:\")\r\n-        final_train_loss = experiment.training_report.train_losses[-1]\r\n-        final_val_loss = experiment.training_report.val_losses[-1]\r\n-        print(f\"Final Training Loss: {final_train_loss:.4f}\")\r\n-        print(f\"Final Validation Loss: {final_val_loss:.4f}\")\r\n+        if experiment.training_report and hasattr(experiment.training_report, 'train_losses'):\r\n+            final_train_loss = experiment.training_report.train_losses[-1]\r\n+            print(f\"Final Training Loss: {final_train_loss:.4f}\")\r\n+        else:\r\n+            print(\"Training loss data not available\")\r\n+\r\n+        if experiment.training_report and hasattr(experiment.training_report, 'val_losses'):\r\n+            final_val_loss = experiment.training_report.val_losses[-1]\r\n+            print(f\"Final Validation Loss: {final_val_loss:.4f}\")\r\n+        else:\r\n+            print(\"Validation loss data not available\")\r\n         \r\n         # Save the experiment\r\n         experiment.save_to_json_file()\r\n         print(f\"\\nExperiment saved successfully\")\r\n"
                },
                {
                    "date": 1733065828395,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -132,108 +132,5 @@\n     \r\n     train_model(args.data)\r\n \r\n if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from sklearn.discriminant_analysis import StandardScaler\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters.\"\"\"\r\n-    model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 10,\r\n-        \"d_model\": 128,\r\n-        \"n_heads\": 8,\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,\r\n-        \"dropout\": 0.2,\r\n-        \"max_seq_length\": 96,  # 24 hours * 4 intervals\r\n-        \"transformer_labels_count\": 48  # 12 hours * 4 intervals\r\n-    }\r\n-\r\n-    training_params = {\r\n-        \"learning_rate\": 0.001,  # Increase learning rate\r\n-        \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 32,  # Reduce batch size\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 48,  # 12 hours * 4 intervals\r\n-        \"forecasting_horizon\": 48,       # 12 hours * 4 intervals\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"energy_consumption\",  # Updated to match new column name\r\n-        \"time_series_window_in_hours\": 24,       # Will be multiplied by points_per_hour\r\n-        \"forecasting_horizon_in_hours\": 12,      # Will be multiplied by points_per_hour\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": 48,                      # 12 hours * 4 intervals\r\n-        \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,\r\n-        \"scaling_method\": \"standard\",\r\n-        \"time_series_scaler\": StandardScaler(),\r\n-        \"time_resolution_minutes\": 15,           # Added to specify interval\r\n-        \"points_per_hour\": 4                     # Added to specify points per hour\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n-    # Initialize model registry first\r\n-    initialize_model_registry()\r\n-    \r\n-    # Debug: Print available models\r\n-    from models.registry.factory import ModelFactory\r\n-    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n-    \r\n-    model_params, training_params, dataset_params = get_default_config_params()\r\n-    \r\n-    # Create pipeline configuration\r\n-    config = create_pipeline_config(\r\n-        data_path=data_path,\r\n-        model_type=ModelType.VANILLA_TRANSFORMER,\r\n-        model_params=model_params,\r\n-        training_params=training_params,\r\n-        dataset_params=dataset_params\r\n-    )\r\n-    \r\n-    # Initialize and run pipeline\r\n-    pipeline = TimeSeriesPipeline(config)\r\n-    experiment = pipeline.run()\r\n-    \r\n-    # Print results\r\n-    if experiment is None:\r\n-        print(\"Error: Training failed to produce results\")\r\n-        return\r\n-        \r\n-    print(\"\\nTraining Complete!\")\r\n-    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n-    \r\n-    print(\"\\nEvaluation Metrics:\")\r\n-    for metric, value in experiment.evaluation.total_metrics.items():\r\n-        print(f\"{metric}: {value:.4f}\")\r\n-\r\n-def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n-    args = parser.parse_args()\r\n-    \r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733066459142,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,11 +61,12 @@\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \"time_resolution_minutes\": 15,\r\n         \"points_per_hour\": 4,\r\n-        \"add_time_features\": True,    # Add cyclical time features\r\n-        \"add_holiday_features\": True,  # Add holiday indicators\r\n-        \"add_weather_features\": False  # Optional weather features if available\r\n+        \"add_time_features\": True,\r\n+        \"add_holiday_features\": True,\r\n+        \"add_weather_features\": False,\r\n+        \"weather_variables\": [\"temperature\", \"humidity\", \"wind_speed\"]\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733066558381,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,12 +61,11 @@\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \"time_resolution_minutes\": 15,\r\n         \"points_per_hour\": 4,\r\n-        \"add_time_features\": True,\r\n-        \"add_holiday_features\": True,\r\n-        \"add_weather_features\": False,\r\n-        \"weather_variables\": [\"temperature\", \"humidity\", \"wind_speed\"]\r\n+        \"add_time_features\": True,    # Add cyclical time features\r\n+        \"add_holiday_features\": True,  # Add holiday indicators\r\n+        \"add_weather_features\": False  # Optional weather features if available\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733066618566,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,38 +12,38 @@\n     \"\"\"Get default configuration parameters optimized for 15-min interval data.\"\"\"\r\n     model_params = {\r\n         \"input_features\": 10,\r\n         \"output_features\": 10,\r\n-        \"d_model\": 256,         # Increased model capacity\r\n+        \"d_model\": 256,\r\n         \"n_heads\": 8,\r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 1024,          # Increased feedforward capacity\r\n+        \"d_ff\": 1024,\r\n         \"dropout\": 0.2,\r\n-        \"max_seq_length\": 96,   # 24 hours * 4 intervals per hour\r\n-        \"transformer_labels_count\": 48,  # 12 hours * 4 intervals\r\n+        \"max_seq_length\": 96,\r\n+        \"transformer_labels_count\": 48\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.0005,  # Reduced learning rate for stability\r\n-        \"max_epochs\": 150,       # Increased epochs\r\n+        \"learning_rate\": 0.0005,\r\n+        \"max_epochs\": 150,\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,        # Increased batch size\r\n+        \"batch_size\": 64,\r\n         \"device\": \"cuda\",\r\n         \"transformer_labels_count\": 48,\r\n         \"forecasting_horizon\": 48,\r\n         \"transformer_use_teacher_forcing\": True,\r\n         \"gradient_clipping\": 1.0,\r\n-        \"optimizer\": \"adamw\",    # Changed to AdamW\r\n+        \"optimizer\": \"adamw\",\r\n         \"optimizer_config\": {\r\n             \"weight_decay\": 0.01,\r\n             \"betas\": (0.9, 0.999),\r\n             \"eps\": 1e-8\r\n         },\r\n-        \"scheduler\": \"cosine\",   # Changed to cosine annealing\r\n+        \"scheduler\": \"cosine\",\r\n         \"scheduler_config\": {\r\n-            \"T_max\": 150,        # Match max_epochs\r\n+            \"T_max\": 150,\r\n             \"eta_min\": 1e-6\r\n         }\r\n     }\r\n \r\n@@ -61,11 +61,11 @@\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \"time_resolution_minutes\": 15,\r\n         \"points_per_hour\": 4,\r\n-        \"add_time_features\": True,    # Add cyclical time features\r\n-        \"add_holiday_features\": True,  # Add holiday indicators\r\n-        \"add_weather_features\": False  # Optional weather features if available\r\n+        \"add_time_features\": True,\r\n+        \"add_holiday_features\": True,\r\n+        \"add_weather_features\": False\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733066801206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,10 +59,9 @@\n         \"one_hot_time_variables\": False,\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n-        \"time_resolution_minutes\": 15,\r\n-        \"points_per_hour\": 4,\r\n+        \"time_resolution_minutes\": 15,  # We only specify the resolution\r\n         \"add_time_features\": True,\r\n         \"add_holiday_features\": True,\r\n         \"add_weather_features\": False\r\n     }\r\n"
                }
            ],
            "date": 1733003883280,
            "name": "Commit-0",
            "content": "import argparse\r\nfrom pathlib import Path\r\nfrom typing import Dict, Any, Tuple\r\n\r\nfrom pipeline.utils.config_utils import create_pipeline_config\r\nfrom pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\nfrom models.registry.model_types import ModelType, initialize_model_registry\r\n\r\ndef get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n    \"\"\"Get default configuration parameters.\"\"\"\r\n    model_params = {\r\n        \"input_features\": 10,\r\n        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n        \"d_model\": 128,       # Increased from 64\r\n        \"n_heads\": 8,         # Increased from 4\r\n        \"n_encoder_layers\": 4,\r\n        \"n_decoder_layers\": 4,\r\n        \"d_ff\": 512,         # Increased from 256\r\n        \"dropout\": 0.2,      # Increased from 0.1\r\n        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n        \"transformer_labels_count\": 12\r\n    }\r\n\r\n    training_params = {\r\n        \"learning_rate\": 0.0001,  # Reduced from 0.001\r\n        \"max_epochs\": 100,\r\n        \"use_early_stopping\": True,\r\n        \"early_stopping_patience\": 15,  # Increased from 10\r\n        \"batch_size\": 64,        # Increased from 32\r\n        \"device\": \"cuda\",\r\n        \"transformer_labels_count\": 12,\r\n        \"forecasting_horizon\": 12,\r\n        \"transformer_use_teacher_forcing\": True,\r\n        \"gradient_clip_val\": 1.0  # Add gradient clipping\r\n    }\r\n\r\n    dataset_params = {\r\n        \"time_variable\": \"utc_timestamp\",\r\n        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n        \"time_series_window_in_hours\": 24,\r\n        \"forecasting_horizon_in_hours\": 12,\r\n        \"is_single_time_point_prediction\": False,\r\n        \"include_time_information\": True,\r\n        \"is_training_set\": True,\r\n        \"labels_count\": 12,\r\n        \"one_hot_time_variables\": False,\r\n        \"normalize_data\": True,  # Add data normalization\r\n        \"scaling_method\": \"standard\",  # Use standard scaling\r\n    }\r\n\r\n    return model_params, training_params, dataset_params\r\n\r\ndef train_model(data_path: str) -> None:\r\n    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n    # Initialize model registry first\r\n    initialize_model_registry()\r\n    \r\n    # Debug: Print available models\r\n    from models.registry.factory import ModelFactory\r\n    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n    \r\n    model_params, training_params, dataset_params = get_default_config_params()\r\n    \r\n    # Create pipeline configuration\r\n    config = create_pipeline_config(\r\n        data_path=data_path,\r\n        model_type=ModelType.VANILLA_TRANSFORMER,\r\n        model_params=model_params,\r\n        training_params=training_params,\r\n        dataset_params=dataset_params\r\n    )\r\n    \r\n    # Initialize and run pipeline\r\n    pipeline = TimeSeriesPipeline(config)\r\n    experiment = pipeline.run()\r\n    \r\n    # Print results\r\n    if experiment is None:\r\n        print(\"Error: Training failed to produce results\")\r\n        return\r\n        \r\n    print(\"\\nTraining Complete!\")\r\n    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n    \r\n    print(\"\\nEvaluation Metrics:\")\r\n    for metric, value in experiment.evaluation.total_metrics.items():\r\n        print(f\"{metric}: {value:.4f}\")\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n    args = parser.parse_args()\r\n    \r\n    train_model(args.data)\r\n\r\nif __name__ == '__main__':\r\n    main()"
        }
    ]
}