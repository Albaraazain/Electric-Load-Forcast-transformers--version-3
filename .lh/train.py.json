{
    "sourceFile": "train.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 40,
            "patches": [
                {
                    "date": 1733003883280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733003949096,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,11 +42,9 @@\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n         \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,  # Add data normalization\r\n-        \"scaling_method\": \"standard\",  # Use standard scaling\r\n+        \"one_hot_time_variables\": False\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733004103522,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,18 +21,18 @@\n         \"transformer_labels_count\": 12\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.0001,  # Reduced from 0.001\r\n+        \"learning_rate\": 0.0001,\r\n         \"max_epochs\": 100,\r\n         \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,  # Increased from 10\r\n-        \"batch_size\": 64,        # Increased from 32\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,\r\n         \"device\": \"cuda\",\r\n         \"transformer_labels_count\": 12,\r\n         \"forecasting_horizon\": 12,\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clip_val\": 1.0  # Add gradient clipping\r\n+        \"gradient_clip_val\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n"
                },
                {
                    "date": 1733004211124,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,96 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"Get default configuration parameters.\"\"\"\r\n+    model_params = {\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n+        \"d_model\": 128,       # Increased from 64\r\n+        \"n_heads\": 8,         # Increased from 4\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 512,         # Increased from 256\r\n+        \"dropout\": 0.2,      # Increased from 0.1\r\n+        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n+        \"transformer_labels_count\": 12\r\n+    }\r\n+\r\n+    training_params = {\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,\r\n+        \"device\": \"cuda\",\r\n+        \"transformer_labels_count\": 12,\r\n+        \"forecasting_horizon\": 12,\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n+        \"time_series_window_in_hours\": 24,\r\n+        \"forecasting_horizon_in_hours\": 12,\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": 12,\r\n+        \"one_hot_time_variables\": False\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    # Initialize model registry first\r\n+    initialize_model_registry()\r\n+    \r\n+    # Debug: Print available models\r\n+    from models.registry.factory import ModelFactory\r\n+    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n+    \r\n+    model_params, training_params, dataset_params = get_default_config_params()\r\n+    \r\n+    # Create pipeline configuration\r\n+    config = create_pipeline_config(\r\n+        data_path=data_path,\r\n+        model_type=ModelType.VANILLA_TRANSFORMER,\r\n+        model_params=model_params,\r\n+        training_params=training_params,\r\n+        dataset_params=dataset_params\r\n+    )\r\n+    \r\n+    # Initialize and run pipeline\r\n+    pipeline = TimeSeriesPipeline(config)\r\n+    experiment = pipeline.run()\r\n+    \r\n+    # Print results\r\n+    if experiment is None:\r\n+        print(\"Error: Training failed to produce results\")\r\n+        return\r\n+        \r\n+    print(\"\\nTraining Complete!\")\r\n+    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n+    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+    \r\n+    print(\"\\nEvaluation Metrics:\")\r\n+    for metric, value in experiment.evaluation.total_metrics.items():\r\n+        print(f\"{metric}: {value:.4f}\")\r\n+\r\n+def main():\r\n+    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n+    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    args = parser.parse_args()\r\n+    \r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733004852064,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,31 +8,31 @@\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get default configuration parameters.\"\"\"\r\n     model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n-        \"d_model\": 128,       # Increased from 64\r\n-        \"n_heads\": 8,         # Increased from 4\r\n+        \"input_features\": 1,     # Changed from 10 to 1 since you have only one input feature\r\n+        \"output_features\": 1,    # Correct, already 1\r\n+        \"d_model\": 128,         \r\n+        \"n_heads\": 8,           \r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,         # Increased from 256\r\n-        \"dropout\": 0.2,      # Increased from 0.1\r\n-        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n-        \"transformer_labels_count\": 12\r\n+        \"d_ff\": 512,           \r\n+        \"dropout\": 0.2,        \r\n+        \"max_seq_length\": 96,    # Changed to 96 (24 hours * 4 readings per hour)\r\n+        \"transformer_labels_count\": 48  # Changed to 48 (12 hours * 4 readings per hour)\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n+        \"learning_rate\": 0.001,  # Increased slightly as 0.0001 might be too small\r\n         \"max_epochs\": 100,\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n+        \"batch_size\": 32,        # Reduced from 64 to start\r\n         \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n+        \"transformer_labels_count\": 48,  # Match with model_params\r\n+        \"forecasting_horizon\": 48,       # Changed to match 15-minute intervals\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+        \"gradient_clipping\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n@@ -41,10 +41,12 @@\n         \"forecasting_horizon_in_hours\": 12,\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False\r\n+        \"labels_count\": 48,               # Changed to match 15-minute intervals\r\n+        \"one_hot_time_variables\": False,\r\n+        \"normalize_data\": True,           # Added normalization\r\n+        \"scaling_method\": \"standard\"      # Added scaling method\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n@@ -92,101 +94,5 @@\n     \r\n     train_model(args.data)\r\n \r\n if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters.\"\"\"\r\n-    model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n-        \"d_model\": 128,       # Increased from 64\r\n-        \"n_heads\": 8,         # Increased from 4\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,         # Increased from 256\r\n-        \"dropout\": 0.2,      # Increased from 0.1\r\n-        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n-        \"transformer_labels_count\": 12\r\n-    }\r\n-\r\n-    training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clip_val\": 1.0\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n-    # Initialize model registry first\r\n-    initialize_model_registry()\r\n-    \r\n-    # Debug: Print available models\r\n-    from models.registry.factory import ModelFactory\r\n-    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n-    \r\n-    model_params, training_params, dataset_params = get_default_config_params()\r\n-    \r\n-    # Create pipeline configuration\r\n-    config = create_pipeline_config(\r\n-        data_path=data_path,\r\n-        model_type=ModelType.VANILLA_TRANSFORMER,\r\n-        model_params=model_params,\r\n-        training_params=training_params,\r\n-        dataset_params=dataset_params\r\n-    )\r\n-    \r\n-    # Initialize and run pipeline\r\n-    pipeline = TimeSeriesPipeline(config)\r\n-    experiment = pipeline.run()\r\n-    \r\n-    # Print results\r\n-    if experiment is None:\r\n-        print(\"Error: Training failed to produce results\")\r\n-        return\r\n-        \r\n-    print(\"\\nTraining Complete!\")\r\n-    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n-    \r\n-    print(\"\\nEvaluation Metrics:\")\r\n-    for metric, value in experiment.evaluation.total_metrics.items():\r\n-        print(f\"{metric}: {value:.4f}\")\r\n-\r\n-def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n-    args = parser.parse_args()\r\n-    \r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733004933464,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,31 +8,31 @@\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get default configuration parameters.\"\"\"\r\n     model_params = {\r\n-        \"input_features\": 1,     # Changed from 10 to 1 since you have only one input feature\r\n-        \"output_features\": 1,    # Correct, already 1\r\n-        \"d_model\": 128,         \r\n-        \"n_heads\": 8,           \r\n+        \"input_features\": 10,\r\n+        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n+        \"d_model\": 128,       # Increased from 64\r\n+        \"n_heads\": 8,         # Increased from 4\r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,           \r\n-        \"dropout\": 0.2,        \r\n-        \"max_seq_length\": 96,    # Changed to 96 (24 hours * 4 readings per hour)\r\n-        \"transformer_labels_count\": 48  # Changed to 48 (12 hours * 4 readings per hour)\r\n+        \"d_ff\": 512,         # Increased from 256\r\n+        \"dropout\": 0.2,      # Increased from 0.1\r\n+        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n+        \"transformer_labels_count\": 12\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.001,  # Increased slightly as 0.0001 might be too small\r\n+        \"learning_rate\": 0.0001,\r\n         \"max_epochs\": 100,\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 32,        # Reduced from 64 to start\r\n+        \"batch_size\": 64,\r\n         \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 48,  # Match with model_params\r\n-        \"forecasting_horizon\": 48,       # Changed to match 15-minute intervals\r\n+        \"transformer_labels_count\": 12,\r\n+        \"forecasting_horizon\": 12,\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0\r\n+        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n@@ -41,12 +41,10 @@\n         \"forecasting_horizon_in_hours\": 12,\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 48,               # Changed to match 15-minute intervals\r\n-        \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,           # Added normalization\r\n-        \"scaling_method\": \"standard\"      # Added scaling method\r\n+        \"labels_count\": 12,\r\n+        \"one_hot_time_variables\": False\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733006957066,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,101 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from sklearn.discriminant_analysis import StandardScaler\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"Get default configuration parameters.\"\"\"\r\n+    model_params = {\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n+        \"d_model\": 128,       # Increased from 64\r\n+        \"n_heads\": 8,         # Increased from 4\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 512,         # Increased from 256\r\n+        \"dropout\": 0.2,      # Increased from 0.1\r\n+        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n+        \"transformer_labels_count\": 12\r\n+    }\r\n+\r\n+    training_params = {\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,\r\n+        \"device\": \"cuda\",\r\n+        \"transformer_labels_count\": 12,\r\n+        \"forecasting_horizon\": 12,\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n+        \"time_series_window_in_hours\": 24,\r\n+        \"forecasting_horizon_in_hours\": 12,\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": 12,\r\n+        \"one_hot_time_variables\": False,\r\n+        \"normalize_data\": True,  # Add this\r\n+        \"scaling_method\": \"standard\",  # Add this\r\n+        \"time_series_scaler\": StandardScaler()  # Add this\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    # Initialize model registry first\r\n+    initialize_model_registry()\r\n+    \r\n+    # Debug: Print available models\r\n+    from models.registry.factory import ModelFactory\r\n+    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n+    \r\n+    model_params, training_params, dataset_params = get_default_config_params()\r\n+    \r\n+    # Create pipeline configuration\r\n+    config = create_pipeline_config(\r\n+        data_path=data_path,\r\n+        model_type=ModelType.VANILLA_TRANSFORMER,\r\n+        model_params=model_params,\r\n+        training_params=training_params,\r\n+        dataset_params=dataset_params\r\n+    )\r\n+    \r\n+    # Initialize and run pipeline\r\n+    pipeline = TimeSeriesPipeline(config)\r\n+    experiment = pipeline.run()\r\n+    \r\n+    # Print results\r\n+    if experiment is None:\r\n+        print(\"Error: Training failed to produce results\")\r\n+        return\r\n+        \r\n+    print(\"\\nTraining Complete!\")\r\n+    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n+    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+    \r\n+    print(\"\\nEvaluation Metrics:\")\r\n+    for metric, value in experiment.evaluation.total_metrics.items():\r\n+        print(f\"{metric}: {value:.4f}\")\r\n+\r\n+def main():\r\n+    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n+    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    args = parser.parse_args()\r\n+    \r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733006972835,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,101 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from sklearn.discriminant_analysis import StandardScaler\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"Get default configuration parameters.\"\"\"\r\n+    model_params = {\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 10,  # Change to 10 to match input dimensions\r\n+        \"d_model\": 128,\r\n+        \"n_heads\": 8,\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 512,\r\n+        \"dropout\": 0.2,\r\n+        \"max_seq_length\": 168,\r\n+        \"transformer_labels_count\": 12\r\n+    }\r\n+\r\n+    training_params = {\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,\r\n+        \"device\": \"cuda\",\r\n+        \"transformer_labels_count\": 12,\r\n+        \"forecasting_horizon\": 12,\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n+        \"time_series_window_in_hours\": 24,\r\n+        \"forecasting_horizon_in_hours\": 12,\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": 12,\r\n+        \"one_hot_time_variables\": False,\r\n+        \"normalize_data\": True,  # Add this\r\n+        \"scaling_method\": \"standard\",  # Add this\r\n+        \"time_series_scaler\": StandardScaler()  # Add this\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    # Initialize model registry first\r\n+    initialize_model_registry()\r\n+    \r\n+    # Debug: Print available models\r\n+    from models.registry.factory import ModelFactory\r\n+    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n+    \r\n+    model_params, training_params, dataset_params = get_default_config_params()\r\n+    \r\n+    # Create pipeline configuration\r\n+    config = create_pipeline_config(\r\n+        data_path=data_path,\r\n+        model_type=ModelType.VANILLA_TRANSFORMER,\r\n+        model_params=model_params,\r\n+        training_params=training_params,\r\n+        dataset_params=dataset_params\r\n+    )\r\n+    \r\n+    # Initialize and run pipeline\r\n+    pipeline = TimeSeriesPipeline(config)\r\n+    experiment = pipeline.run()\r\n+    \r\n+    # Print results\r\n+    if experiment is None:\r\n+        print(\"Error: Training failed to produce results\")\r\n+        return\r\n+        \r\n+    print(\"\\nTraining Complete!\")\r\n+    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n+    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+    \r\n+    print(\"\\nEvaluation Metrics:\")\r\n+    for metric, value in experiment.evaluation.total_metrics.items():\r\n+        print(f\"{metric}: {value:.4f}\")\r\n+\r\n+def main():\r\n+    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n+    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    args = parser.parse_args()\r\n+    \r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733006982786,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,18 +23,18 @@\n         \"transformer_labels_count\": 12\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n+        \"learning_rate\": 0.001,  # Increase learning rate\r\n         \"max_epochs\": 100,\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n+        \"batch_size\": 32,  # Reduce batch size\r\n         \"device\": \"cuda\",\r\n         \"transformer_labels_count\": 12,\r\n         \"forecasting_horizon\": 12,\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n+        \"gradient_clipping\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n@@ -97,202 +97,5 @@\n     \r\n     train_model(args.data)\r\n \r\n if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from sklearn.discriminant_analysis import StandardScaler\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters.\"\"\"\r\n-    model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n-        \"d_model\": 128,       # Increased from 64\r\n-        \"n_heads\": 8,         # Increased from 4\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,         # Increased from 256\r\n-        \"dropout\": 0.2,      # Increased from 0.1\r\n-        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n-        \"transformer_labels_count\": 12\r\n-    }\r\n-\r\n-    training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,  # Add this\r\n-        \"scaling_method\": \"standard\",  # Add this\r\n-        \"time_series_scaler\": StandardScaler()  # Add this\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n-    # Initialize model registry first\r\n-    initialize_model_registry()\r\n-    \r\n-    # Debug: Print available models\r\n-    from models.registry.factory import ModelFactory\r\n-    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n-    \r\n-    model_params, training_params, dataset_params = get_default_config_params()\r\n-    \r\n-    # Create pipeline configuration\r\n-    config = create_pipeline_config(\r\n-        data_path=data_path,\r\n-        model_type=ModelType.VANILLA_TRANSFORMER,\r\n-        model_params=model_params,\r\n-        training_params=training_params,\r\n-        dataset_params=dataset_params\r\n-    )\r\n-    \r\n-    # Initialize and run pipeline\r\n-    pipeline = TimeSeriesPipeline(config)\r\n-    experiment = pipeline.run()\r\n-    \r\n-    # Print results\r\n-    if experiment is None:\r\n-        print(\"Error: Training failed to produce results\")\r\n-        return\r\n-        \r\n-    print(\"\\nTraining Complete!\")\r\n-    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n-    \r\n-    print(\"\\nEvaluation Metrics:\")\r\n-    for metric, value in experiment.evaluation.total_metrics.items():\r\n-        print(f\"{metric}: {value:.4f}\")\r\n-\r\n-def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n-    args = parser.parse_args()\r\n-    \r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters.\"\"\"\r\n-    model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n-        \"d_model\": 128,       # Increased from 64\r\n-        \"n_heads\": 8,         # Increased from 4\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,         # Increased from 256\r\n-        \"dropout\": 0.2,      # Increased from 0.1\r\n-        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n-        \"transformer_labels_count\": 12\r\n-    }\r\n-\r\n-    training_params = {\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0  # Changed from gradient_clip_val to gradient_clipping\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n-        \"one_hot_time_variables\": False\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n-    # Initialize model registry first\r\n-    initialize_model_registry()\r\n-    \r\n-    # Debug: Print available models\r\n-    from models.registry.factory import ModelFactory\r\n-    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n-    \r\n-    model_params, training_params, dataset_params = get_default_config_params()\r\n-    \r\n-    # Create pipeline configuration\r\n-    config = create_pipeline_config(\r\n-        data_path=data_path,\r\n-        model_type=ModelType.VANILLA_TRANSFORMER,\r\n-        model_params=model_params,\r\n-        training_params=training_params,\r\n-        dataset_params=dataset_params\r\n-    )\r\n-    \r\n-    # Initialize and run pipeline\r\n-    pipeline = TimeSeriesPipeline(config)\r\n-    experiment = pipeline.run()\r\n-    \r\n-    # Print results\r\n-    if experiment is None:\r\n-        print(\"Error: Training failed to produce results\")\r\n-        return\r\n-        \r\n-    print(\"\\nTraining Complete!\")\r\n-    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n-    \r\n-    print(\"\\nEvaluation Metrics:\")\r\n-    for metric, value in experiment.evaluation.total_metrics.items():\r\n-        print(f\"{metric}: {value:.4f}\")\r\n-\r\n-def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n-    args = parser.parse_args()\r\n-    \r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733061971728,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,17 +11,17 @@\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get default configuration parameters.\"\"\"\r\n     model_params = {\r\n         \"input_features\": 10,\r\n-        \"output_features\": 10,  # Change to 10 to match input dimensions\r\n+        \"output_features\": 10,\r\n         \"d_model\": 128,\r\n         \"n_heads\": 8,\r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n         \"d_ff\": 512,\r\n         \"dropout\": 0.2,\r\n-        \"max_seq_length\": 168,\r\n-        \"transformer_labels_count\": 12\r\n+        \"max_seq_length\": 672,  # Updated: 28 days * 24 hours * 4 intervals\r\n+        \"transformer_labels_count\": 48   # Updated: 12 hours * 4 intervals\r\n     }\r\n \r\n     training_params = {\r\n         \"learning_rate\": 0.001,  # Increase learning rate\r\n@@ -29,27 +29,29 @@\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n         \"batch_size\": 32,  # Reduce batch size\r\n         \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 12,\r\n-        \"forecasting_horizon\": 12,\r\n+        \"transformer_labels_count\": 48,  # Updated: 12 hours * 4 intervals\r\n+        \"forecasting_horizon\": 48,       # Updated: 12 hours * 4 intervals\r\n         \"transformer_use_teacher_forcing\": True,\r\n         \"gradient_clipping\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n+        \"target_variable\": \"energy_consumption\",  # Updated to match new column name\r\n+        \"time_series_window_in_hours\": 24,       # Will automatically use 96 points (24*4)\r\n+        \"forecasting_horizon_in_hours\": 12,      # Will automatically use 48 points (12*4)\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 12,\r\n+        \"labels_count\": 48,                      # Updated: 12 hours * 4 intervals\r\n         \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,  # Add this\r\n-        \"scaling_method\": \"standard\",  # Add this\r\n-        \"time_series_scaler\": StandardScaler()  # Add this\r\n+        \"normalize_data\": True,\r\n+        \"scaling_method\": \"standard\",\r\n+        \"time_series_scaler\": StandardScaler(),\r\n+        \"time_resolution_minutes\": 15,           # Added to specify interval\r\n+        \"points_per_hour\": 4                     # Added to specify points per hour\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733062194876,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,10 +18,10 @@\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n         \"d_ff\": 512,\r\n         \"dropout\": 0.2,\r\n-        \"max_seq_length\": 672,  # Updated: 28 days * 24 hours * 4 intervals\r\n-        \"transformer_labels_count\": 48   # Updated: 12 hours * 4 intervals\r\n+        \"max_seq_length\": 96,  # 24 hours * 4 intervals\r\n+        \"transformer_labels_count\": 48  # 12 hours * 4 intervals\r\n     }\r\n \r\n     training_params = {\r\n         \"learning_rate\": 0.001,  # Increase learning rate\r\n@@ -29,23 +29,23 @@\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n         \"batch_size\": 32,  # Reduce batch size\r\n         \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 48,  # Updated: 12 hours * 4 intervals\r\n-        \"forecasting_horizon\": 48,       # Updated: 12 hours * 4 intervals\r\n+        \"transformer_labels_count\": 48,  # 12 hours * 4 intervals\r\n+        \"forecasting_horizon\": 48,       # 12 hours * 4 intervals\r\n         \"transformer_use_teacher_forcing\": True,\r\n         \"gradient_clipping\": 1.0\r\n     }\r\n \r\n     dataset_params = {\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",  # Updated to match new column name\r\n-        \"time_series_window_in_hours\": 24,       # Will automatically use 96 points (24*4)\r\n-        \"forecasting_horizon_in_hours\": 12,      # Will automatically use 48 points (12*4)\r\n+        \"time_series_window_in_hours\": 24,       # Will be multiplied by points_per_hour\r\n+        \"forecasting_horizon_in_hours\": 12,      # Will be multiplied by points_per_hour\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 48,                      # Updated: 12 hours * 4 intervals\r\n+        \"labels_count\": 48,                      # 12 hours * 4 intervals\r\n         \"one_hot_time_variables\": False,\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n"
                },
                {
                    "date": 1733065770885,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,129 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from sklearn.preprocessing import StandardScaler\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"Get default configuration parameters optimized for 15-min interval data.\"\"\"\r\n+    model_params = {\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 10,\r\n+        \"d_model\": 256,         # Increased model capacity\r\n+        \"n_heads\": 8,\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 1024,          # Increased feedforward capacity\r\n+        \"dropout\": 0.2,\r\n+        \"max_seq_length\": 96,   # 24 hours * 4 intervals per hour\r\n+        \"transformer_labels_count\": 48,  # 12 hours * 4 intervals\r\n+    }\r\n+\r\n+    training_params = {\r\n+        \"learning_rate\": 0.0005,  # Reduced learning rate for stability\r\n+        \"max_epochs\": 150,       # Increased epochs\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"batch_size\": 64,        # Increased batch size\r\n+        \"device\": \"cuda\",\r\n+        \"transformer_labels_count\": 48,\r\n+        \"forecasting_horizon\": 48,\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"gradient_clipping\": 1.0,\r\n+        \"optimizer\": \"adamw\",    # Changed to AdamW\r\n+        \"optimizer_config\": {\r\n+            \"weight_decay\": 0.01,\r\n+            \"betas\": (0.9, 0.999),\r\n+            \"eps\": 1e-8\r\n+        },\r\n+        \"scheduler\": \"cosine\",   # Changed to cosine annealing\r\n+        \"scheduler_config\": {\r\n+            \"T_max\": 150,        # Match max_epochs\r\n+            \"eta_min\": 1e-6\r\n+        }\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"energy_consumption\",\r\n+        \"time_series_window_in_hours\": 24,\r\n+        \"forecasting_horizon_in_hours\": 12,\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": 48,\r\n+        \"one_hot_time_variables\": False,\r\n+        \"normalize_data\": True,\r\n+        \"scaling_method\": \"standard\",\r\n+        \"time_series_scaler\": StandardScaler(),\r\n+        \"time_resolution_minutes\": 15,\r\n+        \"points_per_hour\": 4,\r\n+        \"add_time_features\": True,    # Add cyclical time features\r\n+        \"add_holiday_features\": True,  # Add holiday indicators\r\n+        \"add_weather_features\": False  # Optional weather features if available\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    try:\r\n+        # Initialize model registry\r\n+        initialize_model_registry()\r\n+        \r\n+        # Get configuration parameters\r\n+        model_params, training_params, dataset_params = get_default_config_params()\r\n+        \r\n+        # Create pipeline configuration\r\n+        config = create_pipeline_config(\r\n+            data_path=data_path,\r\n+            model_type=ModelType.VANILLA_TRANSFORMER,\r\n+            model_params=model_params,\r\n+            training_params=training_params,\r\n+            dataset_params=dataset_params\r\n+        )\r\n+        \r\n+        # Initialize and run pipeline\r\n+        pipeline = TimeSeriesPipeline(config)\r\n+        experiment = pipeline.run()\r\n+        \r\n+        if experiment is None:\r\n+            raise RuntimeError(\"Training failed to produce results\")\r\n+        \r\n+        # Print results\r\n+        print(\"\\nTraining Complete!\")\r\n+        print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n+        print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+        \r\n+        print(\"\\nEvaluation Metrics:\")\r\n+        for metric, value in experiment.evaluation.total_metrics.items():\r\n+            print(f\"{metric}: {value:.4f}\")\r\n+        \r\n+        # Print training history\r\n+        print(\"\\nTraining History:\")\r\n+        final_train_loss = experiment.training_report.train_losses[-1]\r\n+        final_val_loss = experiment.training_report.val_losses[-1]\r\n+        print(f\"Final Training Loss: {final_train_loss:.4f}\")\r\n+        print(f\"Final Validation Loss: {final_val_loss:.4f}\")\r\n+        \r\n+        # Save the experiment\r\n+        experiment.save_to_json_file()\r\n+        print(f\"\\nExperiment saved successfully\")\r\n+        \r\n+    except Exception as e:\r\n+        print(f\"Error during training: {str(e)}\")\r\n+        raise\r\n+\r\n+def main():\r\n+    parser = argparse.ArgumentParser(description='Train transformer model for 15-min interval energy forecasting')\r\n+    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    args = parser.parse_args()\r\n+    \r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733065822550,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -104,12 +104,19 @@\n             print(f\"{metric}: {value:.4f}\")\r\n         \r\n         # Print training history\r\n         print(\"\\nTraining History:\")\r\n-        final_train_loss = experiment.training_report.train_losses[-1]\r\n-        final_val_loss = experiment.training_report.val_losses[-1]\r\n-        print(f\"Final Training Loss: {final_train_loss:.4f}\")\r\n-        print(f\"Final Validation Loss: {final_val_loss:.4f}\")\r\n+        if experiment.training_report and hasattr(experiment.training_report, 'train_losses'):\r\n+            final_train_loss = experiment.training_report.train_losses[-1]\r\n+            print(f\"Final Training Loss: {final_train_loss:.4f}\")\r\n+        else:\r\n+            print(\"Training loss data not available\")\r\n+\r\n+        if experiment.training_report and hasattr(experiment.training_report, 'val_losses'):\r\n+            final_val_loss = experiment.training_report.val_losses[-1]\r\n+            print(f\"Final Validation Loss: {final_val_loss:.4f}\")\r\n+        else:\r\n+            print(\"Validation loss data not available\")\r\n         \r\n         # Save the experiment\r\n         experiment.save_to_json_file()\r\n         print(f\"\\nExperiment saved successfully\")\r\n"
                },
                {
                    "date": 1733065828395,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -132,108 +132,5 @@\n     \r\n     train_model(args.data)\r\n \r\n if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from sklearn.discriminant_analysis import StandardScaler\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters.\"\"\"\r\n-    model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 10,\r\n-        \"d_model\": 128,\r\n-        \"n_heads\": 8,\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 512,\r\n-        \"dropout\": 0.2,\r\n-        \"max_seq_length\": 96,  # 24 hours * 4 intervals\r\n-        \"transformer_labels_count\": 48  # 12 hours * 4 intervals\r\n-    }\r\n-\r\n-    training_params = {\r\n-        \"learning_rate\": 0.001,  # Increase learning rate\r\n-        \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 32,  # Reduce batch size\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 48,  # 12 hours * 4 intervals\r\n-        \"forecasting_horizon\": 48,       # 12 hours * 4 intervals\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"gradient_clipping\": 1.0\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"energy_consumption\",  # Updated to match new column name\r\n-        \"time_series_window_in_hours\": 24,       # Will be multiplied by points_per_hour\r\n-        \"forecasting_horizon_in_hours\": 12,      # Will be multiplied by points_per_hour\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": 48,                      # 12 hours * 4 intervals\r\n-        \"one_hot_time_variables\": False,\r\n-        \"normalize_data\": True,\r\n-        \"scaling_method\": \"standard\",\r\n-        \"time_series_scaler\": StandardScaler(),\r\n-        \"time_resolution_minutes\": 15,           # Added to specify interval\r\n-        \"points_per_hour\": 4                     # Added to specify points per hour\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n-    # Initialize model registry first\r\n-    initialize_model_registry()\r\n-    \r\n-    # Debug: Print available models\r\n-    from models.registry.factory import ModelFactory\r\n-    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n-    \r\n-    model_params, training_params, dataset_params = get_default_config_params()\r\n-    \r\n-    # Create pipeline configuration\r\n-    config = create_pipeline_config(\r\n-        data_path=data_path,\r\n-        model_type=ModelType.VANILLA_TRANSFORMER,\r\n-        model_params=model_params,\r\n-        training_params=training_params,\r\n-        dataset_params=dataset_params\r\n-    )\r\n-    \r\n-    # Initialize and run pipeline\r\n-    pipeline = TimeSeriesPipeline(config)\r\n-    experiment = pipeline.run()\r\n-    \r\n-    # Print results\r\n-    if experiment is None:\r\n-        print(\"Error: Training failed to produce results\")\r\n-        return\r\n-        \r\n-    print(\"\\nTraining Complete!\")\r\n-    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n-    \r\n-    print(\"\\nEvaluation Metrics:\")\r\n-    for metric, value in experiment.evaluation.total_metrics.items():\r\n-        print(f\"{metric}: {value:.4f}\")\r\n-\r\n-def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n-    args = parser.parse_args()\r\n-    \r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733066459142,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,11 +61,12 @@\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \"time_resolution_minutes\": 15,\r\n         \"points_per_hour\": 4,\r\n-        \"add_time_features\": True,    # Add cyclical time features\r\n-        \"add_holiday_features\": True,  # Add holiday indicators\r\n-        \"add_weather_features\": False  # Optional weather features if available\r\n+        \"add_time_features\": True,\r\n+        \"add_holiday_features\": True,\r\n+        \"add_weather_features\": False,\r\n+        \"weather_variables\": [\"temperature\", \"humidity\", \"wind_speed\"]\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733066558381,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,12 +61,11 @@\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \"time_resolution_minutes\": 15,\r\n         \"points_per_hour\": 4,\r\n-        \"add_time_features\": True,\r\n-        \"add_holiday_features\": True,\r\n-        \"add_weather_features\": False,\r\n-        \"weather_variables\": [\"temperature\", \"humidity\", \"wind_speed\"]\r\n+        \"add_time_features\": True,    # Add cyclical time features\r\n+        \"add_holiday_features\": True,  # Add holiday indicators\r\n+        \"add_weather_features\": False  # Optional weather features if available\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733066618566,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,38 +12,38 @@\n     \"\"\"Get default configuration parameters optimized for 15-min interval data.\"\"\"\r\n     model_params = {\r\n         \"input_features\": 10,\r\n         \"output_features\": 10,\r\n-        \"d_model\": 256,         # Increased model capacity\r\n+        \"d_model\": 256,\r\n         \"n_heads\": 8,\r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 1024,          # Increased feedforward capacity\r\n+        \"d_ff\": 1024,\r\n         \"dropout\": 0.2,\r\n-        \"max_seq_length\": 96,   # 24 hours * 4 intervals per hour\r\n-        \"transformer_labels_count\": 48,  # 12 hours * 4 intervals\r\n+        \"max_seq_length\": 96,\r\n+        \"transformer_labels_count\": 48\r\n     }\r\n \r\n     training_params = {\r\n-        \"learning_rate\": 0.0005,  # Reduced learning rate for stability\r\n-        \"max_epochs\": 150,       # Increased epochs\r\n+        \"learning_rate\": 0.0005,\r\n+        \"max_epochs\": 150,\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,        # Increased batch size\r\n+        \"batch_size\": 64,\r\n         \"device\": \"cuda\",\r\n         \"transformer_labels_count\": 48,\r\n         \"forecasting_horizon\": 48,\r\n         \"transformer_use_teacher_forcing\": True,\r\n         \"gradient_clipping\": 1.0,\r\n-        \"optimizer\": \"adamw\",    # Changed to AdamW\r\n+        \"optimizer\": \"adamw\",\r\n         \"optimizer_config\": {\r\n             \"weight_decay\": 0.01,\r\n             \"betas\": (0.9, 0.999),\r\n             \"eps\": 1e-8\r\n         },\r\n-        \"scheduler\": \"cosine\",   # Changed to cosine annealing\r\n+        \"scheduler\": \"cosine\",\r\n         \"scheduler_config\": {\r\n-            \"T_max\": 150,        # Match max_epochs\r\n+            \"T_max\": 150,\r\n             \"eta_min\": 1e-6\r\n         }\r\n     }\r\n \r\n@@ -61,11 +61,11 @@\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \"time_resolution_minutes\": 15,\r\n         \"points_per_hour\": 4,\r\n-        \"add_time_features\": True,    # Add cyclical time features\r\n-        \"add_holiday_features\": True,  # Add holiday indicators\r\n-        \"add_weather_features\": False  # Optional weather features if available\r\n+        \"add_time_features\": True,\r\n+        \"add_holiday_features\": True,\r\n+        \"add_weather_features\": False\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733066801206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,10 +59,9 @@\n         \"one_hot_time_variables\": False,\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n-        \"time_resolution_minutes\": 15,\r\n-        \"points_per_hour\": 4,\r\n+        \"time_resolution_minutes\": 15,  # We only specify the resolution\r\n         \"add_time_features\": True,\r\n         \"add_holiday_features\": True,\r\n         \"add_weather_features\": False\r\n     }\r\n"
                },
                {
                    "date": 1733088299204,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+# path: pipeline/implementations/time_series_pipeline.py\r\n import argparse\r\n from pathlib import Path\r\n from typing import Dict, Any, Tuple\r\n \r\n"
                },
                {
                    "date": 1733091381483,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,4 @@\n-# path: pipeline/implementations/time_series_pipeline.py\r\n import argparse\r\n from pathlib import Path\r\n from typing import Dict, Any, Tuple\r\n \r\n@@ -9,69 +8,94 @@\n from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n from models.registry.model_types import ModelType, initialize_model_registry\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters optimized for 15-min interval data.\"\"\"\r\n+    \"\"\"Get default configuration parameters optimized for 15-min interval time series data.\"\"\"\r\n     model_params = {\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 10,\r\n-        \"d_model\": 256,\r\n-        \"n_heads\": 8,\r\n+        # Core model parameters\r\n+        \"input_features\": 10,  # Main value + time features\r\n+        \"output_features\": 1,  # Prediction target\r\n+        \"d_model\": 256,       # Model dimension\r\n+        \"n_heads\": 8,         # Number of attention heads\r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 1024,\r\n+        \"d_ff\": 1024,        # Feed forward dimension\r\n         \"dropout\": 0.2,\r\n-        \"max_seq_length\": 96,\r\n-        \"transformer_labels_count\": 48\r\n+        \r\n+        # Sequence parameters\r\n+        \"max_seq_length\": 96,  # 24 hours * 4 (15-min intervals)\r\n+        \"transformer_labels_count\": 48,  # 12 hours ahead prediction\r\n+        \r\n+        # Time series specific parameters\r\n+        \"kernel_size\": 3,     # For convolutional attention\r\n+        \"value_features\": 1,  # Main time series value\r\n+        \"time_features\": 9,   # Additional time-based features\r\n+        \"batch_first\": True   # Handle batch dimension first\r\n     }\r\n \r\n     training_params = {\r\n+        # Basic training parameters\r\n         \"learning_rate\": 0.0005,\r\n         \"max_epochs\": 150,\r\n+        \"batch_size\": 64,\r\n+        \"device\": \"cuda\",\r\n+        \r\n+        # Early stopping\r\n         \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n-        \"batch_size\": 64,\r\n-        \"device\": \"cuda\",\r\n-        \"transformer_labels_count\": 48,\r\n-        \"forecasting_horizon\": 48,\r\n+        \r\n+        # Time series specific\r\n+        \"transformer_labels_count\": 48,    # Match model params\r\n+        \"forecasting_horizon\": 48,         # 12 hours ahead\r\n         \"transformer_use_teacher_forcing\": True,\r\n+        \r\n+        # Optimization parameters\r\n         \"gradient_clipping\": 1.0,\r\n         \"optimizer\": \"adamw\",\r\n         \"optimizer_config\": {\r\n             \"weight_decay\": 0.01,\r\n             \"betas\": (0.9, 0.999),\r\n             \"eps\": 1e-8\r\n         },\r\n+        \r\n+        # Learning rate scheduling\r\n         \"scheduler\": \"cosine\",\r\n         \"scheduler_config\": {\r\n-            \"T_max\": 150,\r\n+            \"T_max\": 150,     # Match max_epochs\r\n             \"eta_min\": 1e-6\r\n         }\r\n     }\r\n \r\n     dataset_params = {\r\n+        # Time series data parameters\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",\r\n         \"time_series_window_in_hours\": 24,\r\n         \"forecasting_horizon_in_hours\": 12,\r\n+        \r\n+        # Feature generation\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n         \"labels_count\": 48,\r\n         \"one_hot_time_variables\": False,\r\n+        \r\n+        # Data preprocessing\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n-        \"time_resolution_minutes\": 15,  # We only specify the resolution\r\n+        \"time_resolution_minutes\": 15,\r\n+        \r\n+        # Feature flags\r\n         \"add_time_features\": True,\r\n         \"add_holiday_features\": True,\r\n         \"add_weather_features\": False\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n+    \"\"\"Train the time series transformer model using the pipeline.\"\"\"\r\n     try:\r\n         # Initialize model registry\r\n         initialize_model_registry()\r\n         \r\n@@ -80,9 +104,9 @@\n         \r\n         # Create pipeline configuration\r\n         config = create_pipeline_config(\r\n             data_path=data_path,\r\n-            model_type=ModelType.VANILLA_TRANSFORMER,\r\n+            model_type=ModelType.TIME_SERIES_TRANSFORMER,  # Changed to time series transformer\r\n             model_params=model_params,\r\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n@@ -94,9 +118,9 @@\n         if experiment is None:\r\n             raise RuntimeError(\"Training failed to produce results\")\r\n         \r\n         # Print results\r\n-        print(\"\\nTraining Complete!\")\r\n+        print(\"\\nTime Series Transformer Training Complete!\")\r\n         print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n         print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n         \r\n         print(\"\\nEvaluation Metrics:\")\r\n@@ -107,16 +131,12 @@\n         print(\"\\nTraining History:\")\r\n         if experiment.training_report and hasattr(experiment.training_report, 'train_losses'):\r\n             final_train_loss = experiment.training_report.train_losses[-1]\r\n             print(f\"Final Training Loss: {final_train_loss:.4f}\")\r\n-        else:\r\n-            print(\"Training loss data not available\")\r\n \r\n         if experiment.training_report and hasattr(experiment.training_report, 'val_losses'):\r\n             final_val_loss = experiment.training_report.val_losses[-1]\r\n             print(f\"Final Validation Loss: {final_val_loss:.4f}\")\r\n-        else:\r\n-            print(\"Validation loss data not available\")\r\n         \r\n         # Save the experiment\r\n         experiment.save_to_json_file()\r\n         print(f\"\\nExperiment saved successfully\")\r\n@@ -125,9 +145,11 @@\n         print(f\"Error during training: {str(e)}\")\r\n         raise\r\n \r\n def main():\r\n-    parser = argparse.ArgumentParser(description='Train transformer model for 15-min interval energy forecasting')\r\n+    parser = argparse.ArgumentParser(\r\n+        description='Train time series transformer model for 15-min interval energy forecasting'\r\n+    )\r\n     parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n     args = parser.parse_args()\r\n     \r\n     train_model(args.data)\r\n"
                },
                {
                    "date": 1733092299798,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     \"\"\"Get default configuration parameters optimized for 15-min interval time series data.\"\"\"\r\n     model_params = {\r\n         # Core model parameters\r\n         \"input_features\": 10,  # Main value + time features\r\n-        \"output_features\": 1,  # Prediction target\r\n+        \"output_features\": 10,  # Prediction target\r\n         \"d_model\": 256,       # Model dimension\r\n         \"n_heads\": 8,         # Number of attention heads\r\n         \"n_encoder_layers\": 4,\r\n         \"n_decoder_layers\": 4,\r\n"
                },
                {
                    "date": 1733094486474,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -33,9 +33,9 @@\n     }\r\n \r\n     training_params = {\r\n         # Basic training parameters\r\n-        \"learning_rate\": 0.0005,\r\n+        \"learning_rate\": 0.001,\r\n         \"max_epochs\": 150,\r\n         \"batch_size\": 64,\r\n         \"device\": \"cuda\",\r\n         \r\n"
                },
                {
                    "date": 1733098704894,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n from models.registry.model_types import ModelType, initialize_model_registry\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters optimized for 15-min interval time series data.\"\"\"\r\n+    \"\"\"Get default configuration parameters optimized for convolutional transformer with 15-min interval data.\"\"\"\r\n     model_params = {\r\n         # Core model parameters\r\n         \"input_features\": 10,  # Main value + time features\r\n         \"output_features\": 10,  # Prediction target\r\n@@ -24,20 +24,25 @@\n         # Sequence parameters\r\n         \"max_seq_length\": 96,  # 24 hours * 4 (15-min intervals)\r\n         \"transformer_labels_count\": 48,  # 12 hours ahead prediction\r\n         \r\n-        # Time series specific parameters\r\n-        \"kernel_size\": 3,     # For convolutional attention\r\n+        # Convolutional transformer specific parameters\r\n+        \"kernel_size\": 3,     # Size of convolutional kernel\r\n         \"value_features\": 1,  # Main time series value\r\n         \"time_features\": 9,   # Additional time-based features\r\n-        \"batch_first\": True   # Handle batch dimension first\r\n+        \"batch_first\": True,  # Handle batch dimension first\r\n+        \r\n+        # Additional conv-transformer parameters\r\n+        \"use_conv_attention\": True,\r\n+        \"conv_layers\": 2,     # Number of convolutional layers\r\n+        \"conv_dropout\": 0.1   # Dropout rate for convolutional layers\r\n     }\r\n \r\n     training_params = {\r\n         # Basic training parameters\r\n         \"learning_rate\": 0.001,\r\n         \"max_epochs\": 150,\r\n-        \"batch_size\": 64,\r\n+        \"batch_size\": 32,     # Reduced batch size due to increased complexity\r\n         \"device\": \"cuda\",\r\n         \r\n         # Early stopping\r\n         \"use_early_stopping\": True,\r\n@@ -93,9 +98,9 @@\n \r\n     return model_params, training_params, dataset_params\r\n \r\n def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the time series transformer model using the pipeline.\"\"\"\r\n+    \"\"\"Train the convolutional transformer model using the pipeline.\"\"\"\r\n     try:\r\n         # Initialize model registry\r\n         initialize_model_registry()\r\n         \r\n@@ -104,9 +109,9 @@\n         \r\n         # Create pipeline configuration\r\n         config = create_pipeline_config(\r\n             data_path=data_path,\r\n-            model_type=ModelType.TIME_SERIES_TRANSFORMER,  # Changed to time series transformer\r\n+            model_type=ModelType.CONV_TRANSFORMER,  # Changed to convolutional transformer\r\n             model_params=model_params,\r\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n@@ -118,9 +123,9 @@\n         if experiment is None:\r\n             raise RuntimeError(\"Training failed to produce results\")\r\n         \r\n         # Print results\r\n-        print(\"\\nTime Series Transformer Training Complete!\")\r\n+        print(\"\\nConvolutional Transformer Training Complete!\")\r\n         print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n         print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n         \r\n         print(\"\\nEvaluation Metrics:\")\r\n@@ -146,9 +151,9 @@\n         raise\r\n \r\n def main():\r\n     parser = argparse.ArgumentParser(\r\n-        description='Train time series transformer model for 15-min interval energy forecasting'\r\n+        description='Train convolutional transformer model for 15-min interval energy forecasting'\r\n     )\r\n     parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n     args = parser.parse_args()\r\n     \r\n"
                },
                {
                    "date": 1733100073900,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,66 +8,65 @@\n from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n from models.registry.model_types import ModelType, initialize_model_registry\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters optimized for convolutional transformer with 15-min interval data.\"\"\"\r\n+    \"\"\"Get default configuration parameters optimized for 15-min interval time series data.\"\"\"\r\n     model_params = {\r\n         # Core model parameters\r\n         \"input_features\": 10,  # Main value + time features\r\n         \"output_features\": 10,  # Prediction target\r\n-        \"d_model\": 256,       # Model dimension\r\n-        \"n_heads\": 8,         # Number of attention heads\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 1024,        # Feed forward dimension\r\n-        \"dropout\": 0.2,\r\n+        \"d_model\": 128,       # Reduced from 256 to prevent overfitting\r\n+        \"n_heads\": 4,         # Reduced from 8 to match d_model reduction\r\n+        \"n_encoder_layers\": 3, # Reduced from 4\r\n+        \"n_decoder_layers\": 3, # Reduced from 4\r\n+        \"d_ff\": 512,         # Reduced from 1024\r\n+        \"dropout\": 0.3,      # Increased from 0.2 for better regularization\r\n         \r\n         # Sequence parameters\r\n         \"max_seq_length\": 96,  # 24 hours * 4 (15-min intervals)\r\n         \"transformer_labels_count\": 48,  # 12 hours ahead prediction\r\n         \r\n-        # Convolutional transformer specific parameters\r\n-        \"kernel_size\": 3,     # Size of convolutional kernel\r\n+        # Time series specific parameters\r\n+        \"kernel_size\": 5,     # Increased from 3 for better temporal context\r\n         \"value_features\": 1,  # Main time series value\r\n         \"time_features\": 9,   # Additional time-based features\r\n         \"batch_first\": True,  # Handle batch dimension first\r\n         \r\n-        # Additional conv-transformer parameters\r\n-        \"use_conv_attention\": True,\r\n-        \"conv_layers\": 2,     # Number of convolutional layers\r\n-        \"conv_dropout\": 0.1   # Dropout rate for convolutional layers\r\n+        # Additional regularization\r\n+        \"conv_dropout\": 0.2   # Specific dropout for convolutional layers\r\n     }\r\n \r\n     training_params = {\r\n         # Basic training parameters\r\n-        \"learning_rate\": 0.001,\r\n-        \"max_epochs\": 150,\r\n-        \"batch_size\": 32,     # Reduced batch size due to increased complexity\r\n+        \"learning_rate\": 0.002,    # Increased initial learning rate\r\n+        \"max_epochs\": 100,         # Reduced from 150\r\n+        \"batch_size\": 64,          # Kept same\r\n         \"device\": \"cuda\",\r\n         \r\n         # Early stopping\r\n         \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n+        \"early_stopping_patience\": 10,  # Reduced from 15 for faster response\r\n         \r\n         # Time series specific\r\n-        \"transformer_labels_count\": 48,    # Match model params\r\n-        \"forecasting_horizon\": 48,         # 12 hours ahead\r\n+        \"transformer_labels_count\": 48,\r\n+        \"forecasting_horizon\": 48,\r\n         \"transformer_use_teacher_forcing\": True,\r\n         \r\n         # Optimization parameters\r\n-        \"gradient_clipping\": 1.0,\r\n+        \"gradient_clipping\": 0.5,   # Reduced from 1.0 for stability\r\n         \"optimizer\": \"adamw\",\r\n         \"optimizer_config\": {\r\n-            \"weight_decay\": 0.01,\r\n-            \"betas\": (0.9, 0.999),\r\n+            \"weight_decay\": 0.05,   # Increased from 0.01 for better regularization\r\n+            \"betas\": (0.9, 0.98),   # Modified for transformer training\r\n             \"eps\": 1e-8\r\n         },\r\n         \r\n         # Learning rate scheduling\r\n-        \"scheduler\": \"cosine\",\r\n+        \"scheduler\": \"one_cycle\",   # Changed from cosine to one_cycle\r\n         \"scheduler_config\": {\r\n-            \"T_max\": 150,     # Match max_epochs\r\n-            \"eta_min\": 1e-6\r\n+            \"pct_start\": 0.3,       # Warm-up for 30% of training\r\n+            \"div_factor\": 25.0,     # Initial lr = max_lr/25\r\n+            \"final_div_factor\": 10000.0  # Final lr = initial_lr/10000\r\n         }\r\n     }\r\n \r\n     dataset_params = {\r\n@@ -85,9 +84,9 @@\n         \"one_hot_time_variables\": False,\r\n         \r\n         # Data preprocessing\r\n         \"normalize_data\": True,\r\n-        \"scaling_method\": \"standard\",\r\n+        \"scaling_method\": \"robust\",  # Changed from standard to robust scaling\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \"time_resolution_minutes\": 15,\r\n         \r\n         # Feature flags\r\n"
                },
                {
                    "date": 1733178396049,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,87 +8,97 @@\n from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n from models.registry.model_types import ModelType, initialize_model_registry\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get default configuration parameters optimized for 15-min interval time series data.\"\"\"\r\n+    \"\"\"Get configuration parameters optimized for 15-min to hourly conversion.\"\"\"\r\n     model_params = {\r\n         # Core model parameters\r\n-        \"input_features\": 10,  # Main value + time features\r\n-        \"output_features\": 10,  # Prediction target\r\n-        \"d_model\": 128,       # Reduced from 256 to prevent overfitting\r\n-        \"n_heads\": 4,         # Reduced from 8 to match d_model reduction\r\n-        \"n_encoder_layers\": 3, # Reduced from 4\r\n-        \"n_decoder_layers\": 3, # Reduced from 4\r\n-        \"d_ff\": 512,         # Reduced from 1024\r\n-        \"dropout\": 0.3,      # Increased from 0.2 for better regularization\r\n+        \"input_features\": 10,\r\n+        \"output_features\": 10,\r\n+        \"d_model\": 256,\r\n+        \"n_heads\": 8,\r\n+        \"n_encoder_layers\": 4,\r\n+        \"n_decoder_layers\": 4,\r\n+        \"d_ff\": 1024,\r\n+        \"dropout\": 0.2,\r\n         \r\n-        # Sequence parameters\r\n-        \"max_seq_length\": 96,  # 24 hours * 4 (15-min intervals)\r\n-        \"transformer_labels_count\": 48,  # 12 hours ahead prediction\r\n+        # Sequence parameters adjusted for 15min->hourly conversion\r\n+        \"max_seq_length\": 96,  # 24 hours of 15-min intervals\r\n+        \"transformer_labels_count\": 24,  # 24 hourly predictions\r\n         \r\n+        # Resolution-specific parameters\r\n+        \"input_resolution_minutes\": 15,  # 15-minute input data\r\n+        \"forecast_resolution_minutes\": 60,  # Hourly predictions\r\n+        \"lookback_periods\": 24,  # 24 hours of history\r\n+        \"forecast_periods\": 24,  # 24 hours of predictions\r\n+        \r\n         # Time series specific parameters\r\n-        \"kernel_size\": 5,     # Increased from 3 for better temporal context\r\n-        \"value_features\": 1,  # Main time series value\r\n-        \"time_features\": 9,   # Additional time-based features\r\n-        \"batch_first\": True,  # Handle batch dimension first\r\n+        \"kernel_size\": 4,     # Adjusted for 15-min resolution\r\n+        \"value_features\": 1,\r\n+        \"time_features\": 9,\r\n+        \"batch_first\": True,\r\n         \r\n-        # Additional regularization\r\n-        \"conv_dropout\": 0.2   # Specific dropout for convolutional layers\r\n+        # Regularization\r\n+        \"conv_dropout\": 0.2\r\n     }\r\n \r\n     training_params = {\r\n         # Basic training parameters\r\n-        \"learning_rate\": 0.002,    # Increased initial learning rate\r\n-        \"max_epochs\": 100,         # Reduced from 150\r\n-        \"batch_size\": 64,          # Kept same\r\n+        \"learning_rate\": 0.001,\r\n+        \"max_epochs\": 100,\r\n+        \"batch_size\": 64,\r\n         \"device\": \"cuda\",\r\n         \r\n+        # Resolution-specific parameters\r\n+        \"input_resolution_minutes\": 15,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \r\n         # Early stopping\r\n         \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 10,  # Reduced from 15 for faster response\r\n+        \"early_stopping_patience\": 12,\r\n         \r\n         # Time series specific\r\n-        \"transformer_labels_count\": 48,\r\n-        \"forecasting_horizon\": 48,\r\n+        \"transformer_labels_count\": 24,\r\n+        \"forecasting_horizon\": 24,\r\n         \"transformer_use_teacher_forcing\": True,\r\n         \r\n         # Optimization parameters\r\n-        \"gradient_clipping\": 0.5,   # Reduced from 1.0 for stability\r\n+        \"gradient_clip_val\": 0.5,\r\n         \"optimizer\": \"adamw\",\r\n         \"optimizer_config\": {\r\n-            \"weight_decay\": 0.05,   # Increased from 0.01 for better regularization\r\n-            \"betas\": (0.9, 0.98),   # Modified for transformer training\r\n-            \"eps\": 1e-8\r\n+            \"weight_decay\": 0.01,\r\n+            \"betas\": (0.9, 0.98)\r\n         },\r\n         \r\n         # Learning rate scheduling\r\n-        \"scheduler\": \"one_cycle\",   # Changed from cosine to one_cycle\r\n+        \"scheduler\": \"one_cycle\",\r\n         \"scheduler_config\": {\r\n-            \"pct_start\": 0.3,       # Warm-up for 30% of training\r\n-            \"div_factor\": 25.0,     # Initial lr = max_lr/25\r\n-            \"final_div_factor\": 10000.0  # Final lr = initial_lr/10000\r\n+            \"pct_start\": 0.3,\r\n+            \"div_factor\": 25.0,\r\n+            \"final_div_factor\": 1000.0\r\n         }\r\n     }\r\n \r\n     dataset_params = {\r\n         # Time series data parameters\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",\r\n-        \"time_series_window_in_hours\": 24,\r\n-        \"forecasting_horizon_in_hours\": 12,\r\n+        \"input_resolution_minutes\": 15,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"lookback_periods\": 24,\r\n+        \"forecast_periods\": 24,\r\n         \r\n         # Feature generation\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 48,\r\n+        \"labels_count\": 24,  # 24 hourly predictions\r\n         \"one_hot_time_variables\": False,\r\n         \r\n         # Data preprocessing\r\n         \"normalize_data\": True,\r\n-        \"scaling_method\": \"robust\",  # Changed from standard to robust scaling\r\n+        \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n-        \"time_resolution_minutes\": 15,\r\n         \r\n         # Feature flags\r\n         \"add_time_features\": True,\r\n         \"add_holiday_features\": True,\r\n"
                },
                {
                    "date": 1733179517688,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,9 @@\n \r\n from pipeline.utils.config_utils import create_pipeline_config\r\n from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n from models.registry.model_types import ModelType, initialize_model_registry\r\n+from models.register_models import register_models\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get configuration parameters optimized for 15-min to hourly conversion.\"\"\"\r\n     model_params = {\r\n@@ -109,10 +110,10 @@\n \r\n def train_model(data_path: str) -> None:\r\n     \"\"\"Train the convolutional transformer model using the pipeline.\"\"\"\r\n     try:\r\n-        # Initialize model registry\r\n-        initialize_model_registry()\r\n+        # Register models\r\n+        register_models()\r\n         \r\n         # Get configuration parameters\r\n         model_params, training_params, dataset_params = get_default_config_params()\r\n         \r\n"
                },
                {
                    "date": 1733180196542,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,9 +9,9 @@\n from models.registry.model_types import ModelType, initialize_model_registry\r\n from models.register_models import register_models\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get configuration parameters optimized for 15-min to hourly conversion.\"\"\"\r\n+    \"\"\"Get configuration parameters optimized for hourly forecasting.\"\"\"\r\n     model_params = {\r\n         # Core model parameters\r\n         \"input_features\": 10,\r\n         \"output_features\": 10,\r\n@@ -21,14 +21,14 @@\n         \"n_decoder_layers\": 4,\r\n         \"d_ff\": 1024,\r\n         \"dropout\": 0.2,\r\n         \r\n-        # Sequence parameters adjusted for 15min->hourly conversion\r\n-        \"max_seq_length\": 96,  # 24 hours of 15-min intervals\r\n+        # Sequence parameters for hourly data\r\n+        \"max_seq_length\": 24,  # 24 hours of history\r\n         \"transformer_labels_count\": 24,  # 24 hourly predictions\r\n         \r\n-        # Resolution-specific parameters\r\n-        \"input_resolution_minutes\": 15,  # 15-minute input data\r\n+        # Resolution-specific parameters - changed to hourly\r\n+        \"input_resolution_minutes\": 60,  # Hourly input data\r\n         \"forecast_resolution_minutes\": 60,  # Hourly predictions\r\n         \"lookback_periods\": 24,  # 24 hours of history\r\n         \"forecast_periods\": 24,  # 24 hours of predictions\r\n         \r\n@@ -42,26 +42,25 @@\n         \"conv_dropout\": 0.2\r\n     }\r\n \r\n     training_params = {\r\n-        # Basic training parameters\r\n+        # Core training parameters\r\n         \"learning_rate\": 0.001,\r\n         \"max_epochs\": 100,\r\n-        \"batch_size\": 64,\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 12,\r\n+        \"batch_size\": 32,  # Adjusted for hourly data\r\n         \"device\": \"cuda\",\r\n         \r\n-        # Resolution-specific parameters\r\n-        \"input_resolution_minutes\": 15,\r\n+        # Resolution parameters - changed to hourly\r\n+        \"input_resolution_minutes\": 60,\r\n         \"forecast_resolution_minutes\": 60,\r\n         \r\n-        # Early stopping\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 12,\r\n-        \r\n-        # Time series specific\r\n+        # Transformer specific parameters\r\n         \"transformer_labels_count\": 24,\r\n         \"forecasting_horizon\": 24,\r\n         \"transformer_use_teacher_forcing\": True,\r\n+        \"attention_dropout\": 0.1,\r\n         \r\n         # Optimization parameters\r\n         \"gradient_clip_val\": 0.5,\r\n         \"optimizer\": \"adamw\",\r\n@@ -82,9 +81,9 @@\n     dataset_params = {\r\n         # Time series data parameters\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",\r\n-        \"input_resolution_minutes\": 15,\r\n+        \"input_resolution_minutes\": 60,  # Changed to hourly\r\n         \"forecast_resolution_minutes\": 60,\r\n         \"lookback_periods\": 24,\r\n         \"forecast_periods\": 24,\r\n         \r\n@@ -108,20 +107,20 @@\n \r\n     return model_params, training_params, dataset_params\r\n \r\n def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the convolutional transformer model using the pipeline.\"\"\"\r\n+    \"\"\"Train the hourly transformer model using the pipeline.\"\"\"\r\n     try:\r\n         # Register models\r\n         register_models()\r\n         \r\n         # Get configuration parameters\r\n         model_params, training_params, dataset_params = get_default_config_params()\r\n         \r\n-        # Create pipeline configuration\r\n+        # Create pipeline configuration - changed to hourly transformer\r\n         config = create_pipeline_config(\r\n             data_path=data_path,\r\n-            model_type=ModelType.CONV_TRANSFORMER,  # Changed to convolutional transformer\r\n+            model_type=ModelType.HOURLY_TRANSFORMER,  # Changed to hourly transformer\r\n             model_params=model_params,\r\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n"
                },
                {
                    "date": 1733180275391,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,26 +21,26 @@\n         \"n_decoder_layers\": 4,\r\n         \"d_ff\": 1024,\r\n         \"dropout\": 0.2,\r\n         \r\n-        # Sequence parameters for hourly data\r\n-        \"max_seq_length\": 24,  # 24 hours of history\r\n-        \"transformer_labels_count\": 24,  # 24 hourly predictions\r\n-        \r\n-        # Resolution-specific parameters - changed to hourly\r\n+        # Resolution-specific parameters\r\n         \"input_resolution_minutes\": 60,  # Hourly input data\r\n         \"forecast_resolution_minutes\": 60,  # Hourly predictions\r\n         \"lookback_periods\": 24,  # 24 hours of history\r\n         \"forecast_periods\": 24,  # 24 hours of predictions\r\n         \r\n         # Time series specific parameters\r\n-        \"kernel_size\": 4,     # Adjusted for 15-min resolution\r\n+        \"kernel_size\": 3,\r\n         \"value_features\": 1,\r\n         \"time_features\": 9,\r\n         \"batch_first\": True,\r\n         \r\n-        # Regularization\r\n-        \"conv_dropout\": 0.2\r\n+        # Training specific\r\n+        \"batch_size\": 32,\r\n+        \"learning_rate\": 0.001,\r\n+        \"optimizer\": \"adamw\",\r\n+        \"criterion\": \"mse\",\r\n+        \"device\": \"cuda\"\r\n     }\r\n \r\n     training_params = {\r\n         # Core training parameters\r\n"
                },
                {
                    "date": 1733180329917,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,8 +11,11 @@\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get configuration parameters optimized for hourly forecasting.\"\"\"\r\n     model_params = {\r\n+        # Model type\r\n+        \"model_type\": ModelType.HOURLY_TRANSFORMER,  # Add this line\r\n+        \r\n         # Core model parameters\r\n         \"input_features\": 10,\r\n         \"output_features\": 10,\r\n         \"d_model\": 256,\r\n@@ -115,12 +118,12 @@\n         \r\n         # Get configuration parameters\r\n         model_params, training_params, dataset_params = get_default_config_params()\r\n         \r\n-        # Create pipeline configuration - changed to hourly transformer\r\n+        # Create pipeline configuration\r\n         config = create_pipeline_config(\r\n             data_path=data_path,\r\n-            model_type=ModelType.HOURLY_TRANSFORMER,  # Changed to hourly transformer\r\n+            model_type=model_params[\"model_type\"],  # Use the model_type from params\r\n             model_params=model_params,\r\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n"
                },
                {
                    "date": 1733180380951,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,11 +10,13 @@\n from models.register_models import register_models\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get configuration parameters optimized for hourly forecasting.\"\"\"\r\n+    model_type = ModelType.HOURLY_TRANSFORMER  # Define model type once\r\n+    \r\n     model_params = {\r\n-        # Model type\r\n-        \"model_type\": ModelType.HOURLY_TRANSFORMER,  # Add this line\r\n+        # Model type - pass the enum directly\r\n+        \"model_type\": model_type,\r\n         \r\n         # Core model parameters\r\n         \"input_features\": 10,\r\n         \"output_features\": 10,\r\n@@ -118,12 +120,12 @@\n         \r\n         # Get configuration parameters\r\n         model_params, training_params, dataset_params = get_default_config_params()\r\n         \r\n-        # Create pipeline configuration\r\n+        # Create pipeline configuration - pass ModelType enum directly\r\n         config = create_pipeline_config(\r\n             data_path=data_path,\r\n-            model_type=model_params[\"model_type\"],  # Use the model_type from params\r\n+            model_type=model_params[\"model_type\"],\r\n             model_params=model_params,\r\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n"
                },
                {
                    "date": 1733180447373,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,14 +10,9 @@\n from models.register_models import register_models\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get configuration parameters optimized for hourly forecasting.\"\"\"\r\n-    model_type = ModelType.HOURLY_TRANSFORMER  # Define model type once\r\n-    \r\n     model_params = {\r\n-        # Model type - pass the enum directly\r\n-        \"model_type\": model_type,\r\n-        \r\n         # Core model parameters\r\n         \"input_features\": 10,\r\n         \"output_features\": 10,\r\n         \"d_model\": 256,\r\n@@ -120,12 +115,12 @@\n         \r\n         # Get configuration parameters\r\n         model_params, training_params, dataset_params = get_default_config_params()\r\n         \r\n-        # Create pipeline configuration - pass ModelType enum directly\r\n+        # Create pipeline configuration - changed to hourly transformer\r\n         config = create_pipeline_config(\r\n             data_path=data_path,\r\n-            model_type=model_params[\"model_type\"],\r\n+            model_type=ModelType.HOURLY_TRANSFORMER,  # Changed to hourly transformer\r\n             model_params=model_params,\r\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n"
                },
                {
                    "date": 1733180517855,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,12 @@\n from models.register_models import register_models\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get configuration parameters optimized for hourly forecasting.\"\"\"\r\n+    model_type = ModelType.HOURLY_TRANSFORMER  # Define model type once\r\n+    \r\n     model_params = {\r\n+        \"model_type\": model_type,  # Pass the ModelType enum directly\r\n         # Core model parameters\r\n         \"input_features\": 10,\r\n         \"output_features\": 10,\r\n         \"d_model\": 256,\r\n@@ -118,9 +121,9 @@\n         \r\n         # Create pipeline configuration - changed to hourly transformer\r\n         config = create_pipeline_config(\r\n             data_path=data_path,\r\n-            model_type=ModelType.HOURLY_TRANSFORMER,  # Changed to hourly transformer\r\n+            model_type=ModelType.HOURLY_TRANSFORMER,  # Pass enum directly\r\n             model_params=model_params,\r\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n"
                },
                {
                    "date": 1733182868487,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,121 +9,132 @@\n from models.registry.model_types import ModelType, initialize_model_registry\r\n from models.register_models import register_models\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get configuration parameters optimized for hourly forecasting.\"\"\"\r\n-    model_type = ModelType.HOURLY_TRANSFORMER  # Define model type once\r\n-    \r\n+    \"\"\"Get configuration parameters optimized for transformer-based forecasting.\"\"\"\r\n+    # Define key dimensions and data characteristics first for consistency\r\n+    sequence_length = 24  # Hours in a day for hourly data\r\n+    n_features = 7       # Energy value (1) + time features (6)\r\n+    d_model = 512       # Transformer embedding dimension\r\n+\r\n     model_params = {\r\n-        \"model_type\": model_type,  # Pass the ModelType enum directly\r\n-        # Core model parameters\r\n-        \"input_features\": 10,\r\n-        \"output_features\": 10,\r\n-        \"d_model\": 256,\r\n-        \"n_heads\": 8,\r\n-        \"n_encoder_layers\": 4,\r\n-        \"n_decoder_layers\": 4,\r\n-        \"d_ff\": 1024,\r\n-        \"dropout\": 0.2,\r\n+        \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n         \r\n-        # Resolution-specific parameters\r\n-        \"input_resolution_minutes\": 60,  # Hourly input data\r\n-        \"forecast_resolution_minutes\": 60,  # Hourly predictions\r\n-        \"lookback_periods\": 24,  # 24 hours of history\r\n-        \"forecast_periods\": 24,  # 24 hours of predictions\r\n+        # Core architecture parameters\r\n+        \"input_features\": n_features,      # Total number of input features\r\n+        \"output_features\": 1,              # We only predict energy consumption\r\n+        \"d_model\": d_model,               # Embedding dimension\r\n+        \"n_heads\": 8,                     # Number of attention heads\r\n+        \"n_encoder_layers\": 6,            # Number of encoder blocks\r\n+        \"n_decoder_layers\": 6,            # Number of decoder blocks\r\n+        \"d_ff\": d_model * 4,             # Feed-forward dimension (typical ratio)\r\n+        \"dropout\": 0.1,                   # Dropout rate\r\n         \r\n-        # Time series specific parameters\r\n-        \"kernel_size\": 3,\r\n-        \"value_features\": 1,\r\n-        \"time_features\": 9,\r\n-        \"batch_first\": True,\r\n+        # Resolution and sequence parameters\r\n+        \"input_resolution_minutes\": 60,    # Hourly data\r\n+        \"forecast_resolution_minutes\": 60, # Hourly predictions\r\n+        \"lookback_periods\": sequence_length,\r\n+        \"forecast_periods\": sequence_length,\r\n         \r\n-        # Training specific\r\n+        # Component-specific parameters\r\n+        \"kernel_size\": 3,                 # For convolutional attention\r\n+        \"value_features\": 1,              # Energy consumption value\r\n+        \"time_features\": n_features - 1,  # Time-based features\r\n+        \"batch_first\": True,              # Use batch-first tensor format\r\n+        \r\n+        # Training configuration\r\n         \"batch_size\": 32,\r\n-        \"learning_rate\": 0.001,\r\n-        \"optimizer\": \"adamw\",\r\n+        \"learning_rate\": 0.0001,          # Reduced for stability\r\n+        \"optimizer\": \"adamw\",             # AdamW optimizer\r\n         \"criterion\": \"mse\",\r\n-        \"device\": \"cuda\"\r\n+        \"device\": \"cuda\"                  # Use GPU if available\r\n     }\r\n \r\n     training_params = {\r\n-        # Core training parameters\r\n-        \"learning_rate\": 0.001,\r\n+        # Core training settings\r\n+        \"learning_rate\": 0.0001,          # Matching model config\r\n         \"max_epochs\": 100,\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 12,\r\n-        \"batch_size\": 32,  # Adjusted for hourly data\r\n+        \"batch_size\": 32,\r\n         \"device\": \"cuda\",\r\n         \r\n-        # Resolution parameters - changed to hourly\r\n+        # Early stopping configuration\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,     # Increased patience\r\n+        \r\n+        # Sequence parameters\r\n         \"input_resolution_minutes\": 60,\r\n         \"forecast_resolution_minutes\": 60,\r\n+        \"transformer_labels_count\": sequence_length,\r\n+        \"forecasting_horizon\": sequence_length,\r\n         \r\n-        # Transformer specific parameters\r\n-        \"transformer_labels_count\": 24,\r\n-        \"forecasting_horizon\": 24,\r\n+        # Transformer training specifics\r\n         \"transformer_use_teacher_forcing\": True,\r\n+        \"teacher_forcing_ratio\": 0.5,     # Added teacher forcing ratio\r\n         \"attention_dropout\": 0.1,\r\n         \r\n-        # Optimization parameters\r\n-        \"gradient_clip_val\": 0.5,\r\n+        # Optimization settings\r\n+        \"gradient_clip_val\": 1.0,         # Gradient clipping threshold\r\n         \"optimizer\": \"adamw\",\r\n         \"optimizer_config\": {\r\n             \"weight_decay\": 0.01,\r\n-            \"betas\": (0.9, 0.98)\r\n+            \"betas\": (0.9, 0.98),         # Transformer-specific beta values\r\n+            \"eps\": 1e-8\r\n         },\r\n         \r\n         # Learning rate scheduling\r\n         \"scheduler\": \"one_cycle\",\r\n         \"scheduler_config\": {\r\n             \"pct_start\": 0.3,\r\n             \"div_factor\": 25.0,\r\n-            \"final_div_factor\": 1000.0\r\n+            \"final_div_factor\": 1000.0,\r\n+            \"anneal_strategy\": \"cos\"       # Cosine annealing\r\n         }\r\n     }\r\n \r\n     dataset_params = {\r\n-        # Time series data parameters\r\n+        # Data identification\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",\r\n-        \"input_resolution_minutes\": 60,  # Changed to hourly\r\n+        \r\n+        # Resolution and sequence settings\r\n+        \"input_resolution_minutes\": 60,\r\n         \"forecast_resolution_minutes\": 60,\r\n-        \"lookback_periods\": 24,\r\n-        \"forecast_periods\": 24,\r\n+        \"lookback_periods\": sequence_length,\r\n+        \"forecast_periods\": sequence_length,\r\n         \r\n-        # Feature generation\r\n+        # Feature configuration\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": 24,  # 24 hourly predictions\r\n-        \"one_hot_time_variables\": False,\r\n+        \"labels_count\": sequence_length,\r\n+        \"one_hot_time_variables\": False,   # Using cyclical encoding instead\r\n         \r\n         # Data preprocessing\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \r\n-        # Feature flags\r\n-        \"add_time_features\": True,\r\n-        \"add_holiday_features\": True,\r\n-        \"add_weather_features\": False\r\n+        # Feature generation flags\r\n+        \"add_time_features\": True,         # Include time-based features\r\n+        \"add_holiday_features\": True,      # Include holiday information\r\n+        \"add_weather_features\": False      # No weather data available\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the hourly transformer model using the pipeline.\"\"\"\r\n+    \"\"\"Train the transformer model for energy consumption forecasting.\"\"\"\r\n     try:\r\n-        # Register models\r\n+        # Initialize model registry\r\n         register_models()\r\n         \r\n         # Get configuration parameters\r\n         model_params, training_params, dataset_params = get_default_config_params()\r\n         \r\n-        # Create pipeline configuration - changed to hourly transformer\r\n+        # Create pipeline configuration\r\n         config = create_pipeline_config(\r\n             data_path=data_path,\r\n-            model_type=ModelType.HOURLY_TRANSFORMER,  # Pass enum directly\r\n+            model_type=ModelType.HOURLY_TRANSFORMER,\r\n             model_params=model_params,\r\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n@@ -132,40 +143,44 @@\n         pipeline = TimeSeriesPipeline(config)\r\n         experiment = pipeline.run()\r\n         \r\n         if experiment is None:\r\n-            raise RuntimeError(\"Training failed to produce results\")\r\n+            raise RuntimeError(\"Training failed to produce experiment results\")\r\n         \r\n-        # Print results\r\n-        print(\"\\nConvolutional Transformer Training Complete!\")\r\n+        # Print training results\r\n+        print(\"\\nTransformer Training Results:\")\r\n         print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n         print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n         \r\n         print(\"\\nEvaluation Metrics:\")\r\n-        for metric, value in experiment.evaluation.total_metrics.items():\r\n-            print(f\"{metric}: {value:.4f}\")\r\n+        if hasattr(experiment.evaluation, 'total_metrics'):\r\n+            for metric, value in experiment.evaluation.total_metrics.items():\r\n+                print(f\"{metric}: {value:.4f}\")\r\n         \r\n         # Print training history\r\n         print(\"\\nTraining History:\")\r\n-        if experiment.training_report and hasattr(experiment.training_report, 'train_losses'):\r\n-            final_train_loss = experiment.training_report.train_losses[-1]\r\n-            print(f\"Final Training Loss: {final_train_loss:.4f}\")\r\n-\r\n-        if experiment.training_report and hasattr(experiment.training_report, 'val_losses'):\r\n-            final_val_loss = experiment.training_report.val_losses[-1]\r\n-            print(f\"Final Validation Loss: {final_val_loss:.4f}\")\r\n+        if experiment.training_report:\r\n+            if hasattr(experiment.training_report, 'train_losses'):\r\n+                final_train = experiment.training_report.train_losses[-1]\r\n+                best_train = min(experiment.training_report.train_losses)\r\n+                print(f\"Training Loss - Final: {final_train:.4f}, Best: {best_train:.4f}\")\r\n+            \r\n+            if hasattr(experiment.training_report, 'val_losses'):\r\n+                final_val = experiment.training_report.val_losses[-1]\r\n+                best_val = min(experiment.training_report.val_losses)\r\n+                print(f\"Validation Loss - Final: {final_val:.4f}, Best: {best_val:.4f}\")\r\n         \r\n-        # Save the experiment\r\n+        # Save experiment results\r\n         experiment.save_to_json_file()\r\n-        print(f\"\\nExperiment saved successfully\")\r\n+        print(f\"\\nExperiment saved successfully to the experiments directory\")\r\n         \r\n     except Exception as e:\r\n         print(f\"Error during training: {str(e)}\")\r\n         raise\r\n \r\n def main():\r\n     parser = argparse.ArgumentParser(\r\n-        description='Train convolutional transformer model for 15-min interval energy forecasting'\r\n+        description='Train transformer model for energy consumption forecasting'\r\n     )\r\n     parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n     args = parser.parse_args()\r\n     \r\n"
                },
                {
                    "date": 1733183101566,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,114 +10,80 @@\n from models.register_models import register_models\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n     \"\"\"Get configuration parameters optimized for transformer-based forecasting.\"\"\"\r\n-    # Define key dimensions and data characteristics first for consistency\r\n-    sequence_length = 24  # Hours in a day for hourly data\r\n-    n_features = 7       # Energy value (1) + time features (6)\r\n-    d_model = 512       # Transformer embedding dimension\r\n+    # Key dimensions and hyperparameters\r\n+    input_features = 7      # Energy value (1) + time features (6)\r\n+    d_model = 512          # Model dimension\r\n+    n_heads = 8            # Number of attention heads\r\n+    seq_length = 24        # Sequence length (24 hours)\r\n \r\n     model_params = {\r\n         \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n         \r\n-        # Core architecture parameters\r\n-        \"input_features\": n_features,      # Total number of input features\r\n-        \"output_features\": 1,              # We only predict energy consumption\r\n+        # Dimensions and features\r\n+        \"input_features\": input_features,   # Total number of input features\r\n+        \"output_features\": 1,              # Only predict energy consumption\r\n         \"d_model\": d_model,               # Embedding dimension\r\n-        \"n_heads\": 8,                     # Number of attention heads\r\n-        \"n_encoder_layers\": 6,            # Number of encoder blocks\r\n-        \"n_decoder_layers\": 6,            # Number of decoder blocks\r\n-        \"d_ff\": d_model * 4,             # Feed-forward dimension (typical ratio)\r\n-        \"dropout\": 0.1,                   # Dropout rate\r\n+        \"n_heads\": n_heads,               # Must divide d_model evenly\r\n+        \"n_encoder_layers\": 6,\r\n+        \"n_decoder_layers\": 6,\r\n+        \"d_ff\": d_model * 4,             # Feed-forward dimension\r\n+        \"dropout\": 0.1,\r\n         \r\n-        # Resolution and sequence parameters\r\n-        \"input_resolution_minutes\": 60,    # Hourly data\r\n-        \"forecast_resolution_minutes\": 60, # Hourly predictions\r\n-        \"lookback_periods\": sequence_length,\r\n-        \"forecast_periods\": sequence_length,\r\n+        # Sequence parameters\r\n+        \"input_resolution_minutes\": 60,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"lookback_periods\": seq_length,\r\n+        \"forecast_periods\": seq_length,\r\n         \r\n-        # Component-specific parameters\r\n-        \"kernel_size\": 3,                 # For convolutional attention\r\n+        # Feature configuration\r\n         \"value_features\": 1,              # Energy consumption value\r\n-        \"time_features\": n_features - 1,  # Time-based features\r\n-        \"batch_first\": True,              # Use batch-first tensor format\r\n-        \r\n-        # Training configuration\r\n-        \"batch_size\": 32,\r\n-        \"learning_rate\": 0.0001,          # Reduced for stability\r\n-        \"optimizer\": \"adamw\",             # AdamW optimizer\r\n-        \"criterion\": \"mse\",\r\n-        \"device\": \"cuda\"                  # Use GPU if available\r\n+        \"time_features\": input_features - 1,  # Time-based features\r\n+        \"batch_first\": True\r\n     }\r\n \r\n     training_params = {\r\n         # Core training settings\r\n-        \"learning_rate\": 0.0001,          # Matching model config\r\n+        \"batch_size\": 32,\r\n+        \"learning_rate\": 0.0001,           # Small learning rate for stability\r\n         \"max_epochs\": 100,\r\n-        \"batch_size\": 32,\r\n-        \"device\": \"cuda\",\r\n+        \"early_stopping_patience\": 15,\r\n         \r\n-        # Early stopping configuration\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,     # Increased patience\r\n-        \r\n-        # Sequence parameters\r\n-        \"input_resolution_minutes\": 60,\r\n-        \"forecast_resolution_minutes\": 60,\r\n-        \"transformer_labels_count\": sequence_length,\r\n-        \"forecasting_horizon\": sequence_length,\r\n-        \r\n-        # Transformer training specifics\r\n+        # Sequence configuration\r\n+        \"transformer_labels_count\": seq_length,\r\n         \"transformer_use_teacher_forcing\": True,\r\n-        \"teacher_forcing_ratio\": 0.5,     # Added teacher forcing ratio\r\n-        \"attention_dropout\": 0.1,\r\n         \r\n-        # Optimization settings\r\n-        \"gradient_clip_val\": 1.0,         # Gradient clipping threshold\r\n+        # Optimization\r\n         \"optimizer\": \"adamw\",\r\n         \"optimizer_config\": {\r\n             \"weight_decay\": 0.01,\r\n-            \"betas\": (0.9, 0.98),         # Transformer-specific beta values\r\n-            \"eps\": 1e-8\r\n-        },\r\n-        \r\n-        # Learning rate scheduling\r\n-        \"scheduler\": \"one_cycle\",\r\n-        \"scheduler_config\": {\r\n-            \"pct_start\": 0.3,\r\n-            \"div_factor\": 25.0,\r\n-            \"final_div_factor\": 1000.0,\r\n-            \"anneal_strategy\": \"cos\"       # Cosine annealing\r\n+            \"betas\": (0.9, 0.98)           # Standard transformer betas\r\n         }\r\n     }\r\n \r\n     dataset_params = {\r\n-        # Data identification\r\n+        # Data configuration\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",\r\n         \r\n-        # Resolution and sequence settings\r\n+        # Feature generation\r\n         \"input_resolution_minutes\": 60,\r\n         \"forecast_resolution_minutes\": 60,\r\n-        \"lookback_periods\": sequence_length,\r\n-        \"forecast_periods\": sequence_length,\r\n+        \"lookback_periods\": seq_length,\r\n+        \"forecast_periods\": seq_length,\r\n         \r\n-        # Feature configuration\r\n+        # Feature settings\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": sequence_length,\r\n-        \"one_hot_time_variables\": False,   # Using cyclical encoding instead\r\n+        \"labels_count\": seq_length,\r\n+        \"one_hot_time_variables\": False,   # Using cyclical encoding\r\n         \r\n-        # Data preprocessing\r\n+        # Data processing\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n-        \"time_series_scaler\": StandardScaler(),\r\n-        \r\n-        # Feature generation flags\r\n-        \"add_time_features\": True,         # Include time-based features\r\n-        \"add_holiday_features\": True,      # Include holiday information\r\n-        \"add_weather_features\": False      # No weather data available\r\n+        \"time_series_scaler\": StandardScaler()\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733183256088,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,99 +2,189 @@\n from pathlib import Path\r\n from typing import Dict, Any, Tuple\r\n \r\n from sklearn.preprocessing import StandardScaler\r\n+import torch\r\n \r\n from pipeline.utils.config_utils import create_pipeline_config\r\n from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n from models.registry.model_types import ModelType, initialize_model_registry\r\n from models.register_models import register_models\r\n \r\n def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"Get configuration parameters optimized for transformer-based forecasting.\"\"\"\r\n-    # Key dimensions and hyperparameters\r\n-    input_features = 7      # Energy value (1) + time features (6)\r\n-    d_model = 512          # Model dimension\r\n-    n_heads = 8            # Number of attention heads\r\n-    seq_length = 24        # Sequence length (24 hours)\r\n+    \"\"\"\r\n+    Get optimized configuration parameters for transformer-based energy forecasting.\r\n+    Returns a tuple of (model_params, training_params, dataset_params).\r\n+    \"\"\"\r\n+    # Core architecture dimensions\r\n+    sequence_length = 24   # Hours in a day for hourly predictions\r\n+    n_features = 7        # Energy value (1) + cyclical time encodings (6)\r\n+    d_model = 512        # Transformer embedding dimension\r\n+    n_heads = 8          # Number of attention heads (must divide d_model evenly)\r\n+    \r\n+    # Compute derived parameters\r\n+    d_ff = d_model * 4   # Feed-forward dimension (standard transformer ratio)\r\n+    n_layers = 6         # Number of transformer layers (standard configuration)\r\n \r\n     model_params = {\r\n         \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n         \r\n-        # Dimensions and features\r\n-        \"input_features\": input_features,   # Total number of input features\r\n-        \"output_features\": 1,              # Only predict energy consumption\r\n-        \"d_model\": d_model,               # Embedding dimension\r\n-        \"n_heads\": n_heads,               # Must divide d_model evenly\r\n-        \"n_encoder_layers\": 6,\r\n-        \"n_decoder_layers\": 6,\r\n-        \"d_ff\": d_model * 4,             # Feed-forward dimension\r\n+        # Architecture parameters\r\n+        \"input_features\": n_features,\r\n+        \"output_features\": 1,              # Single target value prediction\r\n+        \"d_model\": d_model,\r\n+        \"n_heads\": n_heads,\r\n+        \"n_encoder_layers\": n_layers,\r\n+        \"n_decoder_layers\": n_layers,\r\n+        \"d_ff\": d_ff,\r\n         \"dropout\": 0.1,\r\n         \r\n         # Sequence parameters\r\n         \"input_resolution_minutes\": 60,\r\n         \"forecast_resolution_minutes\": 60,\r\n-        \"lookback_periods\": seq_length,\r\n-        \"forecast_periods\": seq_length,\r\n+        \"lookback_periods\": sequence_length,\r\n+        \"forecast_periods\": sequence_length,\r\n         \r\n         # Feature configuration\r\n-        \"value_features\": 1,              # Energy consumption value\r\n-        \"time_features\": input_features - 1,  # Time-based features\r\n-        \"batch_first\": True\r\n+        \"kernel_size\": 3,\r\n+        \"value_features\": 1,\r\n+        \"time_features\": n_features - 1,\r\n+        \"batch_first\": True,\r\n+        \r\n+        # Embedding parameters\r\n+        \"max_sequence_length\": 5000,       # Maximum supported sequence length\r\n+        \"embedding_dropout\": 0.1,          # Dropout for embeddings\r\n+        \"positional_encoding\": True,       # Use positional encoding\r\n+        \r\n+        # Attention parameters\r\n+        \"attention_dropout\": 0.1,          # Attention-specific dropout\r\n+        \"attention_type\": \"scaled_dot\",    # Type of attention mechanism\r\n+        \r\n+        # Device and precision settings\r\n+        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\r\n+        \"dtype\": torch.float32,            # Precision for computations\r\n+        \r\n+        # Training hyperparameters\r\n+        \"batch_size\": 32,\r\n+        \"learning_rate\": 0.0001,\r\n+        \"optimizer\": \"adamw\",\r\n+        \"criterion\": \"mse\",\r\n+        \"gradient_clip_val\": 1.0\r\n     }\r\n \r\n     training_params = {\r\n         # Core training settings\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n         \"batch_size\": 32,\r\n-        \"learning_rate\": 0.0001,           # Small learning rate for stability\r\n-        \"max_epochs\": 100,\r\n+        \"device\": model_params[\"device\"],\r\n+        \r\n+        # Early stopping\r\n+        \"use_early_stopping\": True,\r\n         \"early_stopping_patience\": 15,\r\n+        \"early_stopping_min_delta\": 1e-4,\r\n+        \"save_best_model\": True,\r\n         \r\n-        # Sequence configuration\r\n-        \"transformer_labels_count\": seq_length,\r\n+        # Sequence handling\r\n+        \"input_resolution_minutes\": 60,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"transformer_labels_count\": sequence_length,\r\n+        \"forecasting_horizon\": sequence_length,\r\n+        \r\n+        # Teacher forcing\r\n         \"transformer_use_teacher_forcing\": True,\r\n+        \"teacher_forcing_ratio\": 0.5,\r\n+        \"teacher_forcing_schedule\": \"linear_decay\",\r\n         \r\n         # Optimization\r\n+        \"gradient_clip_val\": 1.0,\r\n         \"optimizer\": \"adamw\",\r\n         \"optimizer_config\": {\r\n             \"weight_decay\": 0.01,\r\n-            \"betas\": (0.9, 0.98)           # Standard transformer betas\r\n-        }\r\n+            \"betas\": (0.9, 0.98),\r\n+            \"eps\": 1e-8\r\n+        },\r\n+        \r\n+        # Learning rate scheduling\r\n+        \"scheduler\": \"one_cycle\",\r\n+        \"scheduler_config\": {\r\n+            \"pct_start\": 0.3,\r\n+            \"div_factor\": 25.0,\r\n+            \"final_div_factor\": 1000.0,\r\n+            \"anneal_strategy\": \"cos\",\r\n+            \"cycle_momentum\": True\r\n+        },\r\n+        \r\n+        # Validation settings\r\n+        \"validation_frequency\": 1,         # Validate every N epochs\r\n+        \"validation_metric\": \"mae\",        # Primary metric for model selection\r\n+        \r\n+        # Logging and checkpointing\r\n+        \"log_frequency\": 10,               # Log every N batches\r\n+        \"checkpoint_frequency\": 5,         # Save checkpoint every N epochs\r\n+        \"keep_last_n_checkpoints\": 3       # Number of checkpoints to keep\r\n     }\r\n \r\n     dataset_params = {\r\n         # Data configuration\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",\r\n         \r\n-        # Feature generation\r\n+        # Resolution settings\r\n         \"input_resolution_minutes\": 60,\r\n         \"forecast_resolution_minutes\": 60,\r\n-        \"lookback_periods\": seq_length,\r\n-        \"forecast_periods\": seq_length,\r\n+        \"lookback_periods\": sequence_length,\r\n+        \"forecast_periods\": sequence_length,\r\n         \r\n-        # Feature settings\r\n+        # Feature configuration\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n-        \"labels_count\": seq_length,\r\n-        \"one_hot_time_variables\": False,   # Using cyclical encoding\r\n+        \"labels_count\": sequence_length,\r\n+        \"one_hot_time_variables\": False,\r\n         \r\n-        # Data processing\r\n+        # Time features\r\n+        \"add_time_features\": True,\r\n+        \"add_hour_feature\": True,\r\n+        \"add_weekday_feature\": True,\r\n+        \"add_month_feature\": True,\r\n+        \"add_holiday_features\": True,\r\n+        \"add_weather_features\": False,\r\n+        \r\n+        # Data preprocessing\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n-        \"time_series_scaler\": StandardScaler()\r\n+        \"time_series_scaler\": StandardScaler(),\r\n+        \"handle_missing_values\": \"interpolate\",\r\n+        \"remove_outliers\": True,\r\n+        \"outlier_std_threshold\": 3.0,\r\n+        \r\n+        # Data augmentation\r\n+        \"use_data_augmentation\": False,    # Enable if needed\r\n+        \"augmentation_methods\": [\"jitter\", \"scaling\"],\r\n+        \"augmentation_probability\": 0.3,\r\n+        \r\n+        # Sequence handling\r\n+        \"padding_value\": 0.0,\r\n+        \"mask_padding\": True,\r\n+        \"max_sequence_gaps\": 3             # Maximum allowed gaps in sequence\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n def train_model(data_path: str) -> None:\r\n-    \"\"\"Train the transformer model for energy consumption forecasting.\"\"\"\r\n+    \"\"\"\r\n+    Train the transformer model for energy consumption forecasting.\r\n+    \r\n+    Args:\r\n+        data_path: Path to the CSV data file containing energy consumption data\r\n+    \"\"\"\r\n     try:\r\n-        # Initialize model registry\r\n+        # Initialize systems\r\n+        initialize_model_registry()\r\n         register_models()\r\n         \r\n-        # Get configuration parameters\r\n+        # Get and validate configuration\r\n         model_params, training_params, dataset_params = get_default_config_params()\r\n         \r\n         # Create pipeline configuration\r\n         config = create_pipeline_config(\r\n@@ -105,52 +195,67 @@\n             dataset_params=dataset_params\r\n         )\r\n         \r\n         # Initialize and run pipeline\r\n+        print(\"\\nInitializing training pipeline...\")\r\n         pipeline = TimeSeriesPipeline(config)\r\n+        \r\n+        print(\"\\nStarting model training...\")\r\n         experiment = pipeline.run()\r\n         \r\n         if experiment is None:\r\n             raise RuntimeError(\"Training failed to produce experiment results\")\r\n         \r\n-        # Print training results\r\n+        # Print comprehensive training results\r\n         print(\"\\nTransformer Training Results:\")\r\n-        print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n-        print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n+        print(f\"Training Duration: {experiment.training_time:.2f} seconds\")\r\n+        print(f\"Inference Time: {experiment.test_time:.2f} seconds\")\r\n         \r\n-        print(\"\\nEvaluation Metrics:\")\r\n+        print(\"\\nModel Performance Metrics:\")\r\n         if hasattr(experiment.evaluation, 'total_metrics'):\r\n             for metric, value in experiment.evaluation.total_metrics.items():\r\n-                print(f\"{metric}: {value:.4f}\")\r\n+                print(f\"{metric.upper()}: {value:.4f}\")\r\n         \r\n-        # Print training history\r\n         print(\"\\nTraining History:\")\r\n         if experiment.training_report:\r\n             if hasattr(experiment.training_report, 'train_losses'):\r\n                 final_train = experiment.training_report.train_losses[-1]\r\n                 best_train = min(experiment.training_report.train_losses)\r\n                 print(f\"Training Loss - Final: {final_train:.4f}, Best: {best_train:.4f}\")\r\n+                print(f\"Loss Improvement: {((final_train - best_train) / final_train) * 100:.2f}%\")\r\n             \r\n             if hasattr(experiment.training_report, 'val_losses'):\r\n                 final_val = experiment.training_report.val_losses[-1]\r\n                 best_val = min(experiment.training_report.val_losses)\r\n                 print(f\"Validation Loss - Final: {final_val:.4f}, Best: {best_val:.4f}\")\r\n+                print(f\"Validation Improvement: {((final_val - best_val) / final_val) * 100:.2f}%\")\r\n         \r\n-        # Save experiment results\r\n+        # Save experiment\r\n+        print(\"\\nSaving experiment results...\")\r\n         experiment.save_to_json_file()\r\n-        print(f\"\\nExperiment saved successfully to the experiments directory\")\r\n+        print(f\"Experiment saved successfully to the experiments directory\")\r\n         \r\n     except Exception as e:\r\n-        print(f\"Error during training: {str(e)}\")\r\n+        print(f\"\\nError during training: {str(e)}\")\r\n         raise\r\n \r\n def main():\r\n+    # Configure argument parser\r\n     parser = argparse.ArgumentParser(\r\n-        description='Train transformer model for energy consumption forecasting'\r\n+        description='Train transformer model for energy consumption forecasting',\r\n+        formatter_class=argparse.ArgumentDefaultsHelpFormatter\r\n     )\r\n-    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n+    \r\n+    # Add arguments\r\n+    parser.add_argument(\r\n+        '--data',\r\n+        type=str,\r\n+        required=True,\r\n+        help='Path to the CSV data file containing energy consumption data'\r\n+    )\r\n+    \r\n+    # Parse arguments and train\r\n     args = parser.parse_args()\r\n-    \r\n     train_model(args.data)\r\n \r\n if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733183713642,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,50 +124,54 @@\n         \"keep_last_n_checkpoints\": 3       # Number of checkpoints to keep\r\n     }\r\n \r\n     dataset_params = {\r\n-        # Data configuration\r\n+        # Core parameters\r\n         \"time_variable\": \"utc_timestamp\",\r\n         \"target_variable\": \"energy_consumption\",\r\n         \r\n-        # Resolution settings\r\n+        # Resolution and sequence settings\r\n         \"input_resolution_minutes\": 60,\r\n         \"forecast_resolution_minutes\": 60,\r\n         \"lookback_periods\": sequence_length,\r\n         \"forecast_periods\": sequence_length,\r\n         \r\n-        # Feature configuration\r\n+        # Core feature configuration\r\n         \"is_single_time_point_prediction\": False,\r\n         \"include_time_information\": True,\r\n         \"is_training_set\": True,\r\n         \"labels_count\": sequence_length,\r\n         \"one_hot_time_variables\": False,\r\n         \r\n-        # Time features\r\n-        \"add_time_features\": True,\r\n-        \"add_hour_feature\": True,\r\n-        \"add_weekday_feature\": True,\r\n-        \"add_month_feature\": True,\r\n-        \"add_holiday_features\": True,\r\n-        \"add_weather_features\": False,\r\n-        \r\n         # Data preprocessing\r\n         \"normalize_data\": True,\r\n         \"scaling_method\": \"standard\",\r\n         \"time_series_scaler\": StandardScaler(),\r\n         \"handle_missing_values\": \"interpolate\",\r\n         \"remove_outliers\": True,\r\n         \"outlier_std_threshold\": 3.0,\r\n         \r\n+        # Basic feature flags\r\n+        \"add_time_features\": True,\r\n+        \"add_holiday_features\": True,\r\n+        \"add_weather_features\": False,\r\n+        \r\n+        # Detailed time features\r\n+        \"add_hour_feature\": True,\r\n+        \"add_weekday_feature\": True,\r\n+        \"add_month_feature\": True,\r\n+        \"add_season_feature\": False,\r\n+        \"add_year_feature\": False,\r\n+        \r\n         # Data augmentation\r\n-        \"use_data_augmentation\": False,    # Enable if needed\r\n+        \"use_data_augmentation\": False,\r\n         \"augmentation_methods\": [\"jitter\", \"scaling\"],\r\n         \"augmentation_probability\": 0.3,\r\n         \r\n         # Sequence handling\r\n         \"padding_value\": 0.0,\r\n         \"mask_padding\": True,\r\n-        \"max_sequence_gaps\": 3             # Maximum allowed gaps in sequence\r\n+        \"max_sequence_gaps\": 3\r\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n"
                },
                {
                    "date": 1733236914037,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,259 @@\n+import argparse\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from sklearn.preprocessing import StandardScaler\r\n+import torch\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+from models.register_models import register_models\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"\r\n+    Get optimized configuration parameters for transformer-based energy forecasting.\r\n+    Returns a tuple of (model_params, training_params, dataset_params).\r\n+    \"\"\"\r\n+    # Core architecture dimensions\r\n+    sequence_length = 24   # Hours in a day for hourly predictions\r\n+    n_features = 7        # Energy value (1) + cyclical time encodings (6)\r\n+    d_model = 512        # Transformer embedding dimension\r\n+    n_heads = 8          # Number of attention heads (must divide d_model evenly)\r\n+    \r\n+    # Compute derived parameters\r\n+    d_ff = d_model * 4   # Feed-forward dimension (standard transformer ratio)\r\n+    n_layers = 6         # Number of transformer layers (standard configuration)\r\n+\r\n+    model_params = {\r\n+    \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n+    \r\n+    # Architecture parameters\r\n+    \"input_features\": n_features,\r\n+    \"output_features\": 1,\r\n+    \"d_model\": d_model,\r\n+    \"n_heads\": n_heads,\r\n+    \"n_encoder_layers\": n_layers,\r\n+    \"n_decoder_layers\": n_layers,\r\n+    \"d_ff\": d_ff,\r\n+    \"dropout\": 0.1,\r\n+    \r\n+    # Sequence parameters\r\n+    \"input_resolution_minutes\": 60,\r\n+    \"forecast_resolution_minutes\": 60,\r\n+    \"lookback_periods\": sequence_length,\r\n+    \"forecast_periods\": sequence_length,\r\n+    \"max_sequence_length\": 5000,\r\n+    \r\n+    # Feature configuration\r\n+    \"kernel_size\": 3,\r\n+    \"value_features\": 1,\r\n+    \"time_features\": n_features - 1,\r\n+    \"batch_first\": True,\r\n+    \r\n+    # Attention configuration\r\n+    \"attention_dropout\": 0.1,\r\n+    \"attention_type\": \"scaled_dot\",\r\n+    \"embedding_dropout\": 0.1,\r\n+    \"positional_encoding\": True,\r\n+    \r\n+    # Device and training settings\r\n+    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\r\n+    \"batch_size\": 32,\r\n+    \"learning_rate\": 0.0001,\r\n+    \"optimizer\": \"adamw\",\r\n+    \"criterion\": \"mse\"\r\n+}\r\n+\r\n+    training_params = {\r\n+        # Core training settings\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"batch_size\": 32,\r\n+        \"device\": model_params[\"device\"],\r\n+        \r\n+        # Early stopping\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"early_stopping_min_delta\": 1e-4,\r\n+        \"save_best_model\": True,\r\n+        \r\n+        # Sequence handling\r\n+        \"input_resolution_minutes\": 60,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"transformer_labels_count\": sequence_length,\r\n+        \"forecasting_horizon\": sequence_length,\r\n+        \r\n+        # Teacher forcing\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"teacher_forcing_ratio\": 0.5,\r\n+        \"teacher_forcing_schedule\": \"linear_decay\",\r\n+        \r\n+        # Optimization\r\n+        \"gradient_clip_val\": 1.0,\r\n+        \"optimizer\": \"adamw\",\r\n+        \"optimizer_config\": {\r\n+            \"weight_decay\": 0.01,\r\n+            \"betas\": (0.9, 0.98),\r\n+            \"eps\": 1e-8\r\n+        },\r\n+        \r\n+        # Learning rate scheduling\r\n+        \"scheduler\": \"one_cycle\",\r\n+        \"scheduler_config\": {\r\n+            \"pct_start\": 0.3,\r\n+            \"div_factor\": 25.0,\r\n+            \"final_div_factor\": 1000.0,\r\n+            \"anneal_strategy\": \"cos\",\r\n+            \"cycle_momentum\": True\r\n+        },\r\n+        \r\n+        # Validation settings\r\n+        \"validation_frequency\": 1,         # Validate every N epochs\r\n+        \"validation_metric\": \"mae\",        # Primary metric for model selection\r\n+        \r\n+        # Logging and checkpointing\r\n+        \"log_frequency\": 10,               # Log every N batches\r\n+        \"checkpoint_frequency\": 5,         # Save checkpoint every N epochs\r\n+        \"keep_last_n_checkpoints\": 3       # Number of checkpoints to keep\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        # Core parameters\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"energy_consumption\",\r\n+        \r\n+        # Resolution and sequence settings\r\n+        \"input_resolution_minutes\": 60,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"lookback_periods\": sequence_length,\r\n+        \"forecast_periods\": sequence_length,\r\n+        \r\n+        # Core feature configuration\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": sequence_length,\r\n+        \"one_hot_time_variables\": False,\r\n+        \r\n+        # Data preprocessing\r\n+        \"normalize_data\": True,\r\n+        \"scaling_method\": \"standard\",\r\n+        \"time_series_scaler\": StandardScaler(),\r\n+        \"handle_missing_values\": \"interpolate\",\r\n+        \"remove_outliers\": True,\r\n+        \"outlier_std_threshold\": 3.0,\r\n+        \r\n+        # Basic feature flags\r\n+        \"add_time_features\": True,\r\n+        \"add_holiday_features\": True,\r\n+        \"add_weather_features\": False,\r\n+        \r\n+        # Detailed time features\r\n+        \"add_hour_feature\": True,\r\n+        \"add_weekday_feature\": True,\r\n+        \"add_month_feature\": True,\r\n+        \"add_season_feature\": False,\r\n+        \"add_year_feature\": False,\r\n+        \r\n+        # Data augmentation\r\n+        \"use_data_augmentation\": False,\r\n+        \"augmentation_methods\": [\"jitter\", \"scaling\"],\r\n+        \"augmentation_probability\": 0.3,\r\n+        \r\n+        # Sequence handling\r\n+        \"padding_value\": 0.0,\r\n+        \"mask_padding\": True,\r\n+        \"max_sequence_gaps\": 3\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"\r\n+    Train the transformer model for energy consumption forecasting.\r\n+    \r\n+    Args:\r\n+        data_path: Path to the CSV data file containing energy consumption data\r\n+    \"\"\"\r\n+    try:\r\n+        # Initialize systems\r\n+        initialize_model_registry()\r\n+        register_models()\r\n+        \r\n+        # Get and validate configuration\r\n+        model_params, training_params, dataset_params = get_default_config_params()\r\n+        \r\n+        # Create pipeline configuration\r\n+        config = create_pipeline_config(\r\n+            data_path=data_path,\r\n+            model_type=ModelType.HOURLY_TRANSFORMER,\r\n+            model_params=model_params,\r\n+            training_params=training_params,\r\n+            dataset_params=dataset_params\r\n+        )\r\n+        \r\n+        # Initialize and run pipeline\r\n+        print(\"\\nInitializing training pipeline...\")\r\n+        pipeline = TimeSeriesPipeline(config)\r\n+        \r\n+        print(\"\\nStarting model training...\")\r\n+        experiment = pipeline.run()\r\n+        \r\n+        if experiment is None:\r\n+            raise RuntimeError(\"Training failed to produce experiment results\")\r\n+        \r\n+        # Print comprehensive training results\r\n+        print(\"\\nTransformer Training Results:\")\r\n+        print(f\"Training Duration: {experiment.training_time:.2f} seconds\")\r\n+        print(f\"Inference Time: {experiment.test_time:.2f} seconds\")\r\n+        \r\n+        print(\"\\nModel Performance Metrics:\")\r\n+        if hasattr(experiment.evaluation, 'total_metrics'):\r\n+            for metric, value in experiment.evaluation.total_metrics.items():\r\n+                print(f\"{metric.upper()}: {value:.4f}\")\r\n+        \r\n+        print(\"\\nTraining History:\")\r\n+        if experiment.training_report:\r\n+            if hasattr(experiment.training_report, 'train_losses'):\r\n+                final_train = experiment.training_report.train_losses[-1]\r\n+                best_train = min(experiment.training_report.train_losses)\r\n+                print(f\"Training Loss - Final: {final_train:.4f}, Best: {best_train:.4f}\")\r\n+                print(f\"Loss Improvement: {((final_train - best_train) / final_train) * 100:.2f}%\")\r\n+            \r\n+            if hasattr(experiment.training_report, 'val_losses'):\r\n+                final_val = experiment.training_report.val_losses[-1]\r\n+                best_val = min(experiment.training_report.val_losses)\r\n+                print(f\"Validation Loss - Final: {final_val:.4f}, Best: {best_val:.4f}\")\r\n+                print(f\"Validation Improvement: {((final_val - best_val) / final_val) * 100:.2f}%\")\r\n+        \r\n+        # Save experiment\r\n+        print(\"\\nSaving experiment results...\")\r\n+        experiment.save_to_json_file()\r\n+        print(f\"Experiment saved successfully to the experiments directory\")\r\n+        \r\n+    except Exception as e:\r\n+        print(f\"\\nError during training: {str(e)}\")\r\n+        raise\r\n+\r\n+def main():\r\n+    # Configure argument parser\r\n+    parser = argparse.ArgumentParser(\r\n+        description='Train transformer model for energy consumption forecasting',\r\n+        formatter_class=argparse.ArgumentDefaultsHelpFormatter\r\n+    )\r\n+    \r\n+    # Add arguments\r\n+    parser.add_argument(\r\n+        '--data',\r\n+        type=str,\r\n+        required=True,\r\n+        help='Path to the CSV data file containing energy consumption data'\r\n+    )\r\n+    \r\n+    # Parse arguments and train\r\n+    args = parser.parse_args()\r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733237062183,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,46 +25,59 @@\n     d_ff = d_model * 4   # Feed-forward dimension (standard transformer ratio)\r\n     n_layers = 6         # Number of transformer layers (standard configuration)\r\n \r\n     model_params = {\r\n-    \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n-    \r\n-    # Architecture parameters\r\n-    \"input_features\": n_features,\r\n-    \"output_features\": 1,\r\n-    \"d_model\": d_model,\r\n-    \"n_heads\": n_heads,\r\n-    \"n_encoder_layers\": n_layers,\r\n-    \"n_decoder_layers\": n_layers,\r\n-    \"d_ff\": d_ff,\r\n-    \"dropout\": 0.1,\r\n-    \r\n-    # Sequence parameters\r\n-    \"input_resolution_minutes\": 60,\r\n-    \"forecast_resolution_minutes\": 60,\r\n-    \"lookback_periods\": sequence_length,\r\n-    \"forecast_periods\": sequence_length,\r\n-    \"max_sequence_length\": 5000,\r\n-    \r\n-    # Feature configuration\r\n-    \"kernel_size\": 3,\r\n-    \"value_features\": 1,\r\n-    \"time_features\": n_features - 1,\r\n-    \"batch_first\": True,\r\n-    \r\n-    # Attention configuration\r\n-    \"attention_dropout\": 0.1,\r\n-    \"attention_type\": \"scaled_dot\",\r\n-    \"embedding_dropout\": 0.1,\r\n-    \"positional_encoding\": True,\r\n-    \r\n-    # Device and training settings\r\n-    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\r\n-    \"batch_size\": 32,\r\n-    \"learning_rate\": 0.0001,\r\n-    \"optimizer\": \"adamw\",\r\n-    \"criterion\": \"mse\"\r\n-}\r\n+        # Core model identification\r\n+        \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n+        \r\n+        # Time resolution and sequence configuration\r\n+        \"input_resolution_minutes\": 60,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"lookback_periods\": sequence_length,\r\n+        \"forecast_periods\": sequence_length,\r\n+        \r\n+        # Model architecture parameters\r\n+        \"input_features\": n_features,\r\n+        \"output_features\": 1,\r\n+        \"d_model\": d_model,\r\n+        \"n_heads\": n_heads,\r\n+        \"n_encoder_layers\": n_layers,\r\n+        \"n_decoder_layers\": n_layers,\r\n+        \"d_ff\": d_ff,\r\n+        \"dropout\": 0.1,\r\n+        \"max_sequence_length\": 5000,\r\n+        \r\n+        # Time series specific parameters\r\n+        \"value_features\": 1,\r\n+        \"time_features\": n_features - 1,\r\n+        \"kernel_size\": 3,\r\n+        \"batch_first\": True,\r\n+        \r\n+        # Neural network specific\r\n+        \"hidden_dims\": [64, 32],\r\n+        \"activation\": \"relu\",\r\n+        \r\n+        # Training\r\n+        \"batch_size\": 32,\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"optimizer\": \"adamw\",\r\n+        \"optimizer_config\": {\r\n+            \"weight_decay\": 0.01,\r\n+            \"betas\": (0.9, 0.98)\r\n+        },\r\n+        \"scheduler\": \"one_cycle\",\r\n+        \"scheduler_config\": {\r\n+            \"pct_start\": 0.3,\r\n+            \"div_factor\": 25.0,\r\n+            \"final_div_factor\": 1000.0\r\n+        },\r\n+        \"criterion\": \"mse\",\r\n+        \"criterion_config\": {},\r\n+        \r\n+        # Device\r\n+        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n+    }\r\n \r\n     training_params = {\r\n         # Core training settings\r\n         \"learning_rate\": 0.0001,\r\n"
                },
                {
                    "date": 1733251309969,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -181,16 +181,41 @@\n     }\r\n \r\n     return model_params, training_params, dataset_params\r\n \r\n+def print_epoch_summary(epoch: int, total_epochs: int, train_loss: float, \r\n+                       val_loss: float, best_train: float, best_val: float,\r\n+                       time_elapsed: float) -> None:\r\n+    \"\"\"Print a formatted summary of the current training epoch.\"\"\"\r\n+    print(f\"\\nEpoch [{epoch}/{total_epochs}] - Time: {time_elapsed:.2f}s\")\r\n+    print(f\"Training Loss: {train_loss:.4f} (Best: {best_train:.4f})\")\r\n+    if val_loss is not None:\r\n+        print(f\"Validation Loss: {val_loss:.4f} (Best: {best_val:.4f})\")\r\n+        train_improvement = ((train_loss - best_train) / train_loss) * 100\r\n+        val_improvement = ((val_loss - best_val) / val_loss) * 100\r\n+        print(f\"Improvements - Train: {train_improvement:.2f}%, Val: {val_improvement:.2f}%\")\r\n+\r\n def train_model(data_path: str) -> None:\r\n     \"\"\"\r\n-    Train the transformer model for energy consumption forecasting.\r\n+    Train the transformer model for energy consumption forecasting with enhanced progress monitoring.\r\n     \r\n     Args:\r\n         data_path: Path to the CSV data file containing energy consumption data\r\n     \"\"\"\r\n     try:\r\n+        # Initialize logging\r\n+        logger = Logger.get_logger(\r\n+            __name__,\r\n+            LoggerConfig(\r\n+                level=LogLevel.INFO,\r\n+                component_name=\"TrainingPipeline\",\r\n+                include_timestamp=True,\r\n+                json_output=True,\r\n+                file_path=Path(\"logs/training.log\")\r\n+            )\r\n+        )\r\n+        logger.info(\"Starting training process\", {\"data_path\": data_path})\r\n+\r\n         # Initialize systems\r\n         initialize_model_registry()\r\n         register_models()\r\n         \r\n@@ -205,314 +230,107 @@\n             training_params=training_params,\r\n             dataset_params=dataset_params\r\n         )\r\n         \r\n+        # Initialize evaluator for monitoring training progress\r\n+        evaluator = Evaluator(\r\n+            scaler=config.dataset_config.time_series_scaler,\r\n+            metric_config=MetricConfig(\r\n+                resolution_minutes=config.dataset_config.forecast_resolution_minutes\r\n+            ),\r\n+            resolution_minutes=config.dataset_config.forecast_resolution_minutes\r\n+        )\r\n+        \r\n         # Initialize and run pipeline\r\n         print(\"\\nInitializing training pipeline...\")\r\n         pipeline = TimeSeriesPipeline(config)\r\n         \r\n         print(\"\\nStarting model training...\")\r\n-        experiment = pipeline.run()\r\n+        start_time = time.time()\r\n         \r\n-        if experiment is None:\r\n-            raise RuntimeError(\"Training failed to produce experiment results\")\r\n+        # Set up progress tracking\r\n+        display_frequency = max(1, model_params[\"max_epochs\"] // 20)  # Show ~20 updates\r\n+        best_train_loss = float('inf')\r\n+        best_val_loss = float('inf')\r\n         \r\n-        # Print comprehensive training results\r\n-        print(\"\\nTransformer Training Results:\")\r\n-        print(f\"Training Duration: {experiment.training_time:.2f} seconds\")\r\n-        print(f\"Inference Time: {experiment.test_time:.2f} seconds\")\r\n-        \r\n-        print(\"\\nModel Performance Metrics:\")\r\n-        if hasattr(experiment.evaluation, 'total_metrics'):\r\n-            for metric, value in experiment.evaluation.total_metrics.items():\r\n-                print(f\"{metric.upper()}: {value:.4f}\")\r\n-        \r\n-        print(\"\\nTraining History:\")\r\n-        if experiment.training_report:\r\n-            if hasattr(experiment.training_report, 'train_losses'):\r\n-                final_train = experiment.training_report.train_losses[-1]\r\n-                best_train = min(experiment.training_report.train_losses)\r\n-                print(f\"Training Loss - Final: {final_train:.4f}, Best: {best_train:.4f}\")\r\n-                print(f\"Loss Improvement: {((final_train - best_train) / final_train) * 100:.2f}%\")\r\n+        def progress_callback(epoch: int, train_loss: float, val_loss: float) -> None:\r\n+            \"\"\"Callback to monitor training progress.\"\"\"\r\n+            nonlocal best_train_loss, best_val_loss\r\n             \r\n-            if hasattr(experiment.training_report, 'val_losses'):\r\n-                final_val = experiment.training_report.val_losses[-1]\r\n-                best_val = min(experiment.training_report.val_losses)\r\n-                print(f\"Validation Loss - Final: {final_val:.4f}, Best: {best_val:.4f}\")\r\n-                print(f\"Validation Improvement: {((final_val - best_val) / final_val) * 100:.2f}%\")\r\n+            best_train_loss = min(best_train_loss, train_loss)\r\n+            if val_loss is not None:\r\n+                best_val_loss = min(best_val_loss, val_loss)\r\n+            \r\n+            if epoch % display_frequency == 0 or epoch == model_params[\"max_epochs\"] - 1:\r\n+                time_elapsed = time.time() - start_time\r\n+                print_epoch_summary(\r\n+                    epoch + 1,  # Convert to 1-based indexing for display\r\n+                    model_params[\"max_epochs\"],\r\n+                    train_loss,\r\n+                    val_loss,\r\n+                    best_train_loss,\r\n+                    best_val_loss,\r\n+                    time_elapsed\r\n+                )\r\n+                \r\n+                # Log detailed metrics\r\n+                logger.info(\"Training progress\", {\r\n+                    \"epoch\": epoch + 1,\r\n+                    \"train_loss\": float(train_loss),\r\n+                    \"val_loss\": float(val_loss) if val_loss is not None else None,\r\n+                    \"best_train_loss\": float(best_train_loss),\r\n+                    \"best_val_loss\": float(best_val_loss),\r\n+                    \"time_elapsed\": time_elapsed\r\n+                })\r\n         \r\n-        # Save experiment\r\n-        print(\"\\nSaving experiment results...\")\r\n-        experiment.save_to_json_file()\r\n-        print(f\"Experiment saved successfully to the experiments directory\")\r\n+        # Add callback to pipeline\r\n+        pipeline.add_progress_callback(progress_callback)\r\n         \r\n-    except Exception as e:\r\n-        print(f\"\\nError during training: {str(e)}\")\r\n-        raise\r\n-\r\n-def main():\r\n-    # Configure argument parser\r\n-    parser = argparse.ArgumentParser(\r\n-        description='Train transformer model for energy consumption forecasting',\r\n-        formatter_class=argparse.ArgumentDefaultsHelpFormatter\r\n-    )\r\n-    \r\n-    # Add arguments\r\n-    parser.add_argument(\r\n-        '--data',\r\n-        type=str,\r\n-        required=True,\r\n-        help='Path to the CSV data file containing energy consumption data'\r\n-    )\r\n-    \r\n-    # Parse arguments and train\r\n-    args = parser.parse_args()\r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from sklearn.preprocessing import StandardScaler\r\n-import torch\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-from models.register_models import register_models\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"\r\n-    Get optimized configuration parameters for transformer-based energy forecasting.\r\n-    Returns a tuple of (model_params, training_params, dataset_params).\r\n-    \"\"\"\r\n-    # Core architecture dimensions\r\n-    sequence_length = 24   # Hours in a day for hourly predictions\r\n-    n_features = 7        # Energy value (1) + cyclical time encodings (6)\r\n-    d_model = 512        # Transformer embedding dimension\r\n-    n_heads = 8          # Number of attention heads (must divide d_model evenly)\r\n-    \r\n-    # Compute derived parameters\r\n-    d_ff = d_model * 4   # Feed-forward dimension (standard transformer ratio)\r\n-    n_layers = 6         # Number of transformer layers (standard configuration)\r\n-\r\n-    model_params = {\r\n-        \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n-        \r\n-        # Architecture parameters\r\n-        \"input_features\": n_features,\r\n-        \"output_features\": 1,              # Single target value prediction\r\n-        \"d_model\": d_model,\r\n-        \"n_heads\": n_heads,\r\n-        \"n_encoder_layers\": n_layers,\r\n-        \"n_decoder_layers\": n_layers,\r\n-        \"d_ff\": d_ff,\r\n-        \"dropout\": 0.1,\r\n-        \r\n-        # Sequence parameters\r\n-        \"input_resolution_minutes\": 60,\r\n-        \"forecast_resolution_minutes\": 60,\r\n-        \"lookback_periods\": sequence_length,\r\n-        \"forecast_periods\": sequence_length,\r\n-        \r\n-        # Feature configuration\r\n-        \"kernel_size\": 3,\r\n-        \"value_features\": 1,\r\n-        \"time_features\": n_features - 1,\r\n-        \"batch_first\": True,\r\n-        \r\n-        # Embedding parameters\r\n-        \"max_sequence_length\": 5000,       # Maximum supported sequence length\r\n-        \"embedding_dropout\": 0.1,          # Dropout for embeddings\r\n-        \"positional_encoding\": True,       # Use positional encoding\r\n-        \r\n-        # Attention parameters\r\n-        \"attention_dropout\": 0.1,          # Attention-specific dropout\r\n-        \"attention_type\": \"scaled_dot\",    # Type of attention mechanism\r\n-        \r\n-        # Device and precision settings\r\n-        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\r\n-        \"dtype\": torch.float32,            # Precision for computations\r\n-        \r\n-        # Training hyperparameters\r\n-        \"batch_size\": 32,\r\n-        \"learning_rate\": 0.0001,\r\n-        \"optimizer\": \"adamw\",\r\n-        \"criterion\": \"mse\",\r\n-        \"gradient_clip_val\": 1.0\r\n-    }\r\n-\r\n-    training_params = {\r\n-        # Core training settings\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"batch_size\": 32,\r\n-        \"device\": model_params[\"device\"],\r\n-        \r\n-        # Early stopping\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"early_stopping_min_delta\": 1e-4,\r\n-        \"save_best_model\": True,\r\n-        \r\n-        # Sequence handling\r\n-        \"input_resolution_minutes\": 60,\r\n-        \"forecast_resolution_minutes\": 60,\r\n-        \"transformer_labels_count\": sequence_length,\r\n-        \"forecasting_horizon\": sequence_length,\r\n-        \r\n-        # Teacher forcing\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"teacher_forcing_ratio\": 0.5,\r\n-        \"teacher_forcing_schedule\": \"linear_decay\",\r\n-        \r\n-        # Optimization\r\n-        \"gradient_clip_val\": 1.0,\r\n-        \"optimizer\": \"adamw\",\r\n-        \"optimizer_config\": {\r\n-            \"weight_decay\": 0.01,\r\n-            \"betas\": (0.9, 0.98),\r\n-            \"eps\": 1e-8\r\n-        },\r\n-        \r\n-        # Learning rate scheduling\r\n-        \"scheduler\": \"one_cycle\",\r\n-        \"scheduler_config\": {\r\n-            \"pct_start\": 0.3,\r\n-            \"div_factor\": 25.0,\r\n-            \"final_div_factor\": 1000.0,\r\n-            \"anneal_strategy\": \"cos\",\r\n-            \"cycle_momentum\": True\r\n-        },\r\n-        \r\n-        # Validation settings\r\n-        \"validation_frequency\": 1,         # Validate every N epochs\r\n-        \"validation_metric\": \"mae\",        # Primary metric for model selection\r\n-        \r\n-        # Logging and checkpointing\r\n-        \"log_frequency\": 10,               # Log every N batches\r\n-        \"checkpoint_frequency\": 5,         # Save checkpoint every N epochs\r\n-        \"keep_last_n_checkpoints\": 3       # Number of checkpoints to keep\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        # Core parameters\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"energy_consumption\",\r\n-        \r\n-        # Resolution and sequence settings\r\n-        \"input_resolution_minutes\": 60,\r\n-        \"forecast_resolution_minutes\": 60,\r\n-        \"lookback_periods\": sequence_length,\r\n-        \"forecast_periods\": sequence_length,\r\n-        \r\n-        # Core feature configuration\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": sequence_length,\r\n-        \"one_hot_time_variables\": False,\r\n-        \r\n-        # Data preprocessing\r\n-        \"normalize_data\": True,\r\n-        \"scaling_method\": \"standard\",\r\n-        \"time_series_scaler\": StandardScaler(),\r\n-        \"handle_missing_values\": \"interpolate\",\r\n-        \"remove_outliers\": True,\r\n-        \"outlier_std_threshold\": 3.0,\r\n-        \r\n-        # Basic feature flags\r\n-        \"add_time_features\": True,\r\n-        \"add_holiday_features\": True,\r\n-        \"add_weather_features\": False,\r\n-        \r\n-        # Detailed time features\r\n-        \"add_hour_feature\": True,\r\n-        \"add_weekday_feature\": True,\r\n-        \"add_month_feature\": True,\r\n-        \"add_season_feature\": False,\r\n-        \"add_year_feature\": False,\r\n-        \r\n-        # Data augmentation\r\n-        \"use_data_augmentation\": False,\r\n-        \"augmentation_methods\": [\"jitter\", \"scaling\"],\r\n-        \"augmentation_probability\": 0.3,\r\n-        \r\n-        # Sequence handling\r\n-        \"padding_value\": 0.0,\r\n-        \"mask_padding\": True,\r\n-        \"max_sequence_gaps\": 3\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"\r\n-    Train the transformer model for energy consumption forecasting.\r\n-    \r\n-    Args:\r\n-        data_path: Path to the CSV data file containing energy consumption data\r\n-    \"\"\"\r\n-    try:\r\n-        # Initialize systems\r\n-        initialize_model_registry()\r\n-        register_models()\r\n-        \r\n-        # Get and validate configuration\r\n-        model_params, training_params, dataset_params = get_default_config_params()\r\n-        \r\n-        # Create pipeline configuration\r\n-        config = create_pipeline_config(\r\n-            data_path=data_path,\r\n-            model_type=ModelType.HOURLY_TRANSFORMER,\r\n-            model_params=model_params,\r\n-            training_params=training_params,\r\n-            dataset_params=dataset_params\r\n-        )\r\n-        \r\n-        # Initialize and run pipeline\r\n-        print(\"\\nInitializing training pipeline...\")\r\n-        pipeline = TimeSeriesPipeline(config)\r\n-        \r\n-        print(\"\\nStarting model training...\")\r\n+        # Run training\r\n         experiment = pipeline.run()\r\n         \r\n         if experiment is None:\r\n             raise RuntimeError(\"Training failed to produce experiment results\")\r\n         \r\n-        # Print comprehensive training results\r\n-        print(\"\\nTransformer Training Results:\")\r\n-        print(f\"Training Duration: {experiment.training_time:.2f} seconds\")\r\n+        # Calculate final metrics\r\n+        final_metrics = {}\r\n+        if hasattr(experiment.evaluation, 'total_metrics'):\r\n+            final_metrics = experiment.evaluation.total_metrics\r\n+            logger.info(\"Final model metrics\", final_metrics)\r\n+        \r\n+        # Print final summary\r\n+        print(\"\\nTraining Complete!\")\r\n+        print(f\"Total Duration: {time.time() - start_time:.2f} seconds\")\r\n         print(f\"Inference Time: {experiment.test_time:.2f} seconds\")\r\n         \r\n-        print(\"\\nModel Performance Metrics:\")\r\n-        if hasattr(experiment.evaluation, 'total_metrics'):\r\n-            for metric, value in experiment.evaluation.total_metrics.items():\r\n-                print(f\"{metric.upper()}: {value:.4f}\")\r\n+        print(\"\\nFinal Model Performance:\")\r\n+        for metric, value in final_metrics.items():\r\n+            print(f\"{metric.upper()}: {value:.4f}\")\r\n         \r\n-        print(\"\\nTraining History:\")\r\n         if experiment.training_report:\r\n-            if hasattr(experiment.training_report, 'train_losses'):\r\n-                final_train = experiment.training_report.train_losses[-1]\r\n-                best_train = min(experiment.training_report.train_losses)\r\n-                print(f\"Training Loss - Final: {final_train:.4f}, Best: {best_train:.4f}\")\r\n-                print(f\"Loss Improvement: {((final_train - best_train) / final_train) * 100:.2f}%\")\r\n+            train_losses = experiment.training_report.train_losses\r\n+            val_losses = experiment.training_report.val_losses\r\n             \r\n-            if hasattr(experiment.training_report, 'val_losses'):\r\n-                final_val = experiment.training_report.val_losses[-1]\r\n-                best_val = min(experiment.training_report.val_losses)\r\n-                print(f\"Validation Loss - Final: {final_val:.4f}, Best: {best_val:.4f}\")\r\n-                print(f\"Validation Improvement: {((final_val - best_val) / final_val) * 100:.2f}%\")\r\n+            print(\"\\nTraining Summary:\")\r\n+            print(f\"Starting Train Loss: {train_losses[0]:.4f}\")\r\n+            print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\r\n+            print(f\"Best Train Loss: {min(train_losses):.4f}\")\r\n+            \r\n+            if val_losses:\r\n+                print(f\"\\nStarting Val Loss: {val_losses[0]:.4f}\")\r\n+                print(f\"Final Val Loss: {val_losses[-1]:.4f}\")\r\n+                print(f\"Best Val Loss: {min(val_losses):.4f}\")\r\n+            \r\n+            if experiment.training_report.early_stopping_epoch:\r\n+                print(f\"\\nEarly stopping occurred at epoch {experiment.training_report.early_stopping_epoch}\")\r\n         \r\n         # Save experiment\r\n-        print(\"\\nSaving experiment results...\")\r\n+        logger.info(\"Saving experiment results...\")\r\n         experiment.save_to_json_file()\r\n-        print(f\"Experiment saved successfully to the experiments directory\")\r\n+        logger.info(\"Training completed successfully\")\r\n         \r\n     except Exception as e:\r\n-        print(f\"\\nError during training: {str(e)}\")\r\n+        logger.error(\"Training failed\", {\"error\": str(e)})\r\n         raise\r\n \r\n def main():\r\n     # Configure argument parser\r\n"
                },
                {
                    "date": 1733251362810,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,361 @@\n+import argparse\r\n+import time\r\n+from pathlib import Path\r\n+from typing import Dict, Any, Tuple\r\n+\r\n+from sklearn.preprocessing import StandardScaler\r\n+import torch\r\n+\r\n+from pipeline.utils.config_utils import create_pipeline_config\r\n+from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n+from models.registry.model_types import ModelType, initialize_model_registry\r\n+from models.register_models import register_models\r\n+from evaluation.evaluator import Evaluator\r\n+from evaluation.metrics import MetricConfig\r\n+from utils.logging.config import LogLevel, LoggerConfig\r\n+from utils.logging.logger import Logger\r\n+\r\n+\r\n+def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n+    \"\"\"\r\n+    Get optimized configuration parameters for transformer-based energy forecasting.\r\n+    Returns a tuple of (model_params, training_params, dataset_params).\r\n+    \"\"\"\r\n+    # Core architecture dimensions\r\n+    sequence_length = 24   # Hours in a day for hourly predictions\r\n+    n_features = 7        # Energy value (1) + cyclical time encodings (6)\r\n+    d_model = 512        # Transformer embedding dimension\r\n+    n_heads = 8          # Number of attention heads (must divide d_model evenly)\r\n+    \r\n+    # Compute derived parameters\r\n+    d_ff = d_model * 4   # Feed-forward dimension (standard transformer ratio)\r\n+    n_layers = 6         # Number of transformer layers (standard configuration)\r\n+\r\n+    model_params = {\r\n+        # Core model identification\r\n+        \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n+        \r\n+        # Time resolution and sequence configuration\r\n+        \"input_resolution_minutes\": 60,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"lookback_periods\": sequence_length,\r\n+        \"forecast_periods\": sequence_length,\r\n+        \r\n+        # Model architecture parameters\r\n+        \"input_features\": n_features,\r\n+        \"output_features\": 1,\r\n+        \"d_model\": d_model,\r\n+        \"n_heads\": n_heads,\r\n+        \"n_encoder_layers\": n_layers,\r\n+        \"n_decoder_layers\": n_layers,\r\n+        \"d_ff\": d_ff,\r\n+        \"dropout\": 0.1,\r\n+        \"max_sequence_length\": 5000,\r\n+        \r\n+        # Time series specific parameters\r\n+        \"value_features\": 1,\r\n+        \"time_features\": n_features - 1,\r\n+        \"kernel_size\": 3,\r\n+        \"batch_first\": True,\r\n+        \r\n+        # Neural network specific\r\n+        \"hidden_dims\": [64, 32],\r\n+        \"activation\": \"relu\",\r\n+        \r\n+        # Training\r\n+        \"batch_size\": 32,\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"optimizer\": \"adamw\",\r\n+        \"optimizer_config\": {\r\n+            \"weight_decay\": 0.01,\r\n+            \"betas\": (0.9, 0.98)\r\n+        },\r\n+        \"scheduler\": \"one_cycle\",\r\n+        \"scheduler_config\": {\r\n+            \"pct_start\": 0.3,\r\n+            \"div_factor\": 25.0,\r\n+            \"final_div_factor\": 1000.0\r\n+        },\r\n+        \"criterion\": \"mse\",\r\n+        \"criterion_config\": {},\r\n+        \r\n+        # Device\r\n+        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n+    }\r\n+\r\n+    training_params = {\r\n+        # Core training settings\r\n+        \"learning_rate\": 0.0001,\r\n+        \"max_epochs\": 100,\r\n+        \"batch_size\": 32,\r\n+        \"device\": model_params[\"device\"],\r\n+        \r\n+        # Early stopping\r\n+        \"use_early_stopping\": True,\r\n+        \"early_stopping_patience\": 15,\r\n+        \"early_stopping_min_delta\": 1e-4,\r\n+        \"save_best_model\": True,\r\n+        \r\n+        # Sequence handling\r\n+        \"input_resolution_minutes\": 60,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"transformer_labels_count\": sequence_length,\r\n+        \"forecasting_horizon\": sequence_length,\r\n+        \r\n+        # Teacher forcing\r\n+        \"transformer_use_teacher_forcing\": True,\r\n+        \"teacher_forcing_ratio\": 0.5,\r\n+        \"teacher_forcing_schedule\": \"linear_decay\",\r\n+        \r\n+        # Optimization\r\n+        \"gradient_clip_val\": 1.0,\r\n+        \"optimizer\": \"adamw\",\r\n+        \"optimizer_config\": {\r\n+            \"weight_decay\": 0.01,\r\n+            \"betas\": (0.9, 0.98),\r\n+            \"eps\": 1e-8\r\n+        },\r\n+        \r\n+        # Learning rate scheduling\r\n+        \"scheduler\": \"one_cycle\",\r\n+        \"scheduler_config\": {\r\n+            \"pct_start\": 0.3,\r\n+            \"div_factor\": 25.0,\r\n+            \"final_div_factor\": 1000.0,\r\n+            \"anneal_strategy\": \"cos\",\r\n+            \"cycle_momentum\": True\r\n+        },\r\n+        \r\n+        # Validation settings\r\n+        \"validation_frequency\": 1,         # Validate every N epochs\r\n+        \"validation_metric\": \"mae\",        # Primary metric for model selection\r\n+        \r\n+        # Logging and checkpointing\r\n+        \"log_frequency\": 10,               # Log every N batches\r\n+        \"checkpoint_frequency\": 5,         # Save checkpoint every N epochs\r\n+        \"keep_last_n_checkpoints\": 3       # Number of checkpoints to keep\r\n+    }\r\n+\r\n+    dataset_params = {\r\n+        # Core parameters\r\n+        \"time_variable\": \"utc_timestamp\",\r\n+        \"target_variable\": \"energy_consumption\",\r\n+        \r\n+        # Resolution and sequence settings\r\n+        \"input_resolution_minutes\": 60,\r\n+        \"forecast_resolution_minutes\": 60,\r\n+        \"lookback_periods\": sequence_length,\r\n+        \"forecast_periods\": sequence_length,\r\n+        \r\n+        # Core feature configuration\r\n+        \"is_single_time_point_prediction\": False,\r\n+        \"include_time_information\": True,\r\n+        \"is_training_set\": True,\r\n+        \"labels_count\": sequence_length,\r\n+        \"one_hot_time_variables\": False,\r\n+        \r\n+        # Data preprocessing\r\n+        \"normalize_data\": True,\r\n+        \"scaling_method\": \"standard\",\r\n+        \"time_series_scaler\": StandardScaler(),\r\n+        \"handle_missing_values\": \"interpolate\",\r\n+        \"remove_outliers\": True,\r\n+        \"outlier_std_threshold\": 3.0,\r\n+        \r\n+        # Basic feature flags\r\n+        \"add_time_features\": True,\r\n+        \"add_holiday_features\": True,\r\n+        \"add_weather_features\": False,\r\n+        \r\n+        # Detailed time features\r\n+        \"add_hour_feature\": True,\r\n+        \"add_weekday_feature\": True,\r\n+        \"add_month_feature\": True,\r\n+        \"add_season_feature\": False,\r\n+        \"add_year_feature\": False,\r\n+        \r\n+        # Data augmentation\r\n+        \"use_data_augmentation\": False,\r\n+        \"augmentation_methods\": [\"jitter\", \"scaling\"],\r\n+        \"augmentation_probability\": 0.3,\r\n+        \r\n+        # Sequence handling\r\n+        \"padding_value\": 0.0,\r\n+        \"mask_padding\": True,\r\n+        \"max_sequence_gaps\": 3\r\n+    }\r\n+\r\n+    return model_params, training_params, dataset_params\r\n+\r\n+def print_epoch_summary(epoch: int, total_epochs: int, train_loss: float, \r\n+                       val_loss: float, best_train: float, best_val: float,\r\n+                       time_elapsed: float) -> None:\r\n+    \"\"\"Print a formatted summary of the current training epoch.\"\"\"\r\n+    print(f\"\\nEpoch [{epoch}/{total_epochs}] - Time: {time_elapsed:.2f}s\")\r\n+    print(f\"Training Loss: {train_loss:.4f} (Best: {best_train:.4f})\")\r\n+    if val_loss is not None:\r\n+        print(f\"Validation Loss: {val_loss:.4f} (Best: {best_val:.4f})\")\r\n+        train_improvement = ((train_loss - best_train) / train_loss) * 100\r\n+        val_improvement = ((val_loss - best_val) / val_loss) * 100\r\n+        print(f\"Improvements - Train: {train_improvement:.2f}%, Val: {val_improvement:.2f}%\")\r\n+\r\n+def train_model(data_path: str) -> None:\r\n+    \"\"\"\r\n+    Train the transformer model for energy consumption forecasting with enhanced progress monitoring.\r\n+    \r\n+    Args:\r\n+        data_path: Path to the CSV data file containing energy consumption data\r\n+    \"\"\"\r\n+    try:\r\n+        # Initialize logging\r\n+        logger = Logger.get_logger(\r\n+            __name__,\r\n+            LoggerConfig(\r\n+                level=LogLevel.INFO,\r\n+                component_name=\"TrainingPipeline\",\r\n+                include_timestamp=True,\r\n+                json_output=True,\r\n+                file_path=Path(\"logs/training.log\")\r\n+            )\r\n+        )\r\n+        logger.info(\"Starting training process\", {\"data_path\": data_path})\r\n+\r\n+        # Initialize systems\r\n+        initialize_model_registry()\r\n+        register_models()\r\n+        \r\n+        # Get and validate configuration\r\n+        model_params, training_params, dataset_params = get_default_config_params()\r\n+        \r\n+        # Create pipeline configuration\r\n+        config = create_pipeline_config(\r\n+            data_path=data_path,\r\n+            model_type=ModelType.HOURLY_TRANSFORMER,\r\n+            model_params=model_params,\r\n+            training_params=training_params,\r\n+            dataset_params=dataset_params\r\n+        )\r\n+        \r\n+        # Initialize evaluator for monitoring training progress\r\n+        evaluator = Evaluator(\r\n+            scaler=config.dataset_config.time_series_scaler,\r\n+            metric_config=MetricConfig(\r\n+                resolution_minutes=config.dataset_config.forecast_resolution_minutes\r\n+            ),\r\n+            resolution_minutes=config.dataset_config.forecast_resolution_minutes\r\n+        )\r\n+        \r\n+        # Initialize and run pipeline\r\n+        print(\"\\nInitializing training pipeline...\")\r\n+        pipeline = TimeSeriesPipeline(config)\r\n+        \r\n+        print(\"\\nStarting model training...\")\r\n+        start_time = time.time()\r\n+        \r\n+        # Set up progress tracking\r\n+        display_frequency = max(1, model_params[\"max_epochs\"] // 20)  # Show ~20 updates\r\n+        best_train_loss = float('inf')\r\n+        best_val_loss = float('inf')\r\n+        \r\n+        def progress_callback(epoch: int, train_loss: float, val_loss: float) -> None:\r\n+            \"\"\"Callback to monitor training progress.\"\"\"\r\n+            nonlocal best_train_loss, best_val_loss\r\n+            \r\n+            best_train_loss = min(best_train_loss, train_loss)\r\n+            if val_loss is not None:\r\n+                best_val_loss = min(best_val_loss, val_loss)\r\n+            \r\n+            if epoch % display_frequency == 0 or epoch == model_params[\"max_epochs\"] - 1:\r\n+                time_elapsed = time.time() - start_time\r\n+                print_epoch_summary(\r\n+                    epoch + 1,  # Convert to 1-based indexing for display\r\n+                    model_params[\"max_epochs\"],\r\n+                    train_loss,\r\n+                    val_loss,\r\n+                    best_train_loss,\r\n+                    best_val_loss,\r\n+                    time_elapsed\r\n+                )\r\n+                \r\n+                # Log detailed metrics\r\n+                logger.info(\"Training progress\", {\r\n+                    \"epoch\": epoch + 1,\r\n+                    \"train_loss\": float(train_loss),\r\n+                    \"val_loss\": float(val_loss) if val_loss is not None else None,\r\n+                    \"best_train_loss\": float(best_train_loss),\r\n+                    \"best_val_loss\": float(best_val_loss),\r\n+                    \"time_elapsed\": time_elapsed\r\n+                })\r\n+        \r\n+        # Add callback to pipeline\r\n+        pipeline.add_progress_callback(progress_callback)\r\n+        \r\n+        # Run training\r\n+        experiment = pipeline.run()\r\n+        \r\n+        if experiment is None:\r\n+            raise RuntimeError(\"Training failed to produce experiment results\")\r\n+        \r\n+        # Calculate final metrics\r\n+        final_metrics = {}\r\n+        if hasattr(experiment.evaluation, 'total_metrics'):\r\n+            final_metrics = experiment.evaluation.total_metrics\r\n+            logger.info(\"Final model metrics\", final_metrics)\r\n+        \r\n+        # Print final summary\r\n+        print(\"\\nTraining Complete!\")\r\n+        print(f\"Total Duration: {time.time() - start_time:.2f} seconds\")\r\n+        print(f\"Inference Time: {experiment.test_time:.2f} seconds\")\r\n+        \r\n+        print(\"\\nFinal Model Performance:\")\r\n+        for metric, value in final_metrics.items():\r\n+            print(f\"{metric.upper()}: {value:.4f}\")\r\n+        \r\n+        if experiment.training_report:\r\n+            train_losses = experiment.training_report.train_losses\r\n+            val_losses = experiment.training_report.val_losses\r\n+            \r\n+            print(\"\\nTraining Summary:\")\r\n+            print(f\"Starting Train Loss: {train_losses[0]:.4f}\")\r\n+            print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\r\n+            print(f\"Best Train Loss: {min(train_losses):.4f}\")\r\n+            \r\n+            if val_losses:\r\n+                print(f\"\\nStarting Val Loss: {val_losses[0]:.4f}\")\r\n+                print(f\"Final Val Loss: {val_losses[-1]:.4f}\")\r\n+                print(f\"Best Val Loss: {min(val_losses):.4f}\")\r\n+            \r\n+            if experiment.training_report.early_stopping_epoch:\r\n+                print(f\"\\nEarly stopping occurred at epoch {experiment.training_report.early_stopping_epoch}\")\r\n+        \r\n+        # Save experiment\r\n+        logger.info(\"Saving experiment results...\")\r\n+        experiment.save_to_json_file()\r\n+        logger.info(\"Training completed successfully\")\r\n+        \r\n+    except Exception as e:\r\n+        logger.error(\"Training failed\", {\"error\": str(e)})\r\n+        raise\r\n+\r\n+def main():\r\n+    # Configure argument parser\r\n+    parser = argparse.ArgumentParser(\r\n+        description='Train transformer model for energy consumption forecasting',\r\n+        formatter_class=argparse.ArgumentDefaultsHelpFormatter\r\n+    )\r\n+    \r\n+    # Add arguments\r\n+    parser.add_argument(\r\n+        '--data',\r\n+        type=str,\r\n+        required=True,\r\n+        help='Path to the CSV data file containing energy consumption data'\r\n+    )\r\n+    \r\n+    # Parse arguments and train\r\n+    args = parser.parse_args()\r\n+    train_model(args.data)\r\n+\r\n+if __name__ == '__main__':\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733251479701,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -357,360 +357,5 @@\n     args = parser.parse_args()\r\n     train_model(args.data)\r\n \r\n if __name__ == '__main__':\r\n-    main()\n-import argparse\r\n-from pathlib import Path\r\n-from typing import Dict, Any, Tuple\r\n-\r\n-from sklearn.preprocessing import StandardScaler\r\n-import torch\r\n-\r\n-from pipeline.utils.config_utils import create_pipeline_config\r\n-from pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\n-from models.registry.model_types import ModelType, initialize_model_registry\r\n-from models.register_models import register_models\r\n-\r\n-def get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n-    \"\"\"\r\n-    Get optimized configuration parameters for transformer-based energy forecasting.\r\n-    Returns a tuple of (model_params, training_params, dataset_params).\r\n-    \"\"\"\r\n-    # Core architecture dimensions\r\n-    sequence_length = 24   # Hours in a day for hourly predictions\r\n-    n_features = 7        # Energy value (1) + cyclical time encodings (6)\r\n-    d_model = 512        # Transformer embedding dimension\r\n-    n_heads = 8          # Number of attention heads (must divide d_model evenly)\r\n-    \r\n-    # Compute derived parameters\r\n-    d_ff = d_model * 4   # Feed-forward dimension (standard transformer ratio)\r\n-    n_layers = 6         # Number of transformer layers (standard configuration)\r\n-\r\n-    model_params = {\r\n-        # Core model identification\r\n-        \"model_type\": ModelType.HOURLY_TRANSFORMER,\r\n-        \r\n-        # Time resolution and sequence configuration\r\n-        \"input_resolution_minutes\": 60,\r\n-        \"forecast_resolution_minutes\": 60,\r\n-        \"lookback_periods\": sequence_length,\r\n-        \"forecast_periods\": sequence_length,\r\n-        \r\n-        # Model architecture parameters\r\n-        \"input_features\": n_features,\r\n-        \"output_features\": 1,\r\n-        \"d_model\": d_model,\r\n-        \"n_heads\": n_heads,\r\n-        \"n_encoder_layers\": n_layers,\r\n-        \"n_decoder_layers\": n_layers,\r\n-        \"d_ff\": d_ff,\r\n-        \"dropout\": 0.1,\r\n-        \"max_sequence_length\": 5000,\r\n-        \r\n-        # Time series specific parameters\r\n-        \"value_features\": 1,\r\n-        \"time_features\": n_features - 1,\r\n-        \"kernel_size\": 3,\r\n-        \"batch_first\": True,\r\n-        \r\n-        # Neural network specific\r\n-        \"hidden_dims\": [64, 32],\r\n-        \"activation\": \"relu\",\r\n-        \r\n-        # Training\r\n-        \"batch_size\": 32,\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"optimizer\": \"adamw\",\r\n-        \"optimizer_config\": {\r\n-            \"weight_decay\": 0.01,\r\n-            \"betas\": (0.9, 0.98)\r\n-        },\r\n-        \"scheduler\": \"one_cycle\",\r\n-        \"scheduler_config\": {\r\n-            \"pct_start\": 0.3,\r\n-            \"div_factor\": 25.0,\r\n-            \"final_div_factor\": 1000.0\r\n-        },\r\n-        \"criterion\": \"mse\",\r\n-        \"criterion_config\": {},\r\n-        \r\n-        # Device\r\n-        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n-    }\r\n-\r\n-    training_params = {\r\n-        # Core training settings\r\n-        \"learning_rate\": 0.0001,\r\n-        \"max_epochs\": 100,\r\n-        \"batch_size\": 32,\r\n-        \"device\": model_params[\"device\"],\r\n-        \r\n-        # Early stopping\r\n-        \"use_early_stopping\": True,\r\n-        \"early_stopping_patience\": 15,\r\n-        \"early_stopping_min_delta\": 1e-4,\r\n-        \"save_best_model\": True,\r\n-        \r\n-        # Sequence handling\r\n-        \"input_resolution_minutes\": 60,\r\n-        \"forecast_resolution_minutes\": 60,\r\n-        \"transformer_labels_count\": sequence_length,\r\n-        \"forecasting_horizon\": sequence_length,\r\n-        \r\n-        # Teacher forcing\r\n-        \"transformer_use_teacher_forcing\": True,\r\n-        \"teacher_forcing_ratio\": 0.5,\r\n-        \"teacher_forcing_schedule\": \"linear_decay\",\r\n-        \r\n-        # Optimization\r\n-        \"gradient_clip_val\": 1.0,\r\n-        \"optimizer\": \"adamw\",\r\n-        \"optimizer_config\": {\r\n-            \"weight_decay\": 0.01,\r\n-            \"betas\": (0.9, 0.98),\r\n-            \"eps\": 1e-8\r\n-        },\r\n-        \r\n-        # Learning rate scheduling\r\n-        \"scheduler\": \"one_cycle\",\r\n-        \"scheduler_config\": {\r\n-            \"pct_start\": 0.3,\r\n-            \"div_factor\": 25.0,\r\n-            \"final_div_factor\": 1000.0,\r\n-            \"anneal_strategy\": \"cos\",\r\n-            \"cycle_momentum\": True\r\n-        },\r\n-        \r\n-        # Validation settings\r\n-        \"validation_frequency\": 1,         # Validate every N epochs\r\n-        \"validation_metric\": \"mae\",        # Primary metric for model selection\r\n-        \r\n-        # Logging and checkpointing\r\n-        \"log_frequency\": 10,               # Log every N batches\r\n-        \"checkpoint_frequency\": 5,         # Save checkpoint every N epochs\r\n-        \"keep_last_n_checkpoints\": 3       # Number of checkpoints to keep\r\n-    }\r\n-\r\n-    dataset_params = {\r\n-        # Core parameters\r\n-        \"time_variable\": \"utc_timestamp\",\r\n-        \"target_variable\": \"energy_consumption\",\r\n-        \r\n-        # Resolution and sequence settings\r\n-        \"input_resolution_minutes\": 60,\r\n-        \"forecast_resolution_minutes\": 60,\r\n-        \"lookback_periods\": sequence_length,\r\n-        \"forecast_periods\": sequence_length,\r\n-        \r\n-        # Core feature configuration\r\n-        \"is_single_time_point_prediction\": False,\r\n-        \"include_time_information\": True,\r\n-        \"is_training_set\": True,\r\n-        \"labels_count\": sequence_length,\r\n-        \"one_hot_time_variables\": False,\r\n-        \r\n-        # Data preprocessing\r\n-        \"normalize_data\": True,\r\n-        \"scaling_method\": \"standard\",\r\n-        \"time_series_scaler\": StandardScaler(),\r\n-        \"handle_missing_values\": \"interpolate\",\r\n-        \"remove_outliers\": True,\r\n-        \"outlier_std_threshold\": 3.0,\r\n-        \r\n-        # Basic feature flags\r\n-        \"add_time_features\": True,\r\n-        \"add_holiday_features\": True,\r\n-        \"add_weather_features\": False,\r\n-        \r\n-        # Detailed time features\r\n-        \"add_hour_feature\": True,\r\n-        \"add_weekday_feature\": True,\r\n-        \"add_month_feature\": True,\r\n-        \"add_season_feature\": False,\r\n-        \"add_year_feature\": False,\r\n-        \r\n-        # Data augmentation\r\n-        \"use_data_augmentation\": False,\r\n-        \"augmentation_methods\": [\"jitter\", \"scaling\"],\r\n-        \"augmentation_probability\": 0.3,\r\n-        \r\n-        # Sequence handling\r\n-        \"padding_value\": 0.0,\r\n-        \"mask_padding\": True,\r\n-        \"max_sequence_gaps\": 3\r\n-    }\r\n-\r\n-    return model_params, training_params, dataset_params\r\n-\r\n-def print_epoch_summary(epoch: int, total_epochs: int, train_loss: float, \r\n-                       val_loss: float, best_train: float, best_val: float,\r\n-                       time_elapsed: float) -> None:\r\n-    \"\"\"Print a formatted summary of the current training epoch.\"\"\"\r\n-    print(f\"\\nEpoch [{epoch}/{total_epochs}] - Time: {time_elapsed:.2f}s\")\r\n-    print(f\"Training Loss: {train_loss:.4f} (Best: {best_train:.4f})\")\r\n-    if val_loss is not None:\r\n-        print(f\"Validation Loss: {val_loss:.4f} (Best: {best_val:.4f})\")\r\n-        train_improvement = ((train_loss - best_train) / train_loss) * 100\r\n-        val_improvement = ((val_loss - best_val) / val_loss) * 100\r\n-        print(f\"Improvements - Train: {train_improvement:.2f}%, Val: {val_improvement:.2f}%\")\r\n-\r\n-def train_model(data_path: str) -> None:\r\n-    \"\"\"\r\n-    Train the transformer model for energy consumption forecasting with enhanced progress monitoring.\r\n-    \r\n-    Args:\r\n-        data_path: Path to the CSV data file containing energy consumption data\r\n-    \"\"\"\r\n-    try:\r\n-        # Initialize logging\r\n-        logger = Logger.get_logger(\r\n-            __name__,\r\n-            LoggerConfig(\r\n-                level=LogLevel.INFO,\r\n-                component_name=\"TrainingPipeline\",\r\n-                include_timestamp=True,\r\n-                json_output=True,\r\n-                file_path=Path(\"logs/training.log\")\r\n-            )\r\n-        )\r\n-        logger.info(\"Starting training process\", {\"data_path\": data_path})\r\n-\r\n-        # Initialize systems\r\n-        initialize_model_registry()\r\n-        register_models()\r\n-        \r\n-        # Get and validate configuration\r\n-        model_params, training_params, dataset_params = get_default_config_params()\r\n-        \r\n-        # Create pipeline configuration\r\n-        config = create_pipeline_config(\r\n-            data_path=data_path,\r\n-            model_type=ModelType.HOURLY_TRANSFORMER,\r\n-            model_params=model_params,\r\n-            training_params=training_params,\r\n-            dataset_params=dataset_params\r\n-        )\r\n-        \r\n-        # Initialize evaluator for monitoring training progress\r\n-        evaluator = Evaluator(\r\n-            scaler=config.dataset_config.time_series_scaler,\r\n-            metric_config=MetricConfig(\r\n-                resolution_minutes=config.dataset_config.forecast_resolution_minutes\r\n-            ),\r\n-            resolution_minutes=config.dataset_config.forecast_resolution_minutes\r\n-        )\r\n-        \r\n-        # Initialize and run pipeline\r\n-        print(\"\\nInitializing training pipeline...\")\r\n-        pipeline = TimeSeriesPipeline(config)\r\n-        \r\n-        print(\"\\nStarting model training...\")\r\n-        start_time = time.time()\r\n-        \r\n-        # Set up progress tracking\r\n-        display_frequency = max(1, model_params[\"max_epochs\"] // 20)  # Show ~20 updates\r\n-        best_train_loss = float('inf')\r\n-        best_val_loss = float('inf')\r\n-        \r\n-        def progress_callback(epoch: int, train_loss: float, val_loss: float) -> None:\r\n-            \"\"\"Callback to monitor training progress.\"\"\"\r\n-            nonlocal best_train_loss, best_val_loss\r\n-            \r\n-            best_train_loss = min(best_train_loss, train_loss)\r\n-            if val_loss is not None:\r\n-                best_val_loss = min(best_val_loss, val_loss)\r\n-            \r\n-            if epoch % display_frequency == 0 or epoch == model_params[\"max_epochs\"] - 1:\r\n-                time_elapsed = time.time() - start_time\r\n-                print_epoch_summary(\r\n-                    epoch + 1,  # Convert to 1-based indexing for display\r\n-                    model_params[\"max_epochs\"],\r\n-                    train_loss,\r\n-                    val_loss,\r\n-                    best_train_loss,\r\n-                    best_val_loss,\r\n-                    time_elapsed\r\n-                )\r\n-                \r\n-                # Log detailed metrics\r\n-                logger.info(\"Training progress\", {\r\n-                    \"epoch\": epoch + 1,\r\n-                    \"train_loss\": float(train_loss),\r\n-                    \"val_loss\": float(val_loss) if val_loss is not None else None,\r\n-                    \"best_train_loss\": float(best_train_loss),\r\n-                    \"best_val_loss\": float(best_val_loss),\r\n-                    \"time_elapsed\": time_elapsed\r\n-                })\r\n-        \r\n-        # Add callback to pipeline\r\n-        pipeline.add_progress_callback(progress_callback)\r\n-        \r\n-        # Run training\r\n-        experiment = pipeline.run()\r\n-        \r\n-        if experiment is None:\r\n-            raise RuntimeError(\"Training failed to produce experiment results\")\r\n-        \r\n-        # Calculate final metrics\r\n-        final_metrics = {}\r\n-        if hasattr(experiment.evaluation, 'total_metrics'):\r\n-            final_metrics = experiment.evaluation.total_metrics\r\n-            logger.info(\"Final model metrics\", final_metrics)\r\n-        \r\n-        # Print final summary\r\n-        print(\"\\nTraining Complete!\")\r\n-        print(f\"Total Duration: {time.time() - start_time:.2f} seconds\")\r\n-        print(f\"Inference Time: {experiment.test_time:.2f} seconds\")\r\n-        \r\n-        print(\"\\nFinal Model Performance:\")\r\n-        for metric, value in final_metrics.items():\r\n-            print(f\"{metric.upper()}: {value:.4f}\")\r\n-        \r\n-        if experiment.training_report:\r\n-            train_losses = experiment.training_report.train_losses\r\n-            val_losses = experiment.training_report.val_losses\r\n-            \r\n-            print(\"\\nTraining Summary:\")\r\n-            print(f\"Starting Train Loss: {train_losses[0]:.4f}\")\r\n-            print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\r\n-            print(f\"Best Train Loss: {min(train_losses):.4f}\")\r\n-            \r\n-            if val_losses:\r\n-                print(f\"\\nStarting Val Loss: {val_losses[0]:.4f}\")\r\n-                print(f\"Final Val Loss: {val_losses[-1]:.4f}\")\r\n-                print(f\"Best Val Loss: {min(val_losses):.4f}\")\r\n-            \r\n-            if experiment.training_report.early_stopping_epoch:\r\n-                print(f\"\\nEarly stopping occurred at epoch {experiment.training_report.early_stopping_epoch}\")\r\n-        \r\n-        # Save experiment\r\n-        logger.info(\"Saving experiment results...\")\r\n-        experiment.save_to_json_file()\r\n-        logger.info(\"Training completed successfully\")\r\n-        \r\n-    except Exception as e:\r\n-        logger.error(\"Training failed\", {\"error\": str(e)})\r\n-        raise\r\n-\r\n-def main():\r\n-    # Configure argument parser\r\n-    parser = argparse.ArgumentParser(\r\n-        description='Train transformer model for energy consumption forecasting',\r\n-        formatter_class=argparse.ArgumentDefaultsHelpFormatter\r\n-    )\r\n-    \r\n-    # Add arguments\r\n-    parser.add_argument(\r\n-        '--data',\r\n-        type=str,\r\n-        required=True,\r\n-        help='Path to the CSV data file containing energy consumption data'\r\n-    )\r\n-    \r\n-    # Parse arguments and train\r\n-    args = parser.parse_args()\r\n-    train_model(args.data)\r\n-\r\n-if __name__ == '__main__':\r\n     main()\n\\ No newline at end of file\n"
                }
            ],
            "date": 1733003883280,
            "name": "Commit-0",
            "content": "import argparse\r\nfrom pathlib import Path\r\nfrom typing import Dict, Any, Tuple\r\n\r\nfrom pipeline.utils.config_utils import create_pipeline_config\r\nfrom pipeline.implementations.time_series_pipeline import TimeSeriesPipeline\r\nfrom models.registry.model_types import ModelType, initialize_model_registry\r\n\r\ndef get_default_config_params() -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\r\n    \"\"\"Get default configuration parameters.\"\"\"\r\n    model_params = {\r\n        \"input_features\": 10,\r\n        \"output_features\": 1,  # Changed from 10 to 1 since we're predicting a single value\r\n        \"d_model\": 128,       # Increased from 64\r\n        \"n_heads\": 8,         # Increased from 4\r\n        \"n_encoder_layers\": 4,\r\n        \"n_decoder_layers\": 4,\r\n        \"d_ff\": 512,         # Increased from 256\r\n        \"dropout\": 0.2,      # Increased from 0.1\r\n        \"max_seq_length\": 168,  # Changed to weekly pattern (24*7)\r\n        \"transformer_labels_count\": 12\r\n    }\r\n\r\n    training_params = {\r\n        \"learning_rate\": 0.0001,  # Reduced from 0.001\r\n        \"max_epochs\": 100,\r\n        \"use_early_stopping\": True,\r\n        \"early_stopping_patience\": 15,  # Increased from 10\r\n        \"batch_size\": 64,        # Increased from 32\r\n        \"device\": \"cuda\",\r\n        \"transformer_labels_count\": 12,\r\n        \"forecasting_horizon\": 12,\r\n        \"transformer_use_teacher_forcing\": True,\r\n        \"gradient_clip_val\": 1.0  # Add gradient clipping\r\n    }\r\n\r\n    dataset_params = {\r\n        \"time_variable\": \"utc_timestamp\",\r\n        \"target_variable\": \"DE_KN_residential1_grid_import\",\r\n        \"time_series_window_in_hours\": 24,\r\n        \"forecasting_horizon_in_hours\": 12,\r\n        \"is_single_time_point_prediction\": False,\r\n        \"include_time_information\": True,\r\n        \"is_training_set\": True,\r\n        \"labels_count\": 12,\r\n        \"one_hot_time_variables\": False,\r\n        \"normalize_data\": True,  # Add data normalization\r\n        \"scaling_method\": \"standard\",  # Use standard scaling\r\n    }\r\n\r\n    return model_params, training_params, dataset_params\r\n\r\ndef train_model(data_path: str) -> None:\r\n    \"\"\"Train the transformer model using the pipeline.\"\"\"\r\n    # Initialize model registry first\r\n    initialize_model_registry()\r\n    \r\n    # Debug: Print available models\r\n    from models.registry.factory import ModelFactory\r\n    print(f\"Available models in registry: {list(ModelFactory._registry.keys())}\")\r\n    \r\n    model_params, training_params, dataset_params = get_default_config_params()\r\n    \r\n    # Create pipeline configuration\r\n    config = create_pipeline_config(\r\n        data_path=data_path,\r\n        model_type=ModelType.VANILLA_TRANSFORMER,\r\n        model_params=model_params,\r\n        training_params=training_params,\r\n        dataset_params=dataset_params\r\n    )\r\n    \r\n    # Initialize and run pipeline\r\n    pipeline = TimeSeriesPipeline(config)\r\n    experiment = pipeline.run()\r\n    \r\n    # Print results\r\n    if experiment is None:\r\n        print(\"Error: Training failed to produce results\")\r\n        return\r\n        \r\n    print(\"\\nTraining Complete!\")\r\n    print(f\"Training time: {experiment.training_time:.2f} seconds\")\r\n    print(f\"Test time: {experiment.test_time:.2f} seconds\")\r\n    \r\n    print(\"\\nEvaluation Metrics:\")\r\n    for metric, value in experiment.evaluation.total_metrics.items():\r\n        print(f\"{metric}: {value:.4f}\")\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description='Train transformer model for time series forecasting')\r\n    parser.add_argument('--data', type=str, required=True, help='Path to the CSV data file')\r\n    args = parser.parse_args()\r\n    \r\n    train_model(args.data)\r\n\r\nif __name__ == '__main__':\r\n    main()"
        }
    ]
}