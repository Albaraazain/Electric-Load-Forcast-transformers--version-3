{
    "sourceFile": "training/base/base_trainer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1733003498172,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1733003498171,
            "name": "Commit-0",
            "content": "# training/base/base_trainer.py\r\nfrom abc import ABC, abstractmethod\r\nfrom typing import Any, Dict, List\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.optim.lr_scheduler import StepLR\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.optim.optimizer import Optimizer\r\nfrom training.reports.training_report import TrainingReport  # Updated import path\r\nimport os\r\n\r\n\r\nclass TrainingEpoch:\r\n    def __init__(self, epoch_number: int, training_loss: float, validation_loss: float):\r\n        self.epoch_number = epoch_number\r\n        self.training_loss = training_loss\r\n        self.validation_loss = validation_loss\r\n        self.callbacks = []\r\n\r\n    def serialize(self) -> Dict[str, Any]:\r\n        return {\r\n            'epochNumber': self.epoch_number,\r\n            'trainingLoss': self.training_loss,\r\n            'validationLoss': self.validation_loss\r\n        }\r\n\r\n\r\nclass BaseTrainer(ABC):\r\n    def __init__(\r\n            self,\r\n            train_data_loader: DataLoader[torch.Tensor],\r\n            validation_data_loader: DataLoader[torch.Tensor],\r\n            model: nn.Module,\r\n            loss_criterion: nn.Module,\r\n            optimizer: Optimizer,\r\n            epochs_count: int,\r\n            learning_rate_scheduler: StepLR,\r\n            args: Any\r\n    ):\r\n        self.train_data_loader = train_data_loader\r\n        self.validation_data_loader = validation_data_loader\r\n        self.model = model\r\n        self.loss_criterion = loss_criterion\r\n        self.optimizer = optimizer\r\n        self.epochs_count = epochs_count\r\n        self.learning_rate_scheduler = learning_rate_scheduler\r\n        self.args = args\r\n        self.best_model_state = {}\r\n        self.checkpoint_dir = 'checkpoints'\r\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\r\n\r\n    def train(self) -> TrainingReport:  # Fix return type annotation\r\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n        print('Used device: ', device)\r\n        self.model = self.model.to(device)\r\n\r\n        train_losses: List[float] = []\r\n        val_losses: List[float] = []\r\n        learning_rates: List[float] = []\r\n        epochs_without_validation_loss_decrease = 0\r\n        minimum_average_validation_loss = float('inf')\r\n\r\n        for epoch in range(self.epochs_count):\r\n            # Training phase\r\n            training_loss = self.train_phase(device)\r\n            train_losses.append(training_loss)\r\n\r\n            # Validation phase\r\n            validation_loss = self.validation_phase(device)\r\n            val_losses.append(validation_loss)\r\n            learning_rates.append(self.optimizer.param_groups[0]['lr'])\r\n\r\n            self.learning_rate_scheduler.step()\r\n\r\n            if self.args.use_early_stopping:\r\n                if minimum_average_validation_loss <= validation_loss:\r\n                    epochs_without_validation_loss_decrease += 1\r\n                else:\r\n                    epochs_without_validation_loss_decrease = 0\r\n                    minimum_average_validation_loss = validation_loss\r\n                    self.best_model_state = self.model.state_dict().copy()\r\n\r\n                if epochs_without_validation_loss_decrease > self.args.early_stopping_patience:\r\n                    print('Early stopping has happened at epoch', epoch)\r\n                    break\r\n\r\n            # Save best model state\r\n            if validation_loss < minimum_average_validation_loss:\r\n                self.best_model_state = self.model.state_dict().copy()\r\n                torch.save({\r\n                    'epoch': epoch,\r\n                    'model_state_dict': self.best_model_state,\r\n                    'optimizer_state_dict': self.optimizer.state_dict(),\r\n                    'validation_loss': validation_loss,\r\n                }, f'{self.checkpoint_dir}/best_model.pt')\r\n\r\n            print('Epoch: ', epoch)\r\n            print('Average training loss: ', training_loss)\r\n            print('Average validation loss: ', validation_loss)\r\n\r\n        # Load best model and move to CPU\r\n        self.model.load_state_dict(self.best_model_state)\r\n        self.model = self.model.to('cpu')\r\n\r\n        return TrainingReport(  # Use renamed class\r\n            train_losses=train_losses,\r\n            val_losses=val_losses,\r\n            learning_rates=learning_rates,\r\n            epochs=self.epochs_count,\r\n            early_stopping_epoch=epoch if epochs_without_validation_loss_decrease > self.args.early_stopping_patience else None\r\n        )\r\n\r\n    @abstractmethod\r\n    def train_phase(self, device: str) -> float:\r\n        \"\"\"Execute training phase for one epoch.\"\"\"\r\n        pass\r\n\r\n    @abstractmethod\r\n    def validation_phase(self, device: str) -> float:\r\n        \"\"\"Execute validation phase for one epoch.\"\"\"\r\n        pass"
        }
    ]
}