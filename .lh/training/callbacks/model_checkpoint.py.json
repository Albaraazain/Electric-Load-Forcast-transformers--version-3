{
    "sourceFile": "training/callbacks/model_checkpoint.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1733003581239,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1733003581239,
            "name": "Commit-0",
            "content": "# training/callbacks/model_checkpoint.py\r\nfrom typing import Dict, Any, Optional\r\nimport os\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nfrom .base_callback import TrainerCallback\r\n\r\n\r\nclass ModelCheckpoint(TrainerCallback):\r\n    \"\"\"Save model checkpoints during training.\"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            filepath: str,\r\n            monitor: str = 'val_loss',\r\n            save_best_only: bool = True,\r\n            save_weights_only: bool = True\r\n    ):\r\n        self.model: Optional[nn.Module] = None\r\n        self.filepath = filepath\r\n        self.monitor = monitor\r\n        self.save_best_only = save_best_only\r\n        self.save_weights_only = save_weights_only\r\n        self.best_value = float('inf')\r\n\r\n    def on_epoch_end(self, epoch: int, logs: Dict[str, Any]) -> None:\r\n        print(f\"ModelCheckpoint: Epoch {epoch} ended.\")\r\n        current = logs.get(self.monitor)\r\n        if current is None:\r\n            print(\"No monitored value found in logs.\")\r\n            return\r\n\r\n        # Convert epoch and float values to strings before formatting\r\n        format_dict = {'epoch': epoch}\r\n        for k, v in logs.items():\r\n            format_dict[k] = v\r\n    \r\n        try:\r\n            filepath = self.filepath.format(**format_dict)\r\n            print(f\"Formatted filepath: {filepath}\")\r\n        except Exception as e:\r\n            print(f\"Error formatting filepath: {e}\")\r\n            filepath = self.filepath.replace(\"{epoch}\", f\"{epoch:02d}\")\r\n            for k, v in logs.items():\r\n                if isinstance(v, float):\r\n                    filepath = filepath.replace(f\"{{{k}:.2f}}\", f\"{v:.2f}\")\r\n                else:\r\n                    filepath = filepath.replace(f\"{{{k}}}\", str(v))\r\n\r\n        if self.save_best_only:\r\n            if current < self.best_value:\r\n                self.best_value = current\r\n                if saved := self._save_model(filepath):\r\n                    print(f\"Model improved. Saved model to {filepath}\")\r\n                else:\r\n                    print(\"Failed to save model\")\r\n            else:\r\n                print(\"Model did not improve. Not saving.\")\r\n        elif saved := self._save_model(filepath):\r\n            print(f\"Model saved to {filepath}\")\r\n        else:\r\n            print(\"Failed to save model\")\r\n\r\n    def _save_model(self, filepath: str) -> bool:\r\n        \"\"\"Save the model and return True if successful\"\"\"\r\n        if self.model is None:\r\n            print(\"Model is not initialized. Cannot save.\")\r\n            return False\r\n\r\n        try:\r\n            # Create directory if it doesn't exist\r\n            os.makedirs(os.path.dirname(filepath), exist_ok=True)\r\n\r\n            # Save model\r\n            if self.save_weights_only:\r\n                torch.save(self.model.state_dict(), filepath)\r\n            else:\r\n                torch.save(self.model, filepath)\r\n\r\n            # Verify file exists after saving\r\n            if os.path.exists(filepath):\r\n                print(f\"Model successfully saved to {filepath}\")\r\n                return True\r\n            else:\r\n                print(f\"File not found after save attempt: {filepath}\")\r\n                return False\r\n\r\n        except Exception as e:\r\n            print(f\"Failed to save model: {str(e)}\")\r\n            return False\r\n\r\n    def on_training_begin(self, model: nn.Module, config: Dict[str, Any]) -> None:\r\n        self.model = model\r\n\r\n    def on_training_end(self, model: nn.Module, config: Dict[str, Any]) -> None:\r\n        pass\r\n\r\n    def on_epoch_begin(self, epoch: int, logs: Dict[str, Any]) -> None:\r\n        pass\r\n"
        }
    ]
}