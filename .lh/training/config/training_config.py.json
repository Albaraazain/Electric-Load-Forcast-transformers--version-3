{
    "sourceFile": "training/config/training_config.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 11,
            "patches": [
                {
                    "date": 1733004261523,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1733066947979,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n # training/config/training_config.py\r\n-from dataclasses import dataclass\r\n-from typing import Optional\r\n+from dataclasses import dataclass, field\r\n+from typing import Any, Dict, Optional\r\n \r\n @dataclass\r\n class TrainingConfig:\r\n     \"\"\"Base training configuration.\"\"\"\r\n@@ -28,5 +28,10 @@\n     transformer_use_auto_regression: bool = False\r\n     learning_rate_scheduler_step: int = 30\r\n     learning_rate_scheduler_gamma: float = 0.1\r\n     attention_dropout: float = 0.1\r\n-    gradient_clipping: Optional[float] = None  # Added this line\n\\ No newline at end of file\n+    gradient_clipping: Optional[float] = None  # Added this line\r\n+    optimizer: str = 'adam'\r\n+    optimizer_config: Dict[str, Any] = field(default_factory=dict)\r\n+    criterion: str = 'mse'\r\n+    criterion_config: Dict[str, Any] = field(default_factory=dict)\r\n+    device: str = 'cuda'\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733067072861,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n # training/config/training_config.py\r\n-from dataclasses import dataclass, field\r\n-from typing import Any, Dict, Optional\r\n+from dataclasses import dataclass\r\n+from typing import Optional\r\n \r\n @dataclass\r\n class TrainingConfig:\r\n     \"\"\"Base training configuration.\"\"\"\r\n@@ -29,9 +29,5 @@\n     learning_rate_scheduler_step: int = 30\r\n     learning_rate_scheduler_gamma: float = 0.1\r\n     attention_dropout: float = 0.1\r\n     gradient_clipping: Optional[float] = None  # Added this line\r\n-    optimizer: str = 'adam'\r\n-    optimizer_config: Dict[str, Any] = field(default_factory=dict)\r\n-    criterion: str = 'mse'\r\n-    criterion_config: Dict[str, Any] = field(default_factory=dict)\r\n-    device: str = 'cuda'\n\\ No newline at end of file\n+    \n\\ No newline at end of file\n"
                },
                {
                    "date": 1733067338632,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n # training/config/training_config.py\r\n-from dataclasses import dataclass\r\n-from typing import Optional\r\n+from dataclasses import dataclass, field\r\n+from typing import Any, Dict, Optional\r\n \r\n @dataclass\r\n class TrainingConfig:\r\n     \"\"\"Base training configuration.\"\"\"\r\n@@ -29,5 +29,10 @@\n     learning_rate_scheduler_step: int = 30\r\n     learning_rate_scheduler_gamma: float = 0.1\r\n     attention_dropout: float = 0.1\r\n     gradient_clipping: Optional[float] = None  # Added this line\r\n+    optimizer: str = 'adam'\r\n+    optimizer_config: Dict[str, Any] = field(default_factory=dict)\r\n+    scheduler: Optional[str] = None\r\n+    scheduler_config: Dict[str, Any] = field(default_factory=dict)\r\n+\r\n     \n\\ No newline at end of file\n"
                },
                {
                    "date": 1733176713149,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n # training/config/training_config.py\r\n from dataclasses import dataclass, field\r\n-from typing import Any, Dict, Optional\r\n+from typing import Dict, Any, Optional\r\n \r\n @dataclass\r\n class TrainingConfig:\r\n     \"\"\"Base training configuration.\"\"\"\r\n@@ -10,29 +10,67 @@\n     use_early_stopping: bool\r\n     early_stopping_patience: int\r\n     batch_size: int = 32\r\n     device: str = 'cuda'\r\n+    gradient_clip_val: float = 1.0\r\n \r\n-@dataclass\r\n-class NeuralNetTrainingConfig(TrainingConfig):\r\n-    \"\"\"Neural network specific training configuration.\"\"\"\r\n-    learning_rate_scheduler_step: int = 30\r\n-    learning_rate_scheduler_gamma: float = 0.1\r\n-    gradient_clipping: Optional[float] = None\r\n+@dataclass \r\n+class ResolutionBasedTrainingConfig(TrainingConfig):\r\n+    \"\"\"Training configuration that adapts to different time resolutions.\"\"\"\r\n+    # Resolution parameters\r\n+    input_resolution_minutes: int\r\n+    forecast_resolution_minutes: int\r\n+    \r\n+    def __post_init__(self):\r\n+        \"\"\"Adjust training parameters based on resolution.\"\"\"\r\n+        # Adjust batch size based on resolution\r\n+        if self.forecast_resolution_minutes <= 15:\r\n+            # More frequent data points need larger batches\r\n+            self.batch_size = max(64, self.batch_size)\r\n+        elif self.forecast_resolution_minutes >= 1440:\r\n+            # Daily or longer predictions need smaller batches\r\n+            self.batch_size = min(16, self.batch_size)\r\n+            \r\n\\ No newline at end of file\n+        # Adjust early stopping patience based on resolution\r\n+        if self.use_early_stopping:\r\n+            if self.forecast_resolution_minutes <= 60:\r\n+                # More frequent data needs more patience\r\n+                self.early_stopping_patience = max(15, self.early_stopping_patience)\r\n+            elif self.forecast_resolution_minutes >= 1440:\r\n+                # Less frequent data can use shorter patience\r\n+                self.early_stopping_patience = min(8, self.early_stopping_patience)\r\n \r\n @dataclass\r\n-class TransformerTrainingConfig(TrainingConfig):\r\n-    \"\"\"Transformer specific training configuration.\"\"\"\r\n-    transformer_labels_count: int = 1\r\n-    forecasting_horizon: int = 24\r\n-    transformer_use_teacher_forcing: bool = False\r\n-    transformer_use_auto_regression: bool = False\r\n-    learning_rate_scheduler_step: int = 30\r\n-    learning_rate_scheduler_gamma: float = 0.1\r\n+class TransformerTrainingConfig(ResolutionBasedTrainingConfig):\r\n+    \"\"\"Training configuration specific to transformer models.\"\"\"\r\n+    # Transformer-specific parameters\r\n+    transformer_labels_count: int\r\n+    forecasting_horizon: int\r\n+    transformer_use_teacher_forcing: bool\r\n     attention_dropout: float = 0.1\r\n-    gradient_clipping: Optional[float] = None  # Added this line\r\n-    optimizer: str = 'adam'\r\n-    optimizer_config: Dict[str, Any] = field(default_factory=dict)\r\n-    scheduler: Optional[str] = None\r\n-    scheduler_config: Dict[str, Any] = field(default_factory=dict)\r\n+    \r\n+    # Optimizer settings\r\n+    optimizer: str = 'adamw'\r\n+    optimizer_config: Dict[str, Any] = field(default_factory=lambda: {\r\n+        'weight_decay': 0.01,\r\n+        'betas': (0.9, 0.98)\r\n+    })\r\n+    \r\n+    # Learning rate scheduler\r\n+    scheduler: Optional[str] = 'one_cycle'\r\n+    scheduler_config: Dict[str, Any] = field(default_factory=lambda: {\r\n+        'pct_start': 0.3,\r\n+        'div_factor': 25.0,\r\n+        'final_div_factor': 1000.0\r\n+    })\r\n \r\n-    \n+    def __post_init__(self):\r\n+        \"\"\"Adjust transformer-specific parameters based on resolution.\"\"\"\r\n+        super().__post_init__()  # Call parent's post init\r\n+        \r\n+        # Adjust attention dropout based on resolution\r\n+        if self.forecast_resolution_minutes <= 60:\r\n+            # More dropout for high-frequency data\r\n+            self.attention_dropout = min(0.3, self.attention_dropout)\r\n+        elif self.forecast_resolution_minutes >= 1440:\r\n+            # Less dropout for low-frequency data\r\n+            self.attention_dropout = max(0.1, self.attention_dropout)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733176807002,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,11 +15,11 @@\n \r\n @dataclass \r\n class ResolutionBasedTrainingConfig(TrainingConfig):\r\n     \"\"\"Training configuration that adapts to different time resolutions.\"\"\"\r\n-    # Resolution parameters\r\n+    # Resolution parameters (required fields first)\r\n+    forecast_resolution_minutes: int\r\n     input_resolution_minutes: int\r\n-    forecast_resolution_minutes: int\r\n     \r\n     def __post_init__(self):\r\n         \"\"\"Adjust training parameters based on resolution.\"\"\"\r\n         # Adjust batch size based on resolution\r\n@@ -41,22 +41,20 @@\n \r\n @dataclass\r\n class TransformerTrainingConfig(ResolutionBasedTrainingConfig):\r\n     \"\"\"Training configuration specific to transformer models.\"\"\"\r\n-    # Transformer-specific parameters\r\n+    # Required transformer-specific parameters first\r\n     transformer_labels_count: int\r\n     forecasting_horizon: int\r\n     transformer_use_teacher_forcing: bool\r\n+    \r\n+    # Optional parameters with defaults\r\n     attention_dropout: float = 0.1\r\n-    \r\n-    # Optimizer settings\r\n     optimizer: str = 'adamw'\r\n     optimizer_config: Dict[str, Any] = field(default_factory=lambda: {\r\n         'weight_decay': 0.01,\r\n         'betas': (0.9, 0.98)\r\n     })\r\n-    \r\n-    # Learning rate scheduler\r\n     scheduler: Optional[str] = 'one_cycle'\r\n     scheduler_config: Dict[str, Any] = field(default_factory=lambda: {\r\n         'pct_start': 0.3,\r\n         'div_factor': 25.0,\r\n"
                },
                {
                    "date": 1733177052594,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,4 @@\n-# training/config/training_config.py\r\n from dataclasses import dataclass, field\r\n from typing import Dict, Any, Optional\r\n \r\n @dataclass\r\n@@ -15,12 +14,10 @@\n \r\n @dataclass \r\n class ResolutionBasedTrainingConfig(TrainingConfig):\r\n     \"\"\"Training configuration that adapts to different time resolutions.\"\"\"\r\n-    # Resolution parameters (required fields first)\r\n     forecast_resolution_minutes: int\r\n     input_resolution_minutes: int\r\n-    \r\n     def __post_init__(self):\r\n         \"\"\"Adjust training parameters based on resolution.\"\"\"\r\n         # Adjust batch size based on resolution\r\n         if self.forecast_resolution_minutes <= 15:\r\n@@ -41,21 +38,21 @@\n \r\n @dataclass\r\n class TransformerTrainingConfig(ResolutionBasedTrainingConfig):\r\n     \"\"\"Training configuration specific to transformer models.\"\"\"\r\n-    # Required transformer-specific parameters first\r\n+    # Required fields must all come first\r\n     transformer_labels_count: int\r\n     forecasting_horizon: int\r\n     transformer_use_teacher_forcing: bool\r\n     \r\n-    # Optional parameters with defaults\r\n+    # Fields with defaults last\r\n     attention_dropout: float = 0.1\r\n     optimizer: str = 'adamw'\r\n+    scheduler: Optional[str] = 'one_cycle'\r\n     optimizer_config: Dict[str, Any] = field(default_factory=lambda: {\r\n         'weight_decay': 0.01,\r\n         'betas': (0.9, 0.98)\r\n     })\r\n-    scheduler: Optional[str] = 'one_cycle'\r\n     scheduler_config: Dict[str, Any] = field(default_factory=lambda: {\r\n         'pct_start': 0.3,\r\n         'div_factor': 25.0,\r\n         'final_div_factor': 1000.0\r\n"
                },
                {
                    "date": 1733177825647,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,48 +7,43 @@\n     learning_rate: float\r\n     max_epochs: int\r\n     use_early_stopping: bool\r\n     early_stopping_patience: int\r\n-    batch_size: int = 32\r\n-    device: str = 'cuda'\r\n-    gradient_clip_val: float = 1.0\r\n \r\n @dataclass \r\n class ResolutionBasedTrainingConfig(TrainingConfig):\r\n     \"\"\"Training configuration that adapts to different time resolutions.\"\"\"\r\n     forecast_resolution_minutes: int\r\n     input_resolution_minutes: int\r\n+    batch_size: int = 32\r\n+    device: str = 'cuda'\r\n+    gradient_clip_val: float = 1.0\r\n     def __post_init__(self):\r\n-        \"\"\"Adjust training parameters based on resolution.\"\"\"\r\n-        # Adjust batch size based on resolution\r\n-        if self.forecast_resolution_minutes <= 15:\r\n-            # More frequent data points need larger batches\r\n+        if self.forecast_resolution_minutes <= 60:\r\n             self.batch_size = max(64, self.batch_size)\r\n+            if self.use_early_stopping:\r\n+                # More frequent data needs more patience\r\n+                self.early_stopping_patience = max(15, self.early_stopping_patience)\r\n         elif self.forecast_resolution_minutes >= 1440:\r\n             # Daily or longer predictions need smaller batches\r\n             self.batch_size = min(16, self.batch_size)\r\n-            \r\n-        # Adjust early stopping patience based on resolution\r\n-        if self.use_early_stopping:\r\n-            if self.forecast_resolution_minutes <= 60:\r\n-                # More frequent data needs more patience\r\n-                self.early_stopping_patience = max(15, self.early_stopping_patience)\r\n-            elif self.forecast_resolution_minutes >= 1440:\r\n+\r\n+            if self.use_early_stopping:\r\n                 # Less frequent data can use shorter patience\r\n                 self.early_stopping_patience = min(8, self.early_stopping_patience)\r\n \r\n @dataclass\r\n class TransformerTrainingConfig(ResolutionBasedTrainingConfig):\r\n     \"\"\"Training configuration specific to transformer models.\"\"\"\r\n     # Required fields must all come first\r\n+    attention_dropout: float = 0.1\r\n+    optimizer: str = 'adamw'\r\n+    scheduler: Optional[str] = 'one_cycle'\r\n+    \r\n+    # Fields without defaults\r\n     transformer_labels_count: int\r\n     forecasting_horizon: int\r\n     transformer_use_teacher_forcing: bool\r\n-    \r\n-    # Fields with defaults last\r\n-    attention_dropout: float = 0.1\r\n-    optimizer: str = 'adamw'\r\n-    scheduler: Optional[str] = 'one_cycle'\r\n     optimizer_config: Dict[str, Any] = field(default_factory=lambda: {\r\n         'weight_decay': 0.01,\r\n         'betas': (0.9, 0.98)\r\n     })\r\n"
                },
                {
                    "date": 1733178179346,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,11 +39,11 @@\n     optimizer: str = 'adamw'\r\n     scheduler: Optional[str] = 'one_cycle'\r\n     \r\n     # Fields without defaults\r\n-    transformer_labels_count: int\r\n-    forecasting_horizon: int\r\n-    transformer_use_teacher_forcing: bool\r\n+    transformer_labels_count: int # type: ignore\r\n+    forecasting_horizon: int # type: ignore\r\n+    transformer_use_teacher_forcing: bool # type: ignore\r\n     optimizer_config: Dict[str, Any] = field(default_factory=lambda: {\r\n         'weight_decay': 0.01,\r\n         'betas': (0.9, 0.98)\r\n     })\r\n"
                },
                {
                    "date": 1733179227642,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,23 +1,28 @@\n from dataclasses import dataclass, field\r\n from typing import Dict, Any, Optional\r\n+import warnings\r\n \r\n @dataclass\r\n-class TrainingConfig:\r\n-    \"\"\"Base training configuration.\"\"\"\r\n+class BaseConfig:\r\n+    \"\"\"Base configuration with required fields.\"\"\"\r\n     learning_rate: float\r\n     max_epochs: int\r\n     use_early_stopping: bool\r\n     early_stopping_patience: int\r\n \r\n @dataclass \r\n-class ResolutionBasedTrainingConfig(TrainingConfig):\r\n+class ResolutionBasedTrainingConfig(BaseConfig):\r\n     \"\"\"Training configuration that adapts to different time resolutions.\"\"\"\r\n+    # Required fields first\r\n     forecast_resolution_minutes: int\r\n     input_resolution_minutes: int\r\n+    \r\n+    # Optional fields with defaults after\r\n     batch_size: int = 32\r\n     device: str = 'cuda'\r\n     gradient_clip_val: float = 1.0\r\n+    \r\n     def __post_init__(self):\r\n         if self.forecast_resolution_minutes <= 60:\r\n             self.batch_size = max(64, self.batch_size)\r\n             if self.use_early_stopping:\r\n@@ -33,17 +38,17 @@\n \r\n @dataclass\r\n class TransformerTrainingConfig(ResolutionBasedTrainingConfig):\r\n     \"\"\"Training configuration specific to transformer models.\"\"\"\r\n-    # Required fields must all come first\r\n+    # Fields that should be required but with defaults for compatibility\r\n+    transformer_labels_count: int = field(default=24)  # Common default for hourly predictions\r\n+    forecasting_horizon: int = field(default=24)  # Common default for day-ahead forecasting\r\n+    transformer_use_teacher_forcing: bool = field(default=True)  # Common default for training\r\n+    \r\n+    # Optional fields with defaults after\r\n     attention_dropout: float = 0.1\r\n     optimizer: str = 'adamw'\r\n     scheduler: Optional[str] = 'one_cycle'\r\n-    \r\n-    # Fields without defaults\r\n-    transformer_labels_count: int # type: ignore\r\n-    forecasting_horizon: int # type: ignore\r\n-    transformer_use_teacher_forcing: bool # type: ignore\r\n     optimizer_config: Dict[str, Any] = field(default_factory=lambda: {\r\n         'weight_decay': 0.01,\r\n         'betas': (0.9, 0.98)\r\n     })\r\n@@ -53,11 +58,30 @@\n         'final_div_factor': 1000.0\r\n     })\r\n \r\n     def __post_init__(self):\r\n-        \"\"\"Adjust transformer-specific parameters based on resolution.\"\"\"\r\n+        \"\"\"Adjust transformer-specific parameters and validate configuration.\"\"\"\r\n         super().__post_init__()  # Call parent's post init\r\n         \r\n+        # Warn about default values being used\r\n+        if self.transformer_labels_count == 24:\r\n+            warnings.warn(\"Using default value (24) for transformer_labels_count. \"\r\n+                        \"Consider explicitly setting this value for your use case.\")\r\n+        \r\n+        if self.forecasting_horizon == 24:\r\n+            warnings.warn(\"Using default value (24) for forecasting_horizon. \"\r\n+                        \"Consider explicitly setting this value for your use case.\")\r\n+        \r\n+        if self.transformer_use_teacher_forcing is True:\r\n+            warnings.warn(\"Using default value (True) for transformer_use_teacher_forcing. \"\r\n+                        \"Consider explicitly setting this value for your use case.\")\r\n+        \r\n+        # Validate values\r\n+        if self.transformer_labels_count <= 0:\r\n+            raise ValueError(\"transformer_labels_count must be positive\")\r\n+        if self.forecasting_horizon <= 0:\r\n+            raise ValueError(\"forecasting_horizon must be positive\")\r\n+            \r\n         # Adjust attention dropout based on resolution\r\n         if self.forecast_resolution_minutes <= 60:\r\n             # More dropout for high-frequency data\r\n             self.attention_dropout = min(0.3, self.attention_dropout)\r\n"
                },
                {
                    "date": 1733180013719,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,11 +12,11 @@\n \r\n @dataclass \r\n class ResolutionBasedTrainingConfig(BaseConfig):\r\n     \"\"\"Training configuration that adapts to different time resolutions.\"\"\"\r\n-    # Required fields first\r\n+    # Time resolution parameters\r\n+    input_resolution_minutes: int\r\n     forecast_resolution_minutes: int\r\n-    input_resolution_minutes: int\r\n     \r\n     # Optional fields with defaults after\r\n     batch_size: int = 32\r\n     device: str = 'cuda'\r\n@@ -25,27 +25,23 @@\n     def __post_init__(self):\r\n         if self.forecast_resolution_minutes <= 60:\r\n             self.batch_size = max(64, self.batch_size)\r\n             if self.use_early_stopping:\r\n-                # More frequent data needs more patience\r\n                 self.early_stopping_patience = max(15, self.early_stopping_patience)\r\n         elif self.forecast_resolution_minutes >= 1440:\r\n-            # Daily or longer predictions need smaller batches\r\n             self.batch_size = min(16, self.batch_size)\r\n-\r\n             if self.use_early_stopping:\r\n-                # Less frequent data can use shorter patience\r\n                 self.early_stopping_patience = min(8, self.early_stopping_patience)\r\n \r\n @dataclass\r\n class TransformerTrainingConfig(ResolutionBasedTrainingConfig):\r\n     \"\"\"Training configuration specific to transformer models.\"\"\"\r\n-    # Fields that should be required but with defaults for compatibility\r\n-    transformer_labels_count: int = field(default=24)  # Common default for hourly predictions\r\n-    forecasting_horizon: int = field(default=24)  # Common default for day-ahead forecasting\r\n-    transformer_use_teacher_forcing: bool = field(default=True)  # Common default for training\r\n+    # Transformer-specific fields\r\n+    transformer_labels_count: int = field(default=24)\r\n+    forecasting_horizon: int = field(default=24)\r\n+    transformer_use_teacher_forcing: bool = field(default=True)\r\n     \r\n-    # Optional fields with defaults after\r\n+    # Model training parameters\r\n     attention_dropout: float = 0.1\r\n     optimizer: str = 'adamw'\r\n     scheduler: Optional[str] = 'one_cycle'\r\n     optimizer_config: Dict[str, Any] = field(default_factory=lambda: {\r\n@@ -59,22 +55,19 @@\n     })\r\n \r\n     def __post_init__(self):\r\n         \"\"\"Adjust transformer-specific parameters and validate configuration.\"\"\"\r\n-        super().__post_init__()  # Call parent's post init\r\n+        super().__post_init__()  # Call parent's post init first\r\n         \r\n-        # Warn about default values being used\r\n+        # Warn about default values\r\n         if self.transformer_labels_count == 24:\r\n-            warnings.warn(\"Using default value (24) for transformer_labels_count. \"\r\n-                        \"Consider explicitly setting this value for your use case.\")\r\n-        \r\n+            warnings.warn(\"Using default value (24) for transformer_labels_count\")\r\n+            \r\n         if self.forecasting_horizon == 24:\r\n-            warnings.warn(\"Using default value (24) for forecasting_horizon. \"\r\n-                        \"Consider explicitly setting this value for your use case.\")\r\n-        \r\n+            warnings.warn(\"Using default value (24) for forecasting_horizon\")\r\n+            \r\n         if self.transformer_use_teacher_forcing is True:\r\n-            warnings.warn(\"Using default value (True) for transformer_use_teacher_forcing. \"\r\n-                        \"Consider explicitly setting this value for your use case.\")\r\n+            warnings.warn(\"Using default value (True) for transformer_use_teacher_forcing\")\r\n         \r\n         # Validate values\r\n         if self.transformer_labels_count <= 0:\r\n             raise ValueError(\"transformer_labels_count must be positive\")\r\n@@ -82,9 +75,7 @@\n             raise ValueError(\"forecasting_horizon must be positive\")\r\n             \r\n         # Adjust attention dropout based on resolution\r\n         if self.forecast_resolution_minutes <= 60:\r\n-            # More dropout for high-frequency data\r\n             self.attention_dropout = min(0.3, self.attention_dropout)\r\n         elif self.forecast_resolution_minutes >= 1440:\r\n-            # Less dropout for low-frequency data\r\n             self.attention_dropout = max(0.1, self.attention_dropout)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1733180683871,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,12 +34,21 @@\n \r\n @dataclass\r\n class TransformerTrainingConfig(ResolutionBasedTrainingConfig):\r\n     \"\"\"Training configuration specific to transformer models.\"\"\"\r\n-    # Transformer-specific fields\r\n-    transformer_labels_count: int = field(default=24)\r\n-    forecasting_horizon: int = field(default=24)\r\n-    transformer_use_teacher_forcing: bool = field(default=True)\r\n+    # Transform-specific fields with explicit defaults\r\n+    transformer_labels_count: int = field(\r\n+        default=24,\r\n+        metadata={\"help\": \"Number of labels for transformer model\"}\r\n+    )\r\n+    forecasting_horizon: int = field(\r\n+        default=24,\r\n+        metadata={\"help\": \"Number of time steps to forecast\"}\r\n+    )\r\n+    transformer_use_teacher_forcing: bool = field(\r\n+        default=True,\r\n+        metadata={\"help\": \"Whether to use teacher forcing during training\"}\r\n+    )\r\n     \r\n     # Model training parameters\r\n     attention_dropout: float = 0.1\r\n     optimizer: str = 'adamw'\r\n@@ -57,17 +66,10 @@\n     def __post_init__(self):\r\n         \"\"\"Adjust transformer-specific parameters and validate configuration.\"\"\"\r\n         super().__post_init__()  # Call parent's post init first\r\n         \r\n-        # Warn about default values\r\n-        if self.transformer_labels_count == 24:\r\n-            warnings.warn(\"Using default value (24) for transformer_labels_count\")\r\n-            \r\n-        if self.forecasting_horizon == 24:\r\n-            warnings.warn(\"Using default value (24) for forecasting_horizon\")\r\n-            \r\n-        if self.transformer_use_teacher_forcing is True:\r\n-            warnings.warn(\"Using default value (True) for transformer_use_teacher_forcing\")\r\n+        # Remove warnings since we're now explicitly setting these values\r\n+        # with proper metadata\r\n         \r\n         # Validate values\r\n         if self.transformer_labels_count <= 0:\r\n             raise ValueError(\"transformer_labels_count must be positive\")\r\n"
                }
            ],
            "date": 1733004261523,
            "name": "Commit-0",
            "content": "# training/config/training_config.py\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional\r\n\r\n@dataclass\r\nclass TrainingConfig:\r\n    \"\"\"Base training configuration.\"\"\"\r\n    learning_rate: float\r\n    max_epochs: int\r\n    use_early_stopping: bool\r\n    early_stopping_patience: int\r\n    batch_size: int = 32\r\n    device: str = 'cuda'\r\n\r\n@dataclass\r\nclass NeuralNetTrainingConfig(TrainingConfig):\r\n    \"\"\"Neural network specific training configuration.\"\"\"\r\n    learning_rate_scheduler_step: int = 30\r\n    learning_rate_scheduler_gamma: float = 0.1\r\n    gradient_clipping: Optional[float] = None\r\n\r\n@dataclass\r\nclass TransformerTrainingConfig(TrainingConfig):\r\n    \"\"\"Transformer specific training configuration.\"\"\"\r\n    transformer_labels_count: int = 1\r\n    forecasting_horizon: int = 24\r\n    transformer_use_teacher_forcing: bool = False\r\n    transformer_use_auto_regression: bool = False\r\n    learning_rate_scheduler_step: int = 30\r\n    learning_rate_scheduler_gamma: float = 0.1\r\n    attention_dropout: float = 0.1\r\n    gradient_clipping: Optional[float] = None  # Added this line"
        }
    ]
}